--- 
title: "Data Science + Public Policy"
author: "Jeff Chen"
date: '`r Sys.Date()`'
output: pdf_document
description: Chapter 10
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
---


### What level of [demand/staff] should I expect?

To put regression into perspective, we will predict [Maryland tollroad transactions](https://data.maryland.gov/Transportation/Toll-Transactions/hrir-ejvj) using an assortment of US county-level data. The dataset is comprised of monthly totals of tollroad transactions by county, joined with US Bureau of Labor Statistics' [monthly employment estimates](https://www.bls.gov/cew/datatoc.htm), US Census Bureau [building permit estimates](https://www.census.gov/construction/bps/), and the Energy Information Agency's [West Texas Crude Fuel Prices](http://www.eia.gov/dnav/pet/pet_pri_spt_s1_d.htm). 

To get started, first we need to import the data set using `digIt`.

```{r, echo=FALSE, message=FALSE, warning = FALSE}
  library(digIt)
  df <- digIt("toll_model")
```


__Explore data__

The dataset is provided in long form, where data from multiple geographic areas are stacked. The dataset contains eight fields: 

- `date` in `MM/DD/YY` format.
- `year` of record.
- `fips` is the Federal Information Processing System (FIPS) code for a given county.
- `transactions` is total monthly toll transactions in a county.
- `transponders` is the total monthly toll transactions conducted using a radio frequency transponder.
- `emp` is the employment in a given month in given county.
- `bldgs` is the number of new building permits issued in a given county.
- `wti_eia` is the West Texas Intermediate spot crude price. This is the only measure that will have the same value across all geographies in a given time period.

To get a feel for the data, we use the `str()` method. All fields should be in numerical or integer form except for `date` and `fips`. 
```{r}
  str(df[1:3,])
```

Reformatting is simple. `date` can be converted into a date object using `as.Date()` and `fips` can be converted into a factor.
```{r}
  df$date <- as.Date(df$date, "%m/%d/%y")
  df$fips <- factor(df$fips)
```

Upon doing so, we can run some cursory exploratory checks ahead of formulating any models. First is to produce correlation matrix. We will beautify the text outputs using a library called `sjPlot` that formats quantitative outputs in  an easier to read fashion. 

The correlation matrix on the pooled data indicates finds that employment is positively associated with both transactions and transponder transactions. 

```{r, fig.height=2, warning=FALSE, message = FALSE}
  library(sjPlot)
  tab <- cor(df[,c(4:8)], use = "complete")
  sjp.corr(tab)
```

For ease of interpretation, we will use a `log-log` specification, meaning that  continuous variables that enter the regression specification are transformed using a natural log. This changes the interpretation of coefficients and in may improve model fit in certain situations.

```{r, fig.height=2, warning=FALSE, message = FALSE}
  tab <- cor(log(df[,c(4:8)]), use = "complete")
  sjp.corr(tab)
```

In addition, we can run an ANOVA (Analysis Of Variance) to understand if using FIPS county codes help explain the variation in transactions by looking at if the mean of transactions for each county are the same. The null hypothesis that county means are equivalent is rejected as the p-value of the F-test below is asymptotically small (< 0.01).
```{r, message=FALSE, warning = FALSE}
  fit <- aov(transactions ~ fips, data = df)
  summary(fit) 
```

The same test using $log(transactions)$ yields a better fit when comparing, indicating that we may want to consider to use log-transformed target feature.
```{r, message=FALSE, warning = FALSE}
  fit <- aov(log(transactions) ~ fips, data = df)
  summary(fit) 
```

Graphically, it is also important to develop a visual understand of how the data are distributed. Using `ggplot2`, we plot the toll transactions over time, finding a degree of seasonality and clear differences in the levels at which traffic flows through each county.
```{r, fig.height=2, message=FALSE, warning = FALSE}
  library(ggplot2)
  ggplot(df, 
         aes(x = date, y = transactions, group = fips, colour = fips)) +
        geom_line() +  geom_point()
```

Recognizing that each county experiences different levels of traffic and economic activity, it is prudent to break apart the data into histograms for each county to expose skewedness and determine if the fundamental Gaussian assumptions of OLS are met. The histograms below indicate that there most measures can be characterized by a central tendency, but there are some measures in some counties that are significantly right skewed.
```{r, fig.height=2, warning=FALSE, message=FALSE}
  
  fips <- unique(df$fips)
  for(k in 1:length(fips)){
    temp <- ggplot(data = df[df$fips==fips[k],], aes(transactions)) + 
            geom_density(fill = "orange") + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("trans",k), temp)
    
    temp <- ggplot(data = df[df$fips==fips[k],], aes(emp)) + 
            geom_density(fill = "red") + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("emp",k), temp)
        
    temp <- ggplot(data = df[df$fips==fips[k],], aes(bldgs)) + 
            geom_density(fill = "navy") + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("bldgs",k), temp) 

  }

  library(gridExtra)
  grid.arrange(trans1, trans2, trans3, trans4, trans5, trans6,
               emp1, emp2, emp3, emp4, emp5, emp6,
               bldgs1, bldgs2, bldgs3, bldgs4, bldgs5, bldgs6, ncol = 6)
  
```

This skewness can be improved by applying mathematical transformations such as $log_{10}$ or natural logarithm. 

```{r, echo=FALSE, fig.height=2, warning=FALSE, message=FALSE}
  
  fips <- unique(df$fips)
  for(k in 1:length(fips)){
    temp <- ggplot(data = df[df$fips==fips[k],], aes(transactions)) + 
            geom_density(fill = "orange") + scale_x_log10() + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("trans",k), temp)
    
    temp <- ggplot(data = df[df$fips==fips[k],], aes(emp)) + 
            geom_density(fill = "red") + scale_x_log10() + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("emp",k), temp)
        
    temp <- ggplot(data = df[df$fips==fips[k],], aes(bldgs)) + 
            geom_density(fill = "navy") + scale_x_log10() + 
            theme(plot.title = element_text(size = 9), axis.text.x=element_blank(),
            axis.text.y=element_blank())
    assign(paste0("bldgs",k), temp) 

  }

  grid.arrange(trans1, trans2, trans3, trans4, trans5, trans6,
               emp1, emp2, emp3, emp4, emp5, emp6,
               bldgs1, bldgs2, bldgs3, bldgs4, bldgs5, bldgs6, ncol = 6)
  
```


__Set train/test samples__
With a basic understanding of the patterns in the underlying data, we can partition the data for training and testing. Given the small sample of points, we will partition the data into a 60-30 split, which is approximately 2012 through 2014 and 2015 through 2016, respectively. This partition is captured in a new variable `flag`. 

```{r}

  df$flag <- 0
  df$flag[df$year > 2014] <- 1
  
```

__Regression__
Running a linear regression is a fairly simple task when using the `lm()` method. The basic syntax involves specifying the $y$, $x$ and the dataframe or matrix.

```{r, eval=FALSE}
  lm(<yvar> ~ <xvars> , data = <data>)
```

When evoked, the `lm()` method produces an class object that contains all the outputs of a regression model, including coefficients, fitness tests, residuals, predictions among other things.As a simple example, we will fit the specification: $\log\text{(transactions)} = \beta_0 + \beta_1 \text{log(employment)} + \epsilon$ using only the training set (`flag == 0`). We can assign the output to `fit`. To see all attributes of the `fit` object, use the `str()` method. For a high-level summary, use the `summary()` method. 

The bivariate regression yielded statistically significant results for employment, indicating that a 100% increase in employment is associated with a 43% increase in highway toll transactions. The amount of variance explained is modest with an $R^2 = 0.321$ with a relatively large $RMSE = 0.6904$. 

```{r, message=FALSE, warning=FALSE}
  fit <- lm(log(transactions) ~ log(emp), data = df[df$flag == 0,])
  summary(fit)
```


The `fit` object contains other rich information about attributes of a regression model, such as the coefficients, residuals, among other features. The full detail coefficients, for instance, can be easily viewed by using the following command: 
```{r, message=FALSE, warning=FALSE}
  summary(fit)$coef
```

To view the full list of attributes contained in the regression object, use the structure method `str()`.

A common step in assessing model fitness regressions is to check if the normality assumptions are met as this will influence reliability and accuracy of the model. The residuals, for example, should be normally distributed; however, the kernel density graph below shows that the residuals (in blue) are bimodally distributed, which is not normally distributed as compared with the simulated normal distribution (yellow). This indicates provides an indication that the regression needs to be refined in order to account for other parameters.


```{r, message=FALSE, warning=FALSE, fig.height=2}
  x <- data.frame(resid = fit$residuals)
  set.seed(50)
  x$norm <- rnorm(nrow(x), mean(x$resid), sd(x$resid))
  ggplot(x, aes(resid)) + 
    geom_density(aes(norm), alpha = 0.4, fill = "yellow") + 
    geom_density(fill = "navy", alpha = 0.6) 
```

Thinking back to the earlier results of the ANOVA test, it makes sense to incorporate the `fips` feature and try a few other features. The results are far more promising, with an R-squared above 0.9 and a RMSE that is roughly one-third the size. The employment feature is statistically significant with a p-value under 0.05 with a coefficient that indicates that essentially every one-person (or 100%) increase in employment is associated with a two fold increase in trips -- this makes sense as people may be commuting for work. Additional building permits (`log(bldgs)`) appear to have a modest effect as may be expected as their effect is likely lagged. The `FIPS` counties, while not fully significant, seem to help explain much of the variability. 

```{r, message=FALSE, warning=FALSE}
  fit <- lm(log(transactions) ~ log(emp) + log(bldgs) + log(wti_eia) + fips, 
              data = df[df$flag == 0,])
  summary(fit)
```

A cursory look at the residuals finds a much more normally distributed set of residuals, indicating that incorporating the `FIPS` codes enables better a more proper model.
```{r, message=FALSE, warning=FALSE, fig.height=2}
  x <- data.frame(resid = fit$residuals)
  set.seed(50)
  x$norm <- rnorm(nrow(x), mean(x$resid), sd(x$resid))
  ggplot(x, aes(resid)) + 
    geom_density(aes(norm), alpha = 0.4, fill = "yellow") + 
    geom_density(fill = "navy", alpha = 0.6) 
```

__Prediction__
The critical step is the prediction step, using the trained regression model to score the test set. In this example, both train and test sets are contained in data frame `df`. Generating predicted values $\hat{y}$ using the input features is a simple task that relies on the `predict()` method. The method accepts the regression object and a new dataset. In our example, we input use `fit` and `df`. The output is a vector of predictions for each line of `df`. We assign this vector as a new column as `df`.

```{r, message=FALSE, warning=FALSE}
  df$yhat <- predict(fit, newdata = df) 
```

With the prediction available, a Mean Absolute Percentage Error (MAPE) can be used to quantify the model fit in terms that are interpretable. To do so, we first need to write a MAPE function.
```{r}
  mape <- function(y_hat, y){
    return(mean(abs(y_hat/y-1), na.rm=T))
  }
```

Upon doing so, we can compare the performance of the model in the training phase and the test phase. The error rate of the training set was 0.8% and grows to 1.4% in the test set, indicating some degree of overfitting. However, in absolute terms, the errors are relatively small.
```{r}
  #train error
    train_error <- mape(df[df$flag == 0, "yhat"], log(df[df$flag == 0, "transactions"]))
    print(paste0("MAPE-train = ", train_error))
  #test error
    test_error <- mape(df[df$flag == 1, "yhat"], log(df[df$flag == 1, "transactions"]))
    print(paste0("MAPE-test = ", test_error))
```

The predictions can be compared against the actual transactional volumes using an ordinary scatter plot. To contextualize the predictions, we also add a 45-degree line that indicates how accurate predictions are relative to actual. Deviations below the line suggest that predictions are underestimation and above the line indicates overestimation. The effects of overfitting are clear in the models when comparing training results to test results. The training sample (left graph) predictions are spot on with predictions landing on the diagonal, whereas some of the test sample (right graph) stray below the line. 
```{r, message=FALSE, warning=FALSE, fig.height = 2}
  library(gridExtra) 
    
  g1 <-  ggplot(data = df[df$flag == 0, ], 
                aes(x = log(transactions), y = log(transactions))) +
          geom_line() +  geom_point()  +  
          ggtitle(paste0("Train set (MAPE = ",round(100*train_error,2),"%)")) +
          geom_point(aes(x = log(transactions), y = yhat, colour = fips)) +
          theme(plot.title = element_text(size = 10), )
  
  g2 <-  ggplot(data = df[df$flag == 1, ], 
                aes(x = log(transactions), y = log(transactions))) +
          geom_line() +  geom_point()  +  
          ggtitle(paste0("Test set (MAPE = ",round(100*test_error,2),"%)")) +
          geom_point(aes(x = log(transactions), y = yhat, colour = fips)) +
          theme(plot.title = element_text(size = 10))
    
  grid.arrange(g1, g2, ncol=2)
      
```


