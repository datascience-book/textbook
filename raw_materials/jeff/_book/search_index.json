[
["index.html", "Data Science: A Practical Strategy (v0.2)", " Data Science: A Practical Strategy (v0.2) Jeff Chen (contact@jeffchen.org) 0.0.0.0.1 _Dan Hammer {-} ##### Version as of January 2018 {-} © 2018 Jeff Chen and Dan Hammer. All Rights Reserved. "],
["data-and-its-many-contexts.html", "Chapter 1 Data And Its Many Contexts 1.1 Fires and Data 1.2 The Answer is 42. Well, Maybe. 1.3 The Buzz and the Buzzworthy 1.4 What’s The Value Proposition? 1.5 Structure", " Chapter 1 Data And Its Many Contexts 1.1 Fires and Data On an August afternoon in 2007, a fire broke out on the 17th floor of the then-vacant Deutsche Bank Building, a skyscraper situated across from the former World Trade Center in New York City. The building, seriously damaged after the 9/11 attacks, had been undergoing hazard abatement and controlled demolition, leading to changes to the building floor plans and safety apparatus. When the New York City Fire Department (FDNY) responded to the scene, it was clear the fire was a serious one, quickly escalating to a seven-alarm fire incident requiring 87 units and 475 firefighters.1 As standpipes had been disabled and floor plans altered, FDNY units found it difficult to navigate the skyscraper and put water on the fire, resorting to unconventional methods of supplying water to crews. Eventually, the fire was put out seven hours after it started, not before two firefighters lost their lives, succumbing to cardiac arrest from heavy smoke inhalation.2 In response to the tragedy, a mayoral investigation found that the deaths could have been prevented had city agencies established information sharing protocols and leveraged a risk-based strategy to mitigate and avoid hazards.3 While an ideal end state would be to end all structural fires, the recommendations focused on reducing death and injury by ensuring that FDNY had the most up-to-date ground intelligence. Risk mitigation strategy was indeed due for improvements. Since the 1950’s, FDNY building inspections were managed using a manual index card system where inspection schedules were based on tiers of perceived risk, where the riskiest buildings needed to be inspected once a year and the least risky buildings were inspected once every seven years. Despite the longevity of the system, it was not without shortcomings, namely that a building’s risk tier was not updated with the latest intelligence nor was it guaranteed that schedules remained on track. To maintain the intelligence would have been a laborious process, requiring cards to be updated by hand for one-third of a million buildings. Hypothetically, if each building record were updated daily and the process were to take one second per record, FDNY would only have a refreshed profile of the city once every 229 days. By then, firefighters could have wasted valuable time at buildings that posed little danger and preventable risks could have led to disaster. It was clear that a data-driven strategy could fundamentally change the nature of fire risk. The New York City Fire Department (FDNY) set out to address the risk management problem by melding data and technology with their operations. On the surface, the idea of using data and technology to reduce the risk of fire is quite alluring. However, under the hood, there were notable obstacles. On the operational side, buy-in was required. Anyone who has observed fire fighters on scene will notice that it is a well-choreographed operation – every person knows their part and believes in the established protocols. For data to drive value, it needed to be integrated and accepted into the culture of a 10,000+ person fire fighting organization. On the technical side, decades worth of index cards needed to be digitized and a scheduling platform needed to be developed. Perhaps most importantly, the system had to work. Scheduling just any inspection is simple; scheduling inspections to buildings with observable risks is far more challenging. Without effective targeting, the entire effort would be for naught. The Commissioner and First Deputy Commissioner at the time both believed that technology had a role to play at FDNY. Aligned with Mayor Michael Bloomberg’s vision of smart, data-driven government, they saw an opportunity to set an example for the nation’s fire service. They relied on the the Assistant Commissioner for Management Initiatives to lead a change management process with fire chiefs and fire officers, information technology (IT) managers, among others to change the flow of operations so that data served as a pillar on which FDNY could rely. Alliances were forged with leading fire personnel such as the Deputy Chief of Fire Operations and Battalion Chiefs to formalize the role of data in the culture of the fire house, amending standard operating procedures (SOPs) to use a digital inspection system. On the IT front, a lead software engineer and project manager meticulously gathered specifications that were then used to construct a scheduling platform. Recognizing that the proof of a risk-based strategy was in the pudding, a Director of Analytics was hired to lead the overhaul of a prediction algorithm to rank buildings based on their risk and convincing stakeholders that a mathematical representation of fire ignition was indeed true. The result was the Risk-Based Inspection System (RBIS), a firefighter-facing data platform that scheduled inspections at buildings with the greatest risk of fire. Three times a week for three hours per session, fire officers logged onto RBIS to obtain a list of buildings for scheduled inspection. Buildings were selected using FireCast, a statistical algorithm developed in-house to predict fires at the building level. Through FireCast, buildings no longer used assumed a static risk classification, but rather a dynamic risk score that took into account the latest information. Prediction often relies on accuracy measures to determine how well algorithms perform in the field; FireCast was no different. The algorithm was able to identify buildings with fires over 80% of the time – a degree of accuracy that was not previously possible. Upon implementing the new system, impacts were observed in leading operational indicators. In the first month, the number of safety violations issued grew by +19% relative to the trend under the index card system, but fell to +10% in the second month. This indicated that the riskiest buildings did indeed have more observable risks than less risky buildings, but the amount of observable risk fell as teams went down the list. From a statistical perspective, the prediction should have yielded far more violations, but efficacy of the prediction program was limited by (1) a fire unit’s time budget to conduct inspections; (2) a policy requiring that time had to be set aside for weekly inspections, which at times led to inspecting buildings that were not observably risky after all truly risky buildings were exhausted; (3) the rule of law giving residents the right to refuse inspection. To measure efficacy, FDNY developed an indicator known as the Pre-Arrival Coverage Rate (PACR), which measures the proportion of buildings that experienced a fire that were inspected within some period (90 days) before the fire occurred – essentially measuring if fire companies had the opportunity to evaluate risks of priority buildings. Under FireCast, FDNY had achieved a PACR of 16.5%, which was an eightfold improvement over the old strategy that yielded 1.5%.4 The RBIS-FireCast program went onto gain recognition as a modern approach to optimizing city operations, recognized by Harvard University’s Ash Center with a Bright Idea Award and serves as a reference implementation for next generation fire fighting technologies.5 1.2 The Answer is 42. Well, Maybe. In the novel The Hitch Hiker’s Guide to the Galaxy, author Douglas Adams describes how a super computer called Deep Thought calculated the ultimate answer to “Life, the Universe and Everything”. The computer’s calculation lasted seven and a half million years and returned a simple answer: 42.6 It is a rather unsatisfying answer. It lacks context. It is abstract. It has perplexed and intrigued readers for decades, and to some degree has amassed a following. The answer lacks a narrative that is expected by all humans. After all, as people, we are social creatures and we need closure. This book answers the question “How do we make data real to people?” Data and the number 42 are kindred spirits – both are often viewed as an abstract concepts that are characterized by commercial buzzwords. To many, working in the data space is described as a discipline for specialists. It is actually quite the opposite. It has a tangible real world role that people use on a day to day basis. Depending on your specialty and environment, the way data is used falls onto a spectrum of use cases – or actions that a user takes with a system, software, or information. On the simpler end are benchmarks – a way of contextualizing numbers. In the middle are empirical explanations of what factors influence observed phenomena. And on the other end are predictions – the product of highly technical, scientific computing. The #42 falls into one of these three use case buckets. The number could have been easily benchmarked and contextualized using units or comparison to other numbers so that a significance and value judgment could be applied. Alas, it did not. Naturally, people would want to understand how 42 was derived, what parameters guided it, and what influenced the magnitude. A 7.5 million year calculation may seem daunting to explain though conversely one may argue that the code was quite inefficient. The algorithm that underlied the estimation was not made open source, thus its rational is still unknown. But it is after all the meaning of literally everything, thus a universal constant is persistent in the past, present and future – making it a prediction of something. Now if only someone could define the unit of analysis. Data use case spectrum 1.2.1 Benchmarking Human nature functions of comparisons. Thus, benchmarking helps to contextualize the current state relative to a reference point. The reference point gives context over time and space to illustrate a trend or can be used to show relative performance. This is the most natural use of data: often times they are presented as a statistic or a graph to which people can react. It is easily achievable data product – a use of data on which a specific user or audience can rely. Everyday, we naturally use benchmarks to understand the state of our world, set expectations and make decisions: Employment. In the news, the monthly employment figures published by the US Bureau of Labor Statistics are contextualized in comparison to the previous month or the same time last year. For example, the first sentence of a August 2017 article from NPR online stated: “The U.S. economy created an estimated 209,000 jobs in July, representing a modest slowdown from the previous month but coming in better than many economists had expected.”7 On its own, the 209,000 jobs figure is a number in a vacuum. In comparison to the previous month, it givens some indication of economic health. SAT Scores. For high school students looking to apply to college, SAT scores have some bearing on the chance of being accepted. Let’s assume a student received the following scores: math = 710 and evidence-based reading and writing = 800 for a combined score of 1510. Is that good enough to get into a school? Many universities have begun to publish the profile of the incoming class the range of combined SAT scores for the middle 50% (“interquartile range”) of admitted students. This range at Columbia University, for example, is 1510 to 1580.8. This means that 75% of admitted students scored 1510 and above, indicating that a higher score may better position the student for success. College admissions, however, take into account a large number of factors as the process is competitive. Using the SAT benchmark alone may be misleading when selecting schools, but certainly can provide a notional understanding of the chances of admittance. Benchmarks tend to rely on summary-level statistics such as counts, proportions, averages, and percentiles. Given their conciseness, it is easy to incorporate them into narratives, reports and visualizations. But they do not tell a full story. 1.2.2 Explaining Benchmarks do not provide evidence as to why things are the way they are or how phenomena come to be. This naturally gives way to the need to explain how things work, or at least quantify relationships between observed phenomena (e.g. dependent variables or target features) and their associated characteristics (e.g. independent variables or input features). Everyday phenomena can be explained using explanatory models with varying degrees of accuracy. A baseball that is hit by a batter will go a certain distance given a number of factors such as the angle of departure, the mass of bat, the velocity of the bat, among others. Traffic on a highway may be higher on a Monday than a Sunday due to the rhythm of the work week, the time of year, weather among other factors. By developing explanatory models, fields from the natural sciences to the social have been able surface quantifiable relationships between A and B. For instance: Housing prices. In real estate, hedonic price models are used to attribute the price of home (dependent variable) to the characteristics and amenities (independent variables), such as the number of rooms, age of home, construction quality, location, school district among others. Usually employing statistical methods such as linear regression, hedonic models allow economists and statisticians to tease out the value of specific amenities holding all else constant or ceteris paribus. For example, the marginal price for each additional room may fetch $30,000 or houses located in the central business district may be worth 15% lower due to noise. Marginal prices help home sellers and buyers better understand the economic value of a home in terms of tangible characteristics.9 Food Stamps and Crime. Explanatory models may be used to evaluate the effect of policy changes. In February 2010, the State of Illinois enacted a new policy to distribute food stamps over the course of a month as opposed to a former practice to distribute them on the first of each month. This policy shift had a clear impact on public safety, reducing grocery store thefts by 32%. By using a technique known as interrupted time series, economists at Purdue University were able to first determine what the level of theft was in the past and would have been had the policy not been implemented, then compared against what actually happened.10 From a strategic perspective, understanding even the direction of impact of this specific policy change may be enough to serve as a reference implementation for other states seeking to curb crime. Explanatory models help to filter all possible influential factors down to those that hold water, making way for compelling narratives, discussion and possibly rules of thumb. Their usefulness in practice, however, is largely backward looking and forecast accuracy is often not a concern. A health study may very well determine that people are two-times more likely to develop a rare form of cancer if exposed to a certain substance, but the predicted probability of developing cancer for a person who uses the substance is only 1% – a value that is not perceived as risky to most. 1.2.3 Predicting Prediction, on the other hand, ventures to make empirical guesses about what will happen and optimizes for accuracy. The quantitative algorithms and approaches used to make predictions overlap with explanatory models. The difference lies in how we validate predictions. Prediction is largely a matter of instituting an experimental design. The process of building a predictive model is quite similar to assessing a student’s academic performance in class. Let’s pretend that students are provided the final exam for a calculus class on day one of a semester. For the duration of the semester, they are instructed to study that exam alone. If the professor gave that same practice test to students as the actual final exam, then students’ grades will very likely overstate their command of calculus for a number of reasons. This farcical scenario almost never happens, yet it is commonly how explantory models are formulated and treated as prediction models. To demonstrate this, what if different test questions were introduced that are framed differently than the practice test: would the students’ grades hold up? In reality, students in the United States’ education system are given plenty of opportunity to train on practice examples through homework assignments and projects, then are tested on quizzes and tests – the answers to which they are not privy. This train-test approach is designed to give professors the best chance at measuring the skills students have developed. Prediction is uncovering stable and accurate relationships such that guesses on new, unseen data are reliable and dependable. In prediction parlance, train-test is a form of model validation: models are developed on some data, leaving another set of data to make sure the accuracy is what we think it is. Similar to RBIS-FireCast, cross-validation and experiment designs open the door to applying data in vastly more advanced ways. Recommendation engines. Digital companies such as Amazon and Netflix collect information on consumer purchases, views, and ratings, then build recommendation engines – models that predict user preferences.11 Often times, recommendation engines offer products that are used by other users who have similar tastes, thereby providing an seamless way to traverse immense amounts of options in a targeted and useful way. Voter targeting. During the 2012 election cycle, data scientists collected survey data on voter attitudes and preferences, then built voter targeting algorithms to predict who is likely to vote for whom and cater messaging for individualized campaigns.12 This micro-targeting concept is a common strategy in marketing and online advertisement, benefiting from the use of cookies.13 Computer vision. Data scientists have been able to predict stock prices of retailers by developing computer vision algorithms that sift through satellite imagery in order to count the number of cars parked near retailers.14 The ability for computer vision algorithms to identify discrete objects or characteristics in images has far reaching applications from monitoring illegal fishing15 to predicting neighborhood wealth.16 Together, these three analytic functions are powerful and should be used to make successful projects. In short: Benchmarks build trust and context for users; Explanations build understanding of processes; and Prediction enables precise action. 1.3 The Buzz and the Buzzworthy In recent memory, there is a spike in terms such as Artificial Intelligence (AI) and Machine Learning – ideas that lie at forefront of the public’s imagination of what data could enable, capturing a greater proportion of searches on Google than other topics. Human fascination with artificial intelligence lies in the idea that tedious, complex tasks can somehow be automated and acted upon in a way that is independent of human hands. This fanciful goal was accelerated at a time after World War II when computer science was in its infancy. British computer scientist and mathematician Alan Turing published an article in 1950 that proposed the Imitation Game, later was renamed the Turing Test17, designed to assess how a machine may demonstrate intelligent behavior that is the or indistinguishable than that of humans. His article frames the test in terms of a machine issuing chess moves in a manner that humans cannot tell is artificial in nature. While Turing himself did not create AI, he helped to shape the philosophical foundations used to define human-like intelligence – exhibiting similar judgment as humans, but doing so mathematically. Figure 1.1: Comparison of popular data and technology search topics, Jan. 2004 to Jan 2017. All values are range standardized where 100 indicates the maximum number of searches on Google. Like many technical fields, interest in AI has experienced cycles of interest with an initial wave of interest in the 1950’s to 1960’s before experiencing the AI winter in the 1970’s.18 The latest cycle was in the early 2000’s with the introduction of autonomous robots for home such as the iRobot Roomba in 2002 that cleans homes.19 Under the hood, the technologies that lead to AI are guided by the idea that a machine can learn from a set of examples (sometimes referred to as labels or targets), doing so in a manner that is free of direct human instruction. This notion of machine learning is accomplished by providing a computer with mathematical functions to find underlying structures and rules in data, absent of a human’s explicit rules and common sense. Machine learning involves algorithms, which are processes and calculations that are followed to solve well-defined problems. Statistical modeling uses many of the same algorithms as machine learning, but takes a perspective of systematically answering human-defined hypotheses. This is all to say that AI, machine learning and statistical modeling overlap in the mathematics that are employed, but there are technical nuances. Generally machine learning focuses on computers learning patterns and statistical modeling focuses on testing hypotheses. Both use similar techniques. AI takes it one step farther by automating actions and behaviors without a human explicitly instructing it to do so. There are many examples of machine and statistical learning, but only few true instances of artificial intelligence per the Turing Test due to the intensity of data and technology required. Despite this, society has, to some subtle degree, begun to brace for the yet-to-be-seen impacts of AI. One example is the “reverse” Turing Test in the form of CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) used to ensure that programmed bots are not conducting human transactions.20 …and Deep Learning? Deep Learning has become popular term in technology circles in recent memory. It is the application of machine learning class of techniques known as Artificial Neural Networks (ANNs) to problems that may benefit from more than one hidden layer – essentially learning data representations (e.g. what constitutes a yes/no, sell/buy/, cat/dog) is dependent on multiple levels of abstraction so that latent characteristics can also be learned. Deep learning is commonly applied often to vision problems, audio data and other highly dimensional data21. For example, if a human were given the following two images, she would be able to determine one is a cup and the other is a bowl. This simple task requires human brains years to be trained and recognize objects and associate the items with their shapes, proportions, sizes, colors, texture among other contextual features. Figure 1.2: Example images: A cup and bowl. [Credit goes here] For computers, however, the task is quite a computationally costly challenge as it requires a machine to learn latent features (e.g. shapes, texture, color) from the grid of pixels that comprise an image. For this to be useful and accurate, ANNs requires a large amount of data, a requirement that is rooted in the mathematical assumptions that underlie the techniques. Lambourghini vs. Jeep. With all the data science techniques to choose from, being effective with data is like the vehicle choice an astute, wealthy adventurer needs to make when traveling into the rainy, muddy jungle and traverse streams and obstacles unknown. Should she drive her $494,000 Lamborghini Aventador super car that can reach speeds of 217 mph on flat roads or her $24,000 Jeep Cherokee with all-wheel drive? The way in which data is used should fit the scope of the problem. Advanced AI and deep learning capabilities have a role to play, but its not for everything. As easily as we can see trade offs of a Jeep and Lamborghini, we should see data-driven techniques in the same light and recognize that a data panacea does not likely exist. Learning to walk. Google Trends indicates that interest in artificial intelligence is consistently outstripped by the fields of statistics and computer science, perhaps as a command of each discipline enables practitioners to be more versatile in how they apply data intelligently The field of statistics is more than means and standard deviations, but the study and practice of collecting and analyzing large quantities of numerical data. Computer science is a field that solves algorithmic problems on scale. Both fields have their roots in mathematics, but maintain divergent views of how to approach and use data. Figure 1.3: Comparison of search topics, Jan. 2004 to Jan 2017. Google Trends. Both computer science and statistics rely on data analysis to aid in the systematic process of inspecting, cleaning, and analyzing data in order to uncover patterns about the data itself and the information it represents. In more contemporary parlance, this is referred to as data analytics, data mining, or data science although there is little consensus on the definitions.22 1.4 What’s The Value Proposition? Over 20 software and coding languages were enumerated in the previous section alone in addition to quite a few types of roles and applications. The ecosystem that is centered upon data analytics is indeed large and rapidly expanding, but why does it matter? As was illustrated through RBIS-FireCast, the value of data analytics will likely vary at each level in an organization. Making successful data projects like RBIS-FireCast is not only a matter of mastering the problem identification process, technical skills, methods, and software described above, but also recognizing that technology is a social affair that requires buy-in, potential, and consensus. Building consensus and buy-in is largely a matter of communicating insights so that it is “real”, so that it answers the questions on peoples’ minds: Executive. In a sentence, what does your data project do for our organization? How can I [the executive] use this to advance my strategic goals? What do people in the ranks think? What do people outside think? What makes you excited about this work? How can you measure the benefit? Does it give our organization more benefit than it costs? What cutting edge technologies will/do you use? Prove to me that this is real through a “live fire” visual demo or presentation. Do you have a clear, buzz worthy one pager I can shop around? Is anyone else doing it – why or why not? What can I do to support this work? Team Lead. In five sentences, what does your project do to support our team? Does it align with the organizational goals? How do you know that this works? Does it try any of the things that I have done previously? How did you build it? Tell me about your road map – what are the milestones? What is the end state? How much resources do you need? When can we raise it to folks above? Analyst/Researcher. What new skills does this project allow me to develop? Are there new and exciting approaches that I can attempt? Will I have autonomy to work on this? Throughout this book, we will concretely illustrate the value of implementing data analytics. 1.5 Structure This book is designed to be a holistic resource, bringing importance to both the technical and qualitative aspects of data science. Each chapter introduces complex technical concepts in humanized contexts before proceeding to the math and coding. We will begin in Chapter 2 by providing a common approach to initiating and delivering data analytics projects as well as setting expectations of what to expect. We then proceed to Chapters 3 and 4 that introduces statistical programming practical applications. Chapters 8 through 16 cover foundations for machine learning and statistical modeling for applied applications. As this book is designed to function not only as a textbook but also as a handbook, each of the machine learning and statistical modeling chapters contain a Do It Yourself (DIY) that poses common types of problems in plain English and provides an “empirical recipe” to solve for it through worked examples (see below). Chapters 17 through 19 focus on additional considerations for data analytics excellence, including presentation of model outputs, ethics considerations, and further skills development to remain competitive in the market. Chapter 2: Light Programming How to read a file direct from the web? Chapter 3: Manipulation / Wrangling How do I auto-populate text and stences pro forma? How much [overlap/commonality/competition] is there between two or more groups? What do I do find trends in transactional or event-level data? How do I extract some basic keywords from unstructured text data? Chapter 4: Exploratory Data Analysis What’s a common exploratory data analysis workflow? Chapter 10: Number Problems What level of [demand/staff] should I expect? When is consistently peak time of the [year/day/month]? How much is customer willing to pay for a specific [attribute]? What’s the best way to fill-in missing [data/characteristics]? Does that look a little [high/low] to you? Where will demand be this time [next year/tomorrow/ next month]? Chapter 11: Discrete Will A or B […or C, …] happen? Who will likely say yes to a [pitch/offer/request/service]? Who has not yet been [converted/sign-on/covered]? What other [goods/resources] would a customer like? What is the profile of someone who [uses X, does y]? Which images contain [x]? Chapter 12: Clustering What are the profiles of [customers/people/events]? What is the underlying pattern of this list of [people/events]? How do I extract the outline from an image? Chapter 13: Survival How long will this [machine/infrastructure/person] last?- worked example = business dynamics from census Which [organization] has the most turn over? = City gov employee data Which [machine/patient/] are at risk of repeat [failure/admission]? Chapter 14: Explanatory What [factors/variables/influences] matter the most? What was the impact of implementing a new [program/policy/strategy]? Chapter 15: Anomalies When was there a paradigm shift? Does the [network traffic/activity] look irregular? Which [person/event/organization] do we understand the least about? Chapter 16: Text What topics are present in this text? How does these [text/comments/article/tweet] lean? https://cityroom.blogs.nytimes.com/2007/08/18/2-firefighters-are-dead-in-deutsche-bank-fire/↩ http://www.nydailynews.com/news/firefighters-dead-7-alarm-deutsche-bank-blaze-article-1.238838↩ http://www1.nyc.gov/assets/doi/downloads/pdf/pr_db_61909_final.pdf↩ http://www.nfpa.org/news-and-research/publications/nfpa-journal/2014/november-december-2014/features/in-pursuit-of-smart↩ https://www.nist.gov/publications/research-roadmap-smart-fire-fighting↩ Adams, D. Hitchhiker’s Guide to the Galaxy. New York: Harmony Books, 2004.↩ http://www.npr.org/sections/thetwo-way/2017/08/04/541562855/u-s-economy-adds-209-000-jobs-in-july-unemployment-dips-to-4-3-percent↩ https://undergrad.admissions.columbia.edu/classprofile/2020↩ https://www.washingtonpost.com/news/wonk/wp/2013/10/29/heres-zillows-strategy-for-dominating-online-real-estate/?utm_term=.ca2d6094674e↩ https://www.washingtonpost.com/news/wonk/wp/2017/07/13/shoplifting-in-chicago-dropped-after-a-change-in-the-food-stamps-program/?utm_term=.6bfe5a75d1fc↩ http://fortune.com/2012/07/30/amazons-recommendation-secret/↩ https://www.technologyreview.com/s/509026/how-obamas-team-used-big-data-to-rally-voters/↩ https://www.vox.com/conversations/2017/3/16/14935336/big-data-politics-donald-trump-2016-elections-polarization↩ http://www.newsweek.com/how-satellite-surveillance-helping-predict-stock-prices-skynet-562973↩ http://www.satellitetoday.com/technology/2017/08/02/using-artificial-intelligence-track-illegal-activities-sea/↩ https://www.wired.com/story/penny-an-ai-that-predicts-a-neighborhoods-wealth-from-space/↩ A. M. Turing (1950) Computing Machinery and Intelligence. Mind 49: 433-460.↩ https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/↩ http://www.bbc.co.uk/timelines/zq376fr#z2c6nbk↩ von Ahn, Luis; Blum, Manuel; Hopper, Nicholas J.; Langford, John (May 2003). CAPTCHA: Using Hard AI Problems for Security. EUROCRYPT 2003: International Conference on the Theory and Applications of Cryptographic Techniques.↩ Bengio, Yoshua; LeCun, Yann; Hinton, Geoffrey (2015). “Deep Learning”. Nature. 521: 436–444.↩ https://www.forbes.com/sites/gilpress/2013/08/19/data-science-whats-the-half-life-of-a-buzzword/#3caa5cc57bfd↩ "],
["a-light-introduction-to-programming.html", "Chapter 2 A Light Introduction To Programming 2.1 Doing visual analytics since 1780’s 2.2 Programming to democratizing skills 2.3 How programming languages work 2.4 Setting up R 2.5 A Gentle Introduction to R 2.6 DIY", " Chapter 2 A Light Introduction To Programming 2.1 Doing visual analytics since 1780’s The modern line chart is something that most people take for granted. Grade school arithmetics class typically requires students to learn to haphazardly draw a two dimensional axis, then plot points at specific coordinates in the Cartesian space, then connect the points using line segments. This simple, routine task was once an innovation. In 1786, a Scottish engineer named William Playfair published Commercial and Political Atlas, an atlas that described economic and political relationships of the 18th century. The impressive book introduced the first line and pie charts, skillfully hand colored and narrated to help readers see economic patterns Granted, the patterns were correlative and circumstantial rather than proven through statistical means, but it was the beginning of graphical methods of statistics – gathering information in a common, comparable form in order to visually identify patterns. For example, in a balance of trade time series chart, England’s imports and exports to and from Denmark and Norway are plotted for the periods of 1700 to 1780. Many features of the chart are quite striking such as a naturally smooth pair of lines for each imports and exports between which two different colors are used to illustrate trade balances in favor and against England. Figure 2.1: Playfair’s balance of trade time-series chart as found in the Commercial and Political Atlas, 1786 In present day, we tend to take the construction of effective statistical visuals for granted. But in Playfair’s time, a three field data set with 240 data elements (\\(\\text{80 years x 3 fields}\\)) was big data. Furthermore, scaling and disseminating this work would have been an arduous process requiring typesetting and colororation by hand – a significant time investment. 2.2 Programming to democratizing skills With the advent of computer programming, the manual mastery of drafting charts became democratized to the masses in the form of software. A new type of engineer emerged – the software engineer – leveraging specialized computer programming languages to instruct machines to undertake user specified tasks. Formerly arduous tasks started to become simplified and genericized. In many ways, each programmed task is designed to imitate an expert’s abilities: inputs such as data are transformed by a computer following a series of pre-defined steps, returning a specific intended result. Code often requires users to be explicit about assumptions, which may require deeper expertise (e.g. what scale should the chart be, what color palette makes sense). An engineer can set sensible defaults that use best practices to fill the user’s knowledge gap, thereby reducing intellectual overhead and allow easier use of software. This enables even novice analysts with little experience to produce charts in which software automatically applies graphical styles while allowing some degree of customization. In a matter of mere minutes, an analyst can produce a smartly designed chart using the collective knowledge of visualization designers and developers. Most people will only need user-friendly, click-based software, catering to a common set of tasks that may need to be done with on a daily operations. But for others, programming is a means of exploring new frontiers – to create and test ideas that may change paradigms or make operations even more efficient. Creating something new and higher level often requires a degree of flexibility that moves beyond common tasks. Imagine the following scenarios: An analyst is responsible for putting together forecasts, but requires five days to click through his software to refresh data and update assumptions. Every month, an outreach coordinator is responsible for pairing new mentors with mentees. The process of manually matching hundreds of people based on their surveyed interests usually takes two months to complete per cohort. Satellite imagery can be used to support intelligence decisions – perhaps in the form of counting buildings or vehicles on the ground. With 196.9 million square miles of area on the Earth’s surface, a massive army of image analysts would need to inspect a daily pipeline of images to extract usable information. Although proven time and time again, manual tasks may be repetitive and may be streamlined. The forecast analyst could write a script that automates the refresh and model calibration. The outreach coordinator could benefit from a program that recommends matches based on digital survey results. The satellite imagery review could be automated using the latest in computer vision technology. Indeed, programming may give way to new softwares that may bring seemingly esoteric tasks into the mainstream. 2.3 How programming languages work In this book, there is much reference to Lambourghinis. They are exciting machines (for some) as the possibilities on the road are endless. It is frightening to others as the sheer speed, expense, and responsibilities are too great. To even develop these sentiments, one needs to first understand what cars are, how are vehicles controlled, how they can be used, what is needed to make use of them, the laws that govern the road, etc. There are many parallels between driving and programming. For those who are new to programming, it is likely that prior experience with software has largely been through user interfaces (UI). UIs typically involve a graphical representation displayed on a screen allowing a user to interact through touch or mouse – selecting buttons in order to perform actions. In many respects, this is the same as all the levers, buttons, knobs, and pedals found in a car driver’s seat: every control is associated with some function, whether accelerating the car or opening the windows. In both cases, in order to accomplish a specific task (e.g. driving from A to B, processing data from form A into form B), the user needs to be explicit. With the exception of self-driving cars, drivers need to regulate the accleration of the car with both gas and brake pedals as well as turn the steering wheel in a coordinated, well-timed series of rotations for some amount of time until the destination is reached. The user actions are translated into signals that a car can understand. For example, when a driver presses down on a gas pedal, a signal is inputted into an eletronic throttle control that interprets the driver’s motion into an amount of fuel that is injected into the engine. The result: acceleration. Having the interpretative layer is a necessity. The alternative could be something quite farcical, requiring the driver to haphazardly squeeze a syringe of fuel into the engine. Computer programming runs along a parallel. Whereas cars have an engine, a computer’s Central Processing Unit (CPU) is essentially a collection of switches that flip on and off. A combination of switches equate to different outcomes, such as displaying a visual or calculated values from an equation. In order to will a computer to do thy bidding, the computer needs to be provided instructions in its own machine language or code that a computer can understand. A programming language is simply a translator between what a user wants to do and presenting those instructions into machine language. The code that is written by the user is known as the source code. In short, computers do not use plain English syntax nor can they guess what the user wants – everything needs to be explicit. The underlying systems that allow gasoline cars, electric cars, and hybrid cars to accelerate differ from one another. All three types of cars are viable methods of transport, but accomplish the task under different assumptions and requirements. Programming languages are no different and generally can be grouped into three types: Compiled languages rely on a compiler program to translate source code into binary machine code, which on its own is self-contained and runnable. After translation, the resulting binary code is contained and can be executed without any reliance on the source code. In data science, a common compiled language is C. As the code is compiled into binary once the source code is completed, the performance tends to be faster. Interpreted languages rely on an interpreter program to execute source code. Unlike compiled languages, interpreted languages are fully reliant on the interpreter program, meaning that each time a user wants to take an action, the source code needs to be re-run and interpreted through the program. The result is a slower language relative to compiled languages. In data science, a common interpreted language is R. Pseudocode languages (p-code langauges) are hybrids between compiled and interpreted languages. The source code is compiled into bytecode – a compact instruction set that is designed for specialized, efficient interpreters when executed. A common example of a p-code language is Python. Where does the code go? Code Editors. All of the above languages follow a paradigm: write code, then execute it. Writing code requires a code editor, a simple tool that can be used to store and edit code. An editor can be as simple as Notepad on Windows or TextEdit on Macs. Practitioners tend to use more specialized code editors that make the coding process more efficient and effective, often times including functions like auto completion and improved readability. These include Sublime Text, Brackets, Notepad++ (Windows Only), Atom, among others. After code is written, it needs to be executed following the type of language. For code written for compiled languages, for example, the code needs to be translated into binary via a compiler, which is not part of the code editor. Brackets is one of many open source code editors that supports a broad range of programming languages. Integrated Development Environments (IDEs). For some languages, IDEs are an all-in-one tool that make software development simpler, containing a source code editor, a compiler or interpreter and a debugging capabilities all in one screen. For example, there are a number of IDEs for Python, such as PyCharm, PyDev + Eclipse, Spyder, and Rodeo. For the R statistical programming language, RStudio is the most used IDE. A screencap of RStudio while being used to write this chapter. Quite meta. What does it mean for data science? Data science principally involves handling, well, data. And all of the above types of languages are up to task. Two languages have risen above the rest: R and Python. Both languages can produce charts that would make Playfair proud, but modern day Playfairs need to be able to have a strong command over the all quantitative techniques that have emerged since the 1780’s. Users of each language can conduct similar tasks, but tend to differ in their aims: R is a common choice language of statisticians and data-intensive researchers as it is flexible for data science and mathematical experimentation. Whether machine learning or any quantitative task, R is optimized specifically for all things data. Any data task can be easily handled using the language’s basic commands and augmented by thousands of code libraries – openly available sets of functions that facilitate more complicated tasks. The language does not naturally lend itself to the development of web applications, although there have been recently introduced software that are designed to run R code as standalone applications. Python is a great all-around scripting language that is common in software engineering environments – typically focused on building and deploying web applications such as websites and Application Programming Interfaces (APIs). A key feature of Python is its focus on simplicity and readability. Whereas R is focused on statistics, Python is reliant on a broad range of libraries to handle basic data tasks. Which one language should a beginner choose first? The decision rests on the tasks that one wishes to accomplish. For prototyping and testing statistical ideas, learning R is a sound choice. It is more than enough to draw statistical conclusions, visually communicate insights, and grow a data science practice. If the nature of projects is more ad hoc as is typical of new service-oriented organizations, then start with R. For cases where deploying software is a necessity – often times more product-oriented organizations, starting with Python makes more sense. A data scientist can start with one language, then learn the other as well. In the field, there is a rather farcical rift between R and Python users. There are many R projects that draw superb insights, but do not garner any support. There are plenty of Python software projects that are put into production, but see no use. Ultimately in the realm of data science, fundamentally understanding the problem is king and everything else follows. 2.4 Setting up R As the author is a statistician, all programming herein is illustrated in the R programming language. To get started, the only installation is required to take advantage of R is found on the Comprehensive R Archive Network website. For ease of development, though not necessary to run R code, an IDE known as Rstudio should be installed. 2.4.1 Installation Installation is a two step process. First, visit the CRAN website: Visit R Click on ‘Download R’ for your computer’s operating system or OS (Windows, Linux or Mac) Download the latest .pkg file that your OS can support Open and install the .pkg file Upon installing R, open the software. The software should display the console, which is a command-line interpreter (left). Human-written source code can be directly run in the console, which then is interpreted into machine language. In addition, the buttons at the top of the interface provides options to write and edit code in a script editor. . Secondly, to make life easier when coding in R, install RStudio. Note that RStudio requires R, but R does not require RStudio. Visit RStudio Click on the Installer that corresponds to your computer’s operating system or OS (Windows, Linux or Mac) Open and follow install instructions. Open RStudio. During the first time, the program will prompt require a version of R to be selected. There should be a four pane layout: Like plain vanilla R, RStudio contains a script editor and console. The code can be directly executed from the script editor, passing commands to the console. As code is run, there may be inputs and outputs that are temporarily kept in the computing environment. The environment pane provides an inventory of the types of objects that currently exist in the environment. Some outputs of the code may be charts, which are rendered in the plot pane. screencap of R console 2.4.2 Justifying open source software If you are working in an organization that is in the nascent stages of using data, it is more than likely that you will need to convince executives, IT managers, your supervisor, co-workers among others that you need access to analytics software. Many will direct the bold and ambitious back to MS Excel as it can indeed cover many fundamental tasks and is inexpensive. But consider this: open source software like R is free and open, and is far more robust, flexible and powerful than any WYSIWIG. But when considering why new open source software does not usually catch on, consider the following strategies: Cost. The cost of software is often cited as a barrier to use. Fortunately, open source software such as R and Python tend to be free to use and accessible by anyone. Since there is no cost barrier, there are world-wide communities of practitioners and experts who build new software from open source code bases, which means cost does not prohibit collaboration. New to modern technologies. Organizations that are newcomers to new open source coding languages may simply be intimidated by the idea of programming. People do not usually like to admit that they are not well-versed in an area of knowledge. Consider illustrating the capabilities in a non-threatening manner – build tangible graphs and visuals that illustrate the promise of using a new open source coding language in non-technical terms while de-emphasizing your technical wizardry. Cybersecurity. Chief Innovation Officers (CIOs) and Chief Information Security Officers (CISOs) will want to know the security of servers will be protected given that an open source software is acquired from the other side of the firewall. With open source software with large robust communities, there are far more people scrutinizing the source code than a single company’s proprietary software. Thus, it is reasonable to argue that cyber vulnerabilities have a higher chance of detection with a larger, dedicated user base. Customer support. Like life insurance, some will want to buy peace of mind when buying a commercial, closed source software. Thus, open source softwares are not typically furnished with dedicated, reliable technical support. In many respects, open source is a matter of self-reliance and communal reliance. Do not quote Ralph Waldo Emerson, but rather advocate for a pilot period where low-risk projects are conducted using an open source software. If worse comes to worse, there are a variety of enterprise-grade open source softwares available for R and Python. 2.5 A Gentle Introduction to R Programming turns human thought into well-defined, structured information that can be actted upon. Let’s take the following quote from a August 2017 CNBC online article about stock performance: The Dow Jones industrial average erased earlier losses to end 56.97 points higher at 21,865.37,… When reading the statement, our brains recognize and hold onto information for future use. Just in a 104 character sentence, we can see that: - the Dow Jones industrial average is a series of characters or a string that is associated with a stock market indicator; - the Dow experienced a rate of change 56.96 – a numerical value – that is measured relative to a magnitude 21,865.37 (another numerical value); - there was indeed growth in the Dow and can be recorded simply as TRUE In programming, each of the above values is a variable – a storage location for a particular value that is associated with an identifier. In R, we can represent the above information in the following matter: indicator &lt;- &quot;Dow Jones Industrial&quot; change &lt;- 56.96 close &lt;- 21865.37 growth &lt;- TRUE There are four variables (indicator, change, close, growth) each with different identifiers. Notice that each value is being named or “assigned” using &lt;-. To check that the assignment worked, we’ll use print() to write indicator and change to the console output. print(indicator) ## [1] &quot;Dow Jones Industrial&quot; print(change) ## [1] 56.96 indicator is set as a string value (charactes) and change is a numeric. In statistial programming, there are many different data types, including: - numeric (e.g. 1, 3.14, 123) - string or characters (e.g. “text goes here”) - factors (e.g. unique levels) - boolean (e.g. FALSE, TRUE) - dates (e.g. 12/6/2016, 2014-03-20) Thus, a variable can hold values of any data type and be overwritten with virtually anything: #string people &lt;- &quot;people&quot; #factor people &lt;- factor(&quot;people&quot;) #boolean people &lt;- FALSE #dates people &lt;- as.Date(&quot;12/6/2016&quot;,&quot;%m/%d/%Y&quot;) Variables can also hold more than one value. In the example below, we overwrite people as a vector, or a sequence of data elements of the same data type. people &lt;- c(134, 542, 324, 102, 402, 383, 853) #set x as a vector print(people) ## [1] 134 542 324 102 402 383 853 Let’s assume that each element of vector x represents a different field office’s productivity with serving constituents and we wanted to print out a simple summary. To do this, we’ll rely on the sum() function to add all values in the vector x, length() to count the number of offices, and paste() to concatenate all the objects into one string object. tot_people = sum(people) #Sum of x num_offices = length(people) #count number of elements units1 &lt;- &quot;satisfied constituents served&quot; #string variable units2 &lt;- &quot;field offices!&quot; #another string variable statement &lt;- paste(tot_people, units1, &quot;in&quot;, num_offices, units2) print(statement) ## [1] &quot;2740 satisfied constituents served in 7 field offices!&quot; 2.5.0.0.1 Quick Exercise Using the people and units variables, write a command that lists the number of people people served per office. 2.5.1 Operators Numerical Operators Numerical values can be modified, manipulated, and combined using operations such as addition (+), substraction (-), multiplication (*), division (/), exponent (**) and modulus (%%). These are the foundations of many analytical operations. 4 + 2 #Addition ## [1] 6 4 - 2 #Substraction ## [1] 2 4 * 2 #Multiplication ## [1] 8 4 / 2 #Division ## [1] 2 4 ** 2 #Exponent ## [1] 16 4 %% 2 #Modulus ## [1] 0 These operators are scalable. If we want to find the number of customers if we wanted to double productivity or half productivity, then it’s like multiplying a scalar to every row in a given column of a spreadsheet. It’s far easier to do this in R using one line of code. ##Double doubled &lt;- people * 2 print(doubled) ## [1] 268 1084 648 204 804 766 1706 ##Half halved &lt;- people / 2 print(halved) ## [1] 67.0 271.0 162.0 51.0 201.0 191.5 426.5 Logical operators Let’s say that you need to flag values based on specific values or specific thresholds. This task would require the use of logical operators, which evaluate a statement and return a boolean that indicates if a statement is TRUE. Below, we compare two quantities: y and x x &lt;- 10 y &lt;- 2 x &gt; y #4 is greater than 2 ## [1] TRUE x &gt;= y #4 greater than or equal to 2 ## [1] TRUE x &lt; y #4 is less than 2 ## [1] FALSE x &lt;= y #4 less than or equal to 2 ## [1] FALSE x != y #4 is not equal to 2 ## [1] TRUE x == y #4 is equal to 2 ## [1] FALSE Using the operators, we can also see how many elements in a vector meet a specific criterion. For example, to flag which records in people are over 200, we can do the following: above &lt;- people &gt; 200 print(above) ## [1] FALSE TRUE TRUE FALSE TRUE TRUE TRUE Notice that the logical operator returned a vector that evaluate the statement for each element in people. To see the split of TRUE vs. FALSE, we an use the table() function to tabulate the number of records in each unique data value. table(above) ## above ## FALSE TRUE ## 2 5 2.5.2 Data classes Data classes are ways to store data in an efficient and usable manner within the R programming environment. These differ from data storage formats (e.g. CSV, JSON), which are widely used regardless of programming language. Each data class is designed with different functions in mind and make certain tasks more efficient. In this section, we’ll cover vectors, lists, matrices, and data frames. Lists [Add section on lists] Vectors As mentioned previously, vectors are sequences of data elements of the same data type. Below, there are three vectors: cities is a string vector whereas pop and area are numeric vectors. cities &lt;- c(&quot;New York&quot;,&quot;Los Angeles&quot;,&quot;Chicago&quot;,&quot;Houston&quot;,&quot;Philadelphia&quot;) pop &lt;- c(8175133, 3792621, 2695598, 2100263, 1526006) area &lt;- c(302.6, 468.7, 227.6, 599.6, 134.1) To extract specific values in a vector, we will need call values by their index position in the list of values. Unlike other programming languages, R index values start from 1 rather than 0. Thus, in order to obtain the first value in all three vectors, one could write the following command: cities[1] ## [1] &quot;New York&quot; pop[1] ## [1] 8175133 area[1] ## [1] 302.6 Using the index values to extract values is quite useful and can be combined in many ways: #Obtain first and fourth cities cities[c(1,4)] ## [1] &quot;New York&quot; &quot;Houston&quot; #Obtain the third through fifth cities cities[3:5] ## [1] &quot;Chicago&quot; &quot;Houston&quot; &quot;Philadelphia&quot; #Use criterion to obtain populations greater than 2 million pop[pop &gt; 2000000] ## [1] 8175133 3792621 2695598 2100263 Matrices Vectors can be combined into into a matrix using cbind(). Matrices are data elements arranged into a n row by m column rectangular layout that allow for easier multi-dimensional data manipulation. Note that all data elements, regardless of the location in a matrix, are of the same data type (e.g. numeric, string, boolean) and take on the data type of the first column of data unless specified otherwise. mat &lt;- cbind(cities, pop, area) In this case, pop and area take on the data type of cities. This can be limiting if an analysis requires multi-dimensional, multi-data type data. But there benefits over vectors, namely the ability to easily and concisely extract complete multiple variable sets of data. #Extract first row mat[1,] ## cities pop area ## &quot;New York&quot; &quot;8175133&quot; &quot;302.6&quot; #Extract second column mat[,2] ## [1] &quot;8175133&quot; &quot;3792621&quot; &quot;2695598&quot; &quot;2100263&quot; &quot;1526006&quot; #Extract 2nd to 3rd rows and the 1st and 3rd column mat[2:3,c(1,3)] ## cities area ## [1,] &quot;Los Angeles&quot; &quot;468.7&quot; ## [2,] &quot;Chicago&quot; &quot;227.6&quot; Data frames If a dataset will have more than one data type, consider using data frames. To create a data frame, use the data.frame() to put two or more vectors together. df &lt;- data.frame(city = cities, population = pop, area = area) One of the properties of data frames is the ability to set, manage and use variable names, which is not available when using matrices. #Being able to call a variable by name using the compact $ operator df$cities ## NULL df$population ## [1] 8175133 3792621 2695598 2100263 1526006 #Check column names colnames(df) ## [1] &quot;city&quot; &quot;population&quot; &quot;area&quot; #Reassign column names using a vector of colnames(df) &lt;- c(&quot;city_name&quot;,&quot;pop&quot;,&quot;area&quot;) #Create a new boolean variable for populations greater than 2.5m and store into data frame df$over_2m &lt;- df$pop &gt; 2500000 #View results print(df) ## city_name pop area over_2m ## 1 New York 8175133 302.6 TRUE ## 2 Los Angeles 3792621 468.7 TRUE ## 3 Chicago 2695598 227.6 TRUE ## 4 Houston 2100263 599.6 FALSE ## 5 Philadelphia 1526006 134.1 FALSE If there is a existing matrix object, as.data.frame() an be used to convert the objet into a data frame. However, this method will preserve the uniform data type of the matrix, which would require manually setting the data type for each variable. new_df &lt;- as.data.frame(mat) Basic Manipulation [Add section on indices] Tips We can check the structure of any data object in R using the str() method, which returns the following for a given data object: - the data format if vector, or class if matrix or data frame - dimensions of the data: number of rows if vector, matrix dimensions [rows, columns], or number of variables and rows if data frames - the first 5 values of the object along with the data format #vector str(cities) ## chr [1:5] &quot;New York&quot; &quot;Los Angeles&quot; &quot;Chicago&quot; &quot;Houston&quot; ... str(pop) ## num [1:5] 8175133 3792621 2695598 2100263 1526006 #matrix str(mat) ## chr [1:5, 1:3] &quot;New York&quot; &quot;Los Angeles&quot; &quot;Chicago&quot; &quot;Houston&quot; ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:3] &quot;cities&quot; &quot;pop&quot; &quot;area&quot; #data frame str(df) ## &#39;data.frame&#39;: 5 obs. of 4 variables: ## $ city_name: Factor w/ 5 levels &quot;Chicago&quot;,&quot;Houston&quot;,..: 4 3 1 2 5 ## $ pop : num 8175133 3792621 2695598 2100263 1526006 ## $ area : num 303 469 228 600 134 ## $ over_2m : logi TRUE TRUE TRUE FALSE FALSE Other tools include dim(), class(), and typeof(), which provide the object dimensions, the class of data object and the type of data objet, respectively. These are handy for understanding basic attributes of any data and can inform the architecture of your data science approach. dim(df) ## [1] 5 4 class(df) ## [1] &quot;data.frame&quot; typeof(df) ## [1] &quot;list&quot; 2.5.3 Libraries: Expanded functionality While R comes with built-in functionality, higher order functions that facilitate more efficient and effective programming may require one to install and load code libraries – user-contributed functions that extend programming languages and are furnished with standardized documentation to help programmers understand their functionality. As R is an open source language, anyone can write and contribute code libraries for general public use. In fact, as of Dec 2016, there are 9,749 contributed packages on the Comprehensive R Archive Network (CRAN). Some of the most used libraries include: ggplot2 – a graphing visualization library dplyr – a data manipulation and processing library caret – a multi-faceted machine learning library stringr – a library for character or string processing lubridate – a library to handle date data sqldf – a library to write SQL code in R Libraries are common among programming languages, such as Python, Java, Julia, and JavaScript. 2.5.3.1 Install and load a package To start, we will install and load ggplot2, one of the most used graphing libraries. Installation is simple enough, using the install.packages() command: install.packages(&quot;ggplot2&quot;) To use the library, use library() to load ggplot2. library(ggplot2) Upon doing so, all is needed is the data in the right form and a basic understanding of ggplot2 syntax (take a look here). 2.5.4 Input/Output (I/O) Input/Output or I/O is the process of having a data scientist communicate with a computer to act on some piece of information. This may involve understanding how to input parameters into a function in order to obtain a desired output. More commonly, I/O from a data perspective is import and export of data into a programming environment. It is important to understand that data is captured in many data formats – or standardized structures for storing data. In the social sciences, data is often requested in comma separated values (.csv), tab separated values (.tsv) or other tabular flat files. Notice that there is one row per observation where the header (first line) contains the names of each of the fields, then values come every row thereafter. Earth science and atmospheric sciences, HDF and NetCDF are commonly used to store highly dimensional (many variables) gridded data. In internet-based technologies, data is often recorded and transmitted in JavaScript Object Notation (JSON) and Extensible Markup Language (XML). Data science is agnostic of formats. The table below provides a selection of common data formats along with an example of how they are rendered and their use. Abbreviation Name + Description Type Example Fields of Use CSV Comma Separated Values Tabular state, pop_mil California, 38.8 Delaware, 0.936 Widely used format in most fields TSV Tab Separated Values Tabular state pop_mil California 38.8 Delaware 0.936 Widely used format in most fields JSON JavaScript Object Notation Tabular [{“state”: “California”, “pop_mil”: 38.8},{“state”: “Delaware”, “pop_mil”: 0.936}] A common format provided from APIs GeoJSON GeoJSON Spatial JSON format with additional metadata parameters that allow geographically referenced data to be used. Common format for web-based GIS XML Extensible Markup Language Tabular &lt;data&gt; &lt;rec&gt; &lt;state&gt;California &lt;/state&gt; &lt;pop_mil&gt;38.8 &lt;pop_mil&gt; &lt;/rec&gt; &lt;rec&gt; &lt;state&gt;Delaware &lt;/state&gt; &lt;pop_mil&gt;0.936 &lt;pop_mil&gt; &lt;/rec&gt; &lt;\\data&gt; Widely used format in most fields SHP Shapefile Spatial Relies on a combination of four files in order to render map objects: .shp, .shx, .sbn, .dbf Common format for Geographic Information Systems (GIS) data TIFF Tagged Image File Format Spatial + Imagery Common format for representing grid-based HDF Hierarchical Data Format Spatial + Imagery Common format for representing grid-based NetCDF Network Common Data Form (NetCDF) for sharing array-oriented scientific data Spatial + Imagery Common format for representing grid-based It is clear that the formats look quite different and thus each require different libraries to import and export data.For example, Excel files can be opened using the gdata and read_xl libraries. CSV files can be managed using built-in functions. JSON files can be opened using jsonlite, rjson among others. 2.5.4.1 I/O by format Given the various formats, importing data is reliant on a number of specialized functions. In most programming languages, there will be one or more specific functions that are specially designed to handle a given data format. .csv, .tsv, .xml, .json, .xlsx Each of these data formats are often used to store tabular data – or data that fits into a neat flat table, though noting that json and xml can store complex, nested data. We demonstrate this basic functionality using an abridged list of documents that was released in 2017 relating to the assassination of JFK. The abridged list is hosted at https://s3.amazonaws.com/dspp/. Table 2.1: Selection of records from the 2017 JFK Assassination Record Release file.name record.num nara.release.date formerly.withheld agency doc.date doc.type DOCID-32330250.pdf 124-90089-10285 2017-10-26T00:00:00Z In Part FBI 05/07/1959 PAPER, TEXTUAL DOCUMENT DOCID-32348429.PDF 104-10069-10076 2017-07-24T00:00:00Z In Part CIA 07/14/1970 PAPER - TEXTUAL DOCUMENT DOCID-32977063.pdf 145-10001-10167 2017-10-26T00:00:00Z In Part DOCID-32392200.PDF 104-10413-10043 2017-07-24T00:00:00Z In Part CIA 04/26/1963 PAPER - TEXTUAL DOCUMENT 104-10194-10006.pdf 104-10194-10006 2017-07-24T00:00:00Z In Full CIA 01/01/0000 PAPER - TEXTUAL DOCUMENT Built-In Functions for Delimited File R has a number of built-in functions for data in character-separated values. The most generalizable are read.table() for import and write.table() for export. Below, read.table() is applied to each .csv and .tsv, importing a new data frame object and assigning it to a data frame named df. Without assigning the data frame, the imported data simply is printed to console (e.g. output on a screen, not saved). The first parameter is the path and file to be imported and the sep parameter indicates the type of delimiter. df &lt;- read.table(&quot;https://s3.amazonaws.com/dspp/jfk.csv&quot;, sep = &quot;,&quot;) df &lt;- read.table(&quot;https://s3.amazonaws.com/dspp/jfk.tsv&quot;, sep = &quot;\\t&quot;) In the case that the header is not in the first row, the skip parameter can be included to indicate how many lines should be skipped until the header is found. In other cases, headers are not available and header = FALSE should be specified. #skip df &lt;- read.table(&quot;https://s3.amazonaws.com/dspp/jfk.tsv&quot;, sep = &quot;\\t&quot;, skip = 2) #no header df &lt;- read.table(&quot;https://s3.amazonaws.com/dspp/jfk.tsv&quot;, sep = &quot;\\t&quot;, header = FALSE) In order to export a data frame to a delimited file, use the write.table() command. Be sure to specify the delimiter sep and row.names = FALSE as the row index number will otherwise be written to file. write.table(df, &quot;jfk-2.tsv&quot;, sep = &quot;\\t&quot;, row.names = FALSE) write.table(df, &quot;jfk-2.txt&quot;, sep = &quot;[virtually anything can go here, but keep it short and unique]&quot;, row.names = FALSE) CSV-specific functions are commonly used for import – read.csv() – and export – write.csv(). The difference is that the delimiter is automatically assumed to be a comma. df &lt;- read.csv(&quot;https://s3.amazonaws.com/dspp/jfk.csv&quot;) write.csv(df, &quot;jfk-2.csv&quot;, row.names = FALSE) Built-In Functions for Reading Lines Not all data will be well-structured. In fact, when considering datasets of vast amounts of text, data often needs to be read line by line without assuming defined delimiters. In some cases, delimiters will be corrupted or misplaced, which may complicate importing data by requiring changes to the data in a raw form. In R, importing raw lines of data can be accomplished using readLines() and exporting lines via writeLines(). df &lt;- readLines(&quot;https://s3.amazonaws.com/dspp/jfk.csv&quot;) writeLines(df, &quot;jfk.txt&quot;) Upon importing data, any number of manipulations can be applied to the data. One Library, Many Formats As previously noted, there are many other formats that are commonly used beyond .csv and .tsv, such as .xml, .json, .xlsx, .dta, .fwf, .csv.gz, .mat, .yml, among others. In the past, to import any of the above-enumerated formats required knowledge of multiple libraries, but the process has been simplified for “flat” data sets where there is no hierarchy or nested relationships. We recommend rio – a R library designed for simplified I/O drawing from multiple popular libraries. Rather than learning the syntax specific to an xml library or xls library, rio boils I/O to two simple functions: import() and export(). # load library library(rio) # example import path &lt;- &quot;https://s3.amazonaws.com/dspp/&quot; df &lt;- import(paste0(path,&quot;jfk.json&quot;)) df &lt;- import(paste0(path,&quot;jfk.xml&quot;)) df &lt;- import(paste0(path,&quot;jfk.csv&quot;)) df &lt;- import(paste0(path,&quot;jfk.xlsx&quot;), sheet = 1) # example export export(df, paste0(path,&quot;jfk.json&quot;)) export(df, paste0(path,&quot;jfk.xml&quot;)) export(df, paste0(path,&quot;jfk.csv&quot;)) export(df, paste0(path,&quot;jfk.xlsx&quot;)) For some formats, additional arguments are required. For example, to import an .xlsx, the user will need to indicate the index number of a worksheet in a workbook (e.g. 1 for first worksheet, 10 for the tenth worksheet, etc.). Throughout the remainder of this book, we will use read.table(), read.csv() and import() interchangeably. Spatial/GIS The role of spatial data is growing in prominence, thus having some familiarity with spatial data will no doubt become commonplace whether to help transform and process data into a usable form or visualize geographic patterns. This will be covered more extensively later in this book, but here, we provide a cursory overview. Spatial data is generally divided into two types: rasters and vectors. Rasters are gridded data – like satellite image composite of the Earth – that have measurements taken at regular intervals that correspond to geographic coordinates. Often times, gridded data is commonly recorded in TIFF, JPEG, NetCDF, among other formats. Raster image of the Earth at Night (2012) as seen from the Suomi NPP satellite Vectors on the other hand are geometric representations of geography recorded in the form of points, lines, and polygons. How this information is stored is somewhat more complicated than rasters, stored across three files that work as one. Suppose that a town has a triangular town boundary that is represented using a set of three coordinates: .shp: the geometric data – coordinates – are recorded as an observation in a geometry file; .shx: the file that indexes the shape geometries – useful for sorting and storage; .dbf: the file that contains all attributes associated with each geometry in the .shp file. Vector of North American states and provinces For brevity, we demonstrate how to load a shapefile of US Congressional Districts as published by the US Census Bureau23. To download the 115th Congress’ districts shapefile, use this link: http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_cd115_500k.zip and unzip the file. Upon inspection of the folder contents, notice that all files have the same file name “cb_2016_us_cd115_20m”, but different extensions. Files that comprise a shapefile Next, install and load the rgdal package, then use the readOGR() function to read in the shapefile. readOGR() requires two parameters: dsn is the path to the folder containing the shapefile, and layer is the shapefile name without extension. library(rgdal) shape &lt;- readOGR(dsn = &quot;your/path/goes/here&quot;, layer = &quot;cb_2016_us_cd115_20m&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/var/folders/sm/bx0lfjp93k357wbghyshyfy80000gp/T//RtmpI0wkp9&quot;, layer: &quot;cb_2016_us_cd115_20m&quot; ## with 437 features ## It has 8 fields ## Integer64 fields read as strings: ALAND AWATER With the data loaded, we can easily view the geometries and begin to manipulate for analytical uses. plot(shape) An Efficient R-Specific Format As files grow large, reading and writing data from a file will consume more time and sometimes make workflows inefficient. Imagine writing a 10 GB file to disk – a relatively modest data file by data science standards – then re-reading it into memory each time it is required. The architects of R have developed a clever format that compresses any R objects, whether data frame or models or other, into a .Rda file. This is done using the save() function, specifying a series of R objects already in the R environment, then saving to a file. For example, if the current R environment contained a data frame object called df and a vector called vec, both objects can be saved together in a file named example.Rda. The resulting file compresses the data in order to reduce the storage footprint. save(df, vec, file = &quot;example.Rda&quot;) To read the file back into memory, simply use the load() command: load(&quot;example.Rda&quot;) Download and Unzip Open data is often times available online and may be in a .zip compressed format. Imagine a scenario where there are hundreds of thousands of files to download. It would be quite tedious to click on each file and save to drive. To solve part of the problem, R is built with functions to facilitate downloading and unzipping of files, namely: download.files() that downloads a file from the internet to a specified directory or temporary directory; tempfile() that create temporary file placeholder in memory to which downloaded files can be saved; unzip() to unzip files for loading; tempdir() that create temporary directories in memory to which files can be decompressed and temporarily stored. A simple workflow for downloading, unzipping and loading a shapefile from the internet could be as follows. Notice that both tempfile() and tempdir() are used – tempfile() to hold one downloaded zipfile whereas tempdir() is used to hold multiple unzipped files. #URL of file location (Census Congressional Districts shapefile) url &lt;- &quot;http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_cd115_20m.zip&quot; #Create a temporary file that will be used to hold downloaded file temp &lt;- tempfile() #Download zipped file to temp download.file(url, temp) #Create temporary directory to hold decompressed files tempdir &lt;- tempdir() #Unzip file into temp directory unzip(temp, exdir = tempdir) #Load RGDAL, then open file from temp dir library(rgdal) shape &lt;- readOGR(dsn = tempdir, layer = &quot;cb_2016_us_cd115_20m&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/var/folders/sm/bx0lfjp93k357wbghyshyfy80000gp/T//RtmpI0wkp9&quot;, layer: &quot;cb_2016_us_cd115_20m&quot; ## with 437 features ## It has 8 fields ## Integer64 fields read as strings: ALAND AWATER The process is much simpler for unzipped files in common formats. Using rio, we can directly use import() to pull and load data directly from a URL. The example below imports data the US Census Bureau’s 2014 population projections by sex, race, and Hispanic origin24. library(rio) url &lt;- &quot;https://www2.census.gov/programs-surveys/popproj/datasets/2014/2014-popproj/np2014_d2.csv&quot; pop &lt;- import(url) head(pop, 5) Table 2.2: US Census Bureau’s 2014 population projections by sex, race, and Hispanic origin RACE_HISP SEX YEAR births 0 0 2014 3969976 0 0 2015 3998730 0 0 2016 4026991 0 0 2017 4054590 0 0 2018 4080454 DigIt This book takes advantage of emerging data sources that exemplify the sort of data that one might expect to encounter in the wild. The data is available to the public, but for convenience, a simple R wrapper has been developed to make data more easily accessible and allow users dig into data more seamlessly. This wrapper is available via Github: https://github.com/SigmaMonstR/digIt. To install the digIt wrapper tools, run the following code: #Install a special package to install packages from Github install.packages(&quot;devtools&quot;) library(devtools) #Install from the Github repository install_github(&quot;SigmaMonstR/digIt&quot;) #Load library library(digIt) The wrapper was built with two basic functions: digList() and digIt(). The former is used to obtain a list of available datasets. For more detail in an interactive table, specify digList(detail = TRUE). #Get list of datasets digList() ## [1] &quot;accel_exercise&quot; &quot;acs_wages&quot; &quot;acs_health&quot; ## [4] &quot;bitcoin&quot; &quot;cex_binary&quot; &quot;cex_cost&quot; ## [7] &quot;color_segment_kansas&quot; &quot;color_segment_milky&quot; &quot;doe_gas_price&quot; ## [10] &quot;flu&quot; &quot;hail_201601&quot; &quot;hail_201602&quot; ## [13] &quot;hail_201603&quot; &quot;hail_201604&quot; &quot;image_set_marine1&quot; ## [16] &quot;image_set_sky&quot; &quot;image_set_snowboard&quot; &quot;jfk_release&quot; ## [19] &quot;long_jump_olympics&quot; &quot;long_jump_top25&quot; &quot;ndvi&quot; ## [22] &quot;nyc311_gridded&quot; &quot;speech_2009&quot; &quot;speech_2010&quot; ## [25] &quot;speech_2011&quot; &quot;speech_2012&quot; &quot;speech_2013&quot; ## [28] &quot;speech_2014&quot; &quot;speech_2015&quot; &quot;speech_2016&quot; ## [31] &quot;speech_2017&quot; &quot;stopwords&quot; &quot;toll_model&quot; ## [34] &quot;toll_transactions&quot; &quot;us_district&quot; &quot;watch_list_eu&quot; ## [37] &quot;watch_list_uk&quot; &quot;watch_list_un&quot; &quot;watch_list_us&quot; ## [40] &quot;wmata&quot; To obtain a specific dataset, use the digIt() function, specifying a minimum of the dataset name. #Load actual datasets into environment data &lt;- digIt(&quot;long_jump_top25&quot;) #Download dataset to working directory digIt(&quot;long_jump_top25&quot;, download = TRUE) #Load file and show read me file data &lt;- digIt(&quot;long_jump_top25&quot;, readme = TRUE) Throughout the book, the digIt() wrapper will be used alongside basic I/O commands. 2.6 DIY 2.6.1 Reading a CSV directly from the web For an initial demo, we will learn to load in a Comma Separated Values (CSV) file containing energy data from the National Institute of Standards and Technology Net Zero Residential Test Facility – a laboratory that produces as much energy as it uses and is a testbed for sustainable and efficient home technologies. The specific dataset that will be used is the hourly photovoltaic sensor dataset https://s3.amazonaws.com/nist-netzero/2015-data-files/PV-hour.csv, which contains hourly estimates of solar energy production and exposure on the Net Zero home’s solar panels. From a sustainability perspective, this data can eventually be used to inform home efficiency policies, solar panel siting, among other things. Our goal in this demonstration is to plot sun exposure by month to see the relative differences. To start, we’ll use the read.csv() function to import the object at the url destination that contains the the CSV dataset and assign the resulting dataframe to the object df. url &lt;- &quot;https://s3.amazonaws.com/nist-netzero/2015-data-files/PV-hour.csv&quot; df &lt;- read.csv(url) With the dataset imported, we will now check the data by using head() to print the first three lines of data, colnames() to see the names of all variables, and str() to look at the data structure. head(df,3) ## Timestamp PV_AmpsAIA1 PV_AmpsAIA2 PV_AmpsBIB1 PV_AmpsBIB2 ## 1 2015-02-01 00:00:00 0.3384166 0.3360405 0.3319659 0.3359764 ## 2 2015-02-01 01:00:00 0.3368160 0.3343775 0.3303361 0.3343271 ## 3 2015-02-01 02:00:00 0.3368169 0.3343866 0.3302828 0.3343303 ## PV_FrequencyF1 PV_FrequencyF2 PV_PVBacksideTemp2 PV_PVBacksideTemp3 ## 1 59.99723 59.98672 -9.643089 -9.956393 ## 2 60.01459 60.00429 -9.600891 -9.534459 ## 3 60.00885 59.99833 -8.754960 -8.652523 ## PV_PVBacksideTemp4 PV_PVBacksideTemp7 PV_PVInsolationHArray ## 1 -9.642983 -9.705920 0.05654048 ## 2 -9.495553 -9.584033 0.06022133 ## 3 -8.569632 -8.539179 0.05883229 ## PV_PVSystem1ACEnergyOSEACPV1OS PV_PVSystem1ACPowerOSPACPV1OS ## 1 0 1.248406 ## 2 0 1.246217 ## 3 0 1.238283 ## PV_PVSystem2ACEnergyOSEACPV2OS PV_PVSystem2ACPowerOSPACPV2OS ## 1 0 1.524752 ## 2 0 1.590054 ## 3 0 1.616503 ## PV_PowerFactor3PhTotalPF3PhT1 PV_PowerFactor3PhTotalPF3PhT2 ## 1 0.1203696 0.1124940 ## 2 0.1204470 0.1127046 ## 3 0.1208075 0.1129438 ## PV_StringCurrentIStr1 PV_StringCurrentIStr2 PV_StringCurrentIStr3 ## 1 -1.114236e-06 -1.510665e-05 9.009047e-05 ## 2 -6.056746e-06 -1.257590e-05 7.790865e-05 ## 3 -1.006405e-05 -1.519916e-05 8.973710e-05 ## PV_StringCurrentIStr4 PV_StringVoltageUStr2 PV_StringVoltageUStr4 ## 1 3.243404e-05 1.335043 1.336095 ## 2 3.251752e-05 1.258958 1.244717 ## 3 2.790793e-05 1.154180 1.130783 ## PV_VoltsANUAN1 PV_VoltsANUAN2 PV_VoltsBNUBN1 PV_VoltsBNUBN2 ## 1 118.3309 118.4064 118.3702 118.4032 ## 2 117.7462 117.8199 117.7857 117.8210 ## 3 117.7640 117.8384 117.8016 117.8368 ## PV_WhoursDeliveredWhD1 PV_WhoursDeliveredWhD2 PV_Watts3PhTotalW3PhT1 ## 1 8826412 8796006 9.543909 ## 2 8826421 8796015 9.469552 ## 3 8826431 8796024 9.497611 ## PV_Watts3PhTotalW3PhT2 ## 1 8.954687 ## 2 8.859995 ## 3 8.887298 str(df) ## &#39;data.frame&#39;: 8737 obs. of 32 variables: ## $ Timestamp : Factor w/ 8737 levels &quot;2015-02-01 00:00:00&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ PV_AmpsAIA1 : num 0.338 0.337 0.337 0.337 0.338 ... ## $ PV_AmpsAIA2 : num 0.336 0.334 0.334 0.335 0.335 ... ## $ PV_AmpsBIB1 : num 0.332 0.33 0.33 0.331 0.331 ... ## $ PV_AmpsBIB2 : num 0.336 0.334 0.334 0.335 0.335 ... ## $ PV_FrequencyF1 : num 60 60 60 60 60 ... ## $ PV_FrequencyF2 : num 60 60 60 60 60 ... ## $ PV_PVBacksideTemp2 : num -9.64 -9.6 -8.75 -7.59 -5.4 ... ## $ PV_PVBacksideTemp3 : num -9.96 -9.53 -8.65 -7.46 -5.22 ... ## $ PV_PVBacksideTemp4 : num -9.64 -9.5 -8.57 -7.51 -5.1 ... ## $ PV_PVBacksideTemp7 : num -9.71 -9.58 -8.54 -7.31 -4.87 ... ## $ PV_PVInsolationHArray : num 0.0565 0.0602 0.0588 0.0588 0.0593 ... ## $ PV_PVSystem1ACEnergyOSEACPV1OS: int 0 0 0 0 0 0 0 10 495 1179 ... ## $ PV_PVSystem1ACPowerOSPACPV1OS : num 1.25 1.25 1.24 1.26 1.25 ... ## $ PV_PVSystem2ACEnergyOSEACPV2OS: int 0 0 0 0 0 0 0 10 488 1167 ... ## $ PV_PVSystem2ACPowerOSPACPV2OS : num 1.52 1.59 1.62 1.64 1.61 ... ## $ PV_PowerFactor3PhTotalPF3PhT1 : num 0.12 0.12 0.121 0.121 0.121 ... ## $ PV_PowerFactor3PhTotalPF3PhT2 : num 0.112 0.113 0.113 0.113 0.113 ... ## $ PV_StringCurrentIStr1 : num -1.11e-06 -6.06e-06 -1.01e-05 -8.98e-06 -2.16e-06 ... ## $ PV_StringCurrentIStr2 : num -1.51e-05 -1.26e-05 -1.52e-05 -1.74e-05 -1.55e-05 ... ## $ PV_StringCurrentIStr3 : num 9.01e-05 7.79e-05 8.97e-05 8.99e-05 8.50e-05 ... ## $ PV_StringCurrentIStr4 : num 3.24e-05 3.25e-05 2.79e-05 2.47e-05 2.92e-05 ... ## $ PV_StringVoltageUStr2 : num 1.34 1.26 1.15 1.28 1.27 ... ## $ PV_StringVoltageUStr4 : num 1.34 1.24 1.13 1.26 1.28 ... ## $ PV_VoltsANUAN1 : num 118 118 118 118 118 ... ## $ PV_VoltsANUAN2 : num 118 118 118 118 118 ... ## $ PV_VoltsBNUBN1 : num 118 118 118 118 118 ... ## $ PV_VoltsBNUBN2 : num 118 118 118 118 118 ... ## $ PV_WhoursDeliveredWhD1 : int 8826412 8826421 8826431 8826440 8826450 8826459 8826469 8826489 8826984 8827679 ... ## $ PV_WhoursDeliveredWhD2 : int 8796006 8796015 8796024 8796033 8796041 8796050 8796059 8796079 8796570 8797263 ... ## $ PV_Watts3PhTotalW3PhT1 : num 9.54 9.47 9.5 9.55 9.56 ... ## $ PV_Watts3PhTotalW3PhT2 : num 8.95 8.86 8.89 8.95 8.96 ... Based on initial examination, the dataset contains 34 variables with over 8,000 observations where Timestamp is the only date variable. However, while the variable contains information on dates, R treats the Timestamp as a factor. In order to accomplish the goal of this demo, we would need to: - (1) convert the Timestamp variable into a date-time object using strptime(), which requires the user to identify the date pattern so that R can extract the right information. In the case of the PV data, the format is in “%Y-%m-%d %H:%M:%S” or “Year-Month-Day Hour:Minute:Second”. - (2) extract the month from the date object using format(), which only requires the date object and the desired output format. In this case, the output format is “%m” df$Timestamp &lt;- strptime(df$Timestamp, &quot;%Y-%m-%d %H:%M:%S&quot;) df$month &lt;- format(df$Timestamp, &quot;%m&quot;) Upon doing so, we can now graph the data. Of particular relevance to our analysis is the PV_PVInsolationHArray variable that contains an estimate of amount of sunlight that impacting the solar array during the last hour. R has a number of rudimentary graphing capabilities such as plot(), which accepts two vectors of data and renders a scatter plot. Below, plot() outputs a scatter plot where month is the x or horizontal axis variable and PV_PVInsolationHArray as the y or vertical axis variable. plot(df$month,df$PV_PVInsolationHArray) The result is not particularly compelling or meaningful as the points are too many and too dense to discern a pattern. The graph could use use style enhancements. Perhaps boxplot would be more suitable to summarize the shape of the PV distribution for each month. Enter ggplot2. library(ggplot2) Upon loading the ggplot2 library, we can quickly get to work. The bare minimum syntax to render a stylized boxplot: ggplot([dataframe goes here], aes([x value here], [y value here])) + geom_boxplot(colour = &quot;[colour]&quot;, fill = &quot;[colour]&quot;) Putting this into action yields the following: ggplot(df, aes(factor(month),PV_PVInsolationHArray)) + geom_boxplot(colour = &quot;grey&quot;, fill = &quot;navy&quot;) From this graph, we can see the peak sunlight months are between May and August where May and August. The third quartile (the upper end of each box) is protracted in May and August, indicating that there are hourly periods where the light is more intense during those months. https://www.census.gov/geo/maps-data/data/cbf/cbf_cds.html↩ https://www.census.gov/data/datasets/2014/demo/popproj/2014-popproj.html↩ "],
["data-manipulation-wrangling-processing.html", "Chapter 3 Data Manipulation / Wrangling / Processing 3.1 Motivation 3.2 Cell-Level Operations 3.3 Matrix and Data Frames 3.4 Control Structures 3.5 Functions 3.6 Etiquette 3.7 Getting into the mentality 3.8 DIY", " Chapter 3 Data Manipulation / Wrangling / Processing 3.1 Motivation Speech contains a wealth of information. As humans, we are taught to understand verbal and written communication – pick out the nouns, verbs, and adjectives, then combine the information to decipher meaing. Take the following excerpt from the 2010 State of the Union: Now, one place to start is serious financial reform. Look, I am not interested in punishing banks. I’m interested in protecting our economy. A strong, healthy financial market makes it possible for businesses to access credit and create new jobs. It channels the savings of families into investments that raise incomes. But that can only happen if we guard against the same recklessness that nearly brought down our entire economy. We need to make sure consumers and middle-class families have the information they need to make financial decisions. We can’t allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy. To many, text might not be considered data despite the fact that any analytical mind with a command of the English language can identify key terms: Now, one place to start is serious financial reform. Look, I am not interested in punishing banks. I’m interested in protecting our economy. A strong, healthy financial market makes it possible for businesses to access credit and create new jobs. It channels the savings of families into investments that raise incomes. But that can only happen if we guard against the same recklessness that nearly brought down our entire economy. We need to make sure consumers and middle-class families have the information they need to makefinancial decisions. We can’t allow financial institutions, including those that take your deposits,to take risks that threaten the whole economy. Much like the logic that guides keyword identification, text can be shaped from an unstructured dataset into a well-defined, structured dataset: Table 3.1: Most frequent terms found in excerpt. Terms Frequency of Term Number of Characters financial 4 9 economy 3 7 families 2 8 interested 2 10 Of course, this process could be done manually, but imagine sorting through all 7,304 words in the 2010 address or scaling the process to the roughly 1.9 million words in addresses State of the Union addresses between 1790 and 2016. All the steps required to convert unstructured text into usable data can be done with a little bit of planning, technical imagination and data manipulation. Every little detail about the data needs to be considered and meticulously converted into a usable form. From a data format perspective, capitalized characters are not the same as lower case. Contractions are not the same as terms that are spelled out. Punctuation affect spacing. Carriage returns and new line markers, while not visible in reading mode, are recorded. Let’s take one line from above and dissect the changes that need to be made: “We need to make sure consumers and middle-class families have the information they need to make financial decisions. We can’t allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy.” We then turn everything into lower case so all letters of the alphabet are read the same. “we need to make sure consumers and middle-class families have the information they need to make financial decisions. we can’t allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy.” Then, we get rid of punctuation by substituting values with empty quotations (&quot;&quot;). “we need to make sure consumers and middleclass families have the information they need to make financial decisions we cant allow financial institutions including those that take your deposits to take risks that threaten the whole economy” Each space between each word can be used as a delimiter that can be used as a symbol for a program to break apart words into elements in a list. Table 3.2: Terms we families financial those that need have decisions that threaten to the we take the make information cant your whole sure they allow deposits economy consumers need financial to and to institutions take middleclass make including risks There are words in there that don’t add much value as they are commonplace and filler. In text processing, these words are known as stop words. In each domain, the list of stop words likely differs, thus data scientists may need to build a customized list. For simplicity, we’ve used a stop words list that is used in the mySQL – an open source relational database management system. The result is the list of remaining words. Table 3.3: Terms after removing stop words make information financial risks consumers make institutions threaten middleclass financial including economy families decisions deposits From that data, we can aggregate the data into a form that is meaningful to answer a research question. For example, the frequency of words may provide a clue as to what the text is about. In this case, each “financial” and “make” appear twice in the text, perhaps indicating that there is an orientation towards action (make) for financial considerations. Table 3.4: Term Frequencies Term Freq Term Freq financial 2 including 1 make 2 information 1 consumers 1 institutions 1 decisions 1 middleclass 1 deposits 1 risks 1 economy 1 threaten 1 families 1 This is just the tip of the iceberg. Text processing is just one example of feature engineering – or the creation and derivation of new information from data that is not in the best of formats. There is often information hidden within information and mastering feature engineering and data manipulation more generally unlocks new possibilities and insights. Much of a data scientist’s time is spent extracting, cleaning, and transforming data for use. In database circles, this process is sometimes referred to as Extract-Transform-Load. In general data circles, it is referred to as data wrangling or data munging. Regardless of the term, without a reputable study to cite, the author supposes based on his and colleagues’ experiences that as much of 80% of a data project is spent manipulating information. Thus, it is evermore important to master the basic skills. Stronger one’s command of programming data manipulations, the faster one can code, the soon one can get to the more interest aspects of using data as a strategy. This chapter is dedicated to building the fundamental skills on which all data projects rely. We begin by highlighting introducing the mechanics of cell level (e.g. text operations, formats) and structural level (e.g. sorting, subsetting, merging) data manipulation. We then proceed to introducing programming paradigms, namely control structures and functions, that vastly improve the efficiency when handling data. The chapter ends on a number of DIY cases. 3.2 Cell-Level Operations Cell-level operations is a matter of changing the contents of data elements. With numeric data, this may involve various arithmetic operations like subtraction and multiplication, but that is applicable assuming that data is already of good quality. More often than not, data cleansing involves finding, extracting, and replacing the contents of string values. For example, below is a vector of four string values: budget &lt;- c(&quot;Captain&#39;s Log, Stardate 1511.8. I have $10.20 for a big galactic mac.&quot;, &quot;The ensign has $1,20 in her pocket.&quot;, &quot;The ExO spent has $0.25 left after paying for overpriced warp core fuel.&quot;, &quot;Chief medical officer is the high roller with $53,13.&quot;) What if we need to extract the total available funds available to buy galactic big macs? All four elements contain dollar values, which can benefit from feature engineering. To do so, we use a combination of text manipulation functions and regular expressions or regex – a series of characters that describe a regularly occurring text pattern. First, commas should be replaced with a period using gsub(), assigning the result to a new object new. Note that in some regions, such as Europe, commas are used as decimals rather than periods. new &lt;- gsub(&quot;,&quot;, &quot;\\\\.&quot;, budget) Second, find the elements that contain the following pattern: a dollar sign followed by one to two digits, followed by a period, then another two digits (\\\\$\\\\d{1,2}\\\\.\\\\d{2}). The pattern can be used with the functions regexpr() to find the positions of the matching patterns in the text, then regmatches() is used to extract. indices &lt;- regexpr(&quot;\\\\$\\\\d{1,2}\\\\.\\\\d{2}&quot;, new) numbers &lt;- regmatches(new, indices) print(numbers) ## [1] &quot;$10.20&quot; &quot;$1.20&quot; &quot;$0.25&quot; &quot;$53.13&quot; Third, we should replace dollar sign with blank and strip out any leading white space using trimws(). numbers &lt;- trimws(gsub(&quot;\\\\$&quot;,&quot;&quot;, numbers)) print(numbers) ## [1] &quot;10.20&quot; &quot;1.20&quot; &quot;0.25&quot; &quot;53.13&quot; Lastly, convert the character vector to numeric, then sum the vector. money &lt;- as.numeric(numbers) print(paste0(&quot;Total galactic big mac funds = $&quot;, sum(money))) ## [1] &quot;Total galactic big mac funds = $64.78&quot; A number of observations. In steps one through three, you will have noticed that the characters &quot;$&quot;, &quot;.&quot;, and &quot;d&quot; were preceded by double backslash. These are known as escaped characters as the double backslash preceding the characters changes their meanings. In step two, a sequence of unusual characters (\\\\$\\\\d{1,2}\\\\.\\\\d{2}) was used to find the $x.xx pattern, which can be broken into specific commands: \\\\$ is a dollar sign. \\\\d{1,2} is a series of numerical characters that is between one to two digits long. \\\\. is a period. \\\\d{2} is a series of numerical characters that is exactly two digits long. Mastering regex is a productivity multiplier, opening the possibility of ultra-precise text replacement, extraction, and other manipulation. Imagine scenarios where raw data is not quality controlled and mass errors plague the usefulness of the data. An analyst may spend days if not weeks or months cleaning data by hand (or rather through find and replace). With regex, haphazard cleaning is no longer an issue. To make the most of regex requires a command of both text manipulation functions that are designed to interpret regex as well as regex itself. 3.2.1 Text manipulation functions Find and replace are useful functions in most word processing and spreadsheet softwares. But what does it take to do find and replace at scale. The following seven text manipulation functions are commonly implemented in programming languages. Each searches for a user-defined pattern and returns a result in a well-defined format. grep(): Returns either the index position of a matched string or the string containing the matched portion. grepl(): Returns a logical vector – a vector of TRUE/FALSE for whether a matched string was found. gsub(): Searches and replaces patterns in strings. regexpr(): Returns the character position of a pattern in a string. strsplit(): Splits strings into a list of values based on a delimiter. regmatches(): Extract substring using information from regexpr() substr(): Extract substring from a string based on string positions. Traditionally, functions like grep() are available through command line interfaces and are a core offering of the R programming language. On their own, some basic tasks can be accomplished such as exact matches of specific text. As will be seen later, these functions combined with regex are quite powerful. To illustrate the basic functionality, let’s assume we have four sentences that indicate when four US laws were signed. laws &lt;- c(&quot;. Dodd-Frank Act was signed into federal law on July 21, 2010.&quot;, &quot;Glass-Steagall Act was signed into federal law by FDR on June 16, 1933&quot;, &quot;Hatch Act went into effect on August 2, 1939&quot;, &quot;Sarbanes-Oxley Act was signed into law on July 30, 2002&quot;) Example Task: I need to find all sentences about laws that were sponsored by two congressmen. Suppose we need to find acts that are named for two congressmen. The grep() function can be used to find the index positions of elements in a vector that contain &quot;-&quot;. Otherwise stated, return the row number for each sentence that contains a hyphen. In this case, the 1st, 2nd, and 4th elements in the laws vector contain hyphens. grep(&quot;-&quot;, laws) ## [1] 1 2 4 grep() can also return the matched value when the option value is set to TRUE. This is handy for inspecting the accuracy of matches. In practice, with large data sets that contain variable names that follow a common convention, column names can be efficiently searched. grep(&quot;-&quot;, laws, value = TRUE) ## [1] &quot;. Dodd-Frank Act was signed into federal law on July 21, 2010.&quot; ## [2] &quot;Glass-Steagall Act was signed into federal law by FDR on June 16, 1933&quot; ## [3] &quot;Sarbanes-Oxley Act was signed into law on July 30, 2002&quot; This can also be expressed in a different way. The grepl() function can be used to obtain a vector of logical values (TRUE/FALSE) that is the same length as the input vector laws. grepl(&quot;-&quot;, laws) ## [1] TRUE TRUE FALSE TRUE Example Task: I need to find text about laws passed in the 21st century. The same functions can be used to find laws that were passed int he 21st century. We can look for sentences that contain 20 followed by any two digits. This is a regex expression that will be elaborated upon in the following section. grep(&quot;20\\\\d{2}&quot;, laws) ## [1] 1 4 Example Task: I need to find the names of co-sponsored laws. Similar to grep(), regexpr() enables more precise search and extraction. Rather than returning which element contains a matched string pattern, regexpr() returns two sets of attributes. The first indicates the position of the first character that is matched in each string (e.g. 7 indicates that the - value is the fifth character is the first string) whereas the second set of attributes indicate the length of the match. Positive values indicate the number of characters and a value of -1 indicates no match. regexpr(&quot;-&quot;, laws) ## [1] 7 6 -1 9 ## attr(,&quot;match.length&quot;) ## [1] 1 1 -1 1 ## attr(,&quot;useBytes&quot;) ## [1] TRUE For even greater precision, a regex search can be used to turn up positions of within an element that contains a match. In the example below, \\\\w is used to find any alphanumeric character. The numbers {3,20} indicate how many characters – in this case, we’re searching for substrings that are between three and 20 charaters long, separated by a hyphen. The result is two set of values. The first set of numbers indicates the position of the first string character that matches the pattern (e.g. 3 indicates that Dodd-Frank starts in the 3rd space in the string). The second number in the following row indicates the length of the match (e.g. 10 indicates that Dodd-Frank is 10 characters long). More of these regex oeprators are described later in this chapter. regexpr(&quot;\\\\w{3,20}-\\\\w{3,20}&quot;, laws) ## [1] 3 1 -1 1 ## attr(,&quot;match.length&quot;) ## [1] 10 14 -1 14 ## attr(,&quot;useBytes&quot;) ## [1] TRUE regexpr alone is not all that useful, but is incredibly powerful when combined with regmatches(), which extracts character values based on the output of regexpr(). Below, we place the results of a search for two alphabetic strings separated by a hyphen in the object result, then pass both the laws vector and the regexpr() result in regmatches(). We print to console the resulting vector. result &lt;- regexpr(&quot;\\\\w{3,20}-\\\\w{3,20}&quot;, laws) matched &lt;- regmatches(laws, result) print(matched) ## [1] &quot;Dodd-Frank&quot; &quot;Glass-Steagall&quot; &quot;Sarbanes-Oxley&quot; Example Task: I need to remove a pesky hyphen. To remove the hyphen, we can use gsub() to find “-” and replace with a space &quot; “. cleaned &lt;- gsub(&quot;-&quot;,&quot; &quot;, matched) print(cleaned) ## [1] &quot;Dodd Frank&quot; &quot;Glass Steagall&quot; &quot;Sarbanes Oxley&quot; Example Task: I need to make a catchy abbreviation for these law names. Using substr(), substrings or parts of strings can be extracted based on their position. To get the first two letters of each law, we can do the following: substr(laws,1,2) ## [1] &quot;. &quot; &quot;Gl&quot; &quot;Ha&quot; &quot;Sa&quot; Or create new vectors that are a concatenation of the first two letters of each name like SoHo, NoHo, or any trendy name. #Get index of second name (search for space followed by two characters) indices &lt;- regexpr(&quot; \\\\w{2}&quot;,cleaned) #extract first two letters secondhalf &lt;- trimws(regmatches(cleaned, indices)) paste0(substr(cleaned,1,2), secondhalf) ## [1] &quot;DoFr&quot; &quot;GlSt&quot; &quot;SaOx&quot; Example Task: I need to extract the last names of each congressman who co-sponsored bills. Lastly, strsplit() can be used to create a list of all names that have been used in laws, simply by using the - as a separator or delimiter. print(matched) ## [1] &quot;Dodd-Frank&quot; &quot;Glass-Steagall&quot; &quot;Sarbanes-Oxley&quot; lawnames &lt;- strsplit(matched, &quot;-&quot;) print(lawnames) ## [[1]] ## [1] &quot;Dodd&quot; &quot;Frank&quot; ## ## [[2]] ## [1] &quot;Glass&quot; &quot;Steagall&quot; ## ## [[3]] ## [1] &quot;Sarbanes&quot; &quot;Oxley&quot; Note that a list object is not the same as a vector. Lists can contain multiple instances of another class of object. The list returned from strsplit() contains multiple vectors. To access each item, simply refer to the element in the list using an index number. To access the Glass-Steagall objects, we can refer to the 2nd object in [[]]. lawnames[[2]] ## [1] &quot;Glass&quot; &quot;Steagall&quot; 3.2.2 Regular Expressions Next up: Regex. These powerful commands give users the flexibility to search data and surface results with possible matches. Before proceeding into more complex string combinations, knowledge of a few cleverly designed capabilities may go a long way: Alternatives (e.g. “OR” searches) can be surfaced by using a pipe “|”. For example, a string search for “Bob or Moe” would be represented as “Bob|Moe”. The extent of a search should be denoted by parentheses (). For example, a string search for “Jenny” or an alternative spelling like Jenny would be represented as “Jenn(y|i)”.&quot; A search for one specific character should be placed between square brackets []. The number of characters is placed between curly brackets {}. In New York City, the famed avenue Broadway is may be written and abbreciated in a number of ways. The vector streets contains a few instances of spellings of Broadway mixed in with other streets that start with the letter B. #A sampling of street names streets &lt;- c(&quot;Bruckner Blvd&quot;, &quot;Bowery&quot;, &quot;Broadway&quot;, &quot;Bway&quot;, &quot;Bdway&quot;, &quot;Broad Street&quot;, &quot;Bridge Street&quot;, &quot;B&#39;way&quot;) #Search for two specific options grep(&quot;Broadway|Bdway&quot;, streets, value = TRUE) ## [1] &quot;Broadway&quot; &quot;Bdway&quot; #Search for two variations of Broadway grep(&quot;B(road|&#39;)way&quot;, streets, value = TRUE) ## [1] &quot;Broadway&quot; &quot;B&#39;way&quot; #Search for cases where either d or apostrophe are between B and way grep(&quot;B[d&#39;]way&quot;, streets, value = TRUE) ## [1] &quot;Bdway&quot; &quot;B&#39;way&quot; 3.2.2.1 Escaped characters Quite a few single characters hold a special meaning in addition to the literal meaning. To disambiguate their meaning, a backslash precedes these characters to denote the alternative meaning. A few include: \\n: new line \\r: carriage return \\t: tab \\': single quote when in a string enclosed in single quotes ('Nay, I can\\'t') \\&quot;: double quote when in a string enclosed in double quotes (&quot;I have a \\&quot;guy\\&quot;.&quot;) In other cases, double backslashes should be used: \\\\.: period. Otherwise, un-escaped periods indicate searches for any single character. \\\\$: dollar sign. A dollar sign without backslashes indicates to find patterns at the end of a string. 3.2.2.2 Character Classes A character class or character set is used to identify specific characters within a string. How would one represent “12.301.1034” or “?!?!?!”? One or more of the following character classes can do the job: [:punct:]: Any and all punctuation such as periods, commas, semicolons, etc. For specific specific punctuation, simply enclose the characters between two brackets. For example, to find only commas and carrots, use [&lt;&gt;,]. [:alpha:]: Alphabetic characters such as a, b, c, etc. With other languages including R, it is commonly written as [a-z] for lower case and [A-Z] for upper case. [:digit:]: Numerical values. With other languages including R, it is commonly written as \\\\d or [0-9]. For any non-digit, write \\\\D. [:alnum:]: Alphanumeric characters (mix of letters and numbers). With other languages including R, it is indicated using to as [0-9A-Za-z] or \\\\w. For any non-alphanumeric character, use \\\\W. [:space:]: Spaces such as tabs, carriage returns, etc. For any white space, use \\\\s. For any non-whitespace character, use \\\\S. [:graph:]: Human readable characters including [:alnum:] and [:punct:]. \\\\b: Used to denote “whole words”. \\\\b should be placed before and after a regex pattern. For example, \\\\b\\\\w{10}\\\\b indicates a 10 letter word. There are quite a few character classes not listed above, but for these constitute the lion’s share. It is worth keeping in mind that the implementation of character classes may differ between programming languages. A number of the above are extensions that have been implemented in R in a specific manner. 3.2.2.3 Quantifiers Each character class on its own indicates a search for one and only one character. In practice, most character searches will involve a search for more than just one character. To indicate such a search, regex relies on quantifiers to indicate the length of patterns. For example, a search for a year between the year 1980 and 2000 will require exactly four digits, but a search for the speed of a gust of wind will likely vary between 1 and 3 digits. The following six quantifiers provide a degree of both flexibility and specificity to accomplish search tasks: {n}: match pattern n times for a preceding character class. For example &quot;\\\\d{4}&quot; looks for a four digit number. {n, m}: match pattern at least n-times and not more than m times for a preceding character class. For example &quot;\\\\d{1,4}&quot; looks for one to four digit number. {n, }: match at least n times for a preceding character class. For example &quot;\\\\d{4,}&quot; looks for a number that has at least four digits. *: Wildcard, or match at least 0 times. +: Match at least once. ?: Match at most once. In the example below, quantifiers are used to extract specific number patterns with a high degree of accuracy. dates &lt;- c(&quot;Octavian became Augustus on 16 Jan 27 BCE&quot;, &quot;In the year 2000, a computer bug was expected to topple society.&quot;, &quot;In the 5400000000 years, our sun will become a red dwarf.&quot;) #Match an element with a 9 digit number grep(&quot;\\\\d{9}&quot;, dates, value = TRUE) ## [1] &quot;In the 5400000000 years, our sun will become a red dwarf.&quot; #Match an element with a 9 digit number grep(&quot;\\\\b\\\\d{4}\\\\b&quot;, dates, value = TRUE) ## [1] &quot;In the year 2000, a computer bug was expected to topple society.&quot; #Match a date that follows 16 January 27 BCE grep(&quot;\\\\d{2}\\\\s\\\\w{3}\\\\s\\\\d{2}\\\\s\\\\w{3}&quot;, dates, value = TRUE) ## [1] &quot;Octavian became Augustus on 16 Jan 27 BCE&quot; 3.2.2.4 Position matching Regex builds in functionality to search for patterns based on position of a substring in a string, such as at the start or end of a string. There are quite a few other position matching patterns, but the following two are the workhorses. $: Search at the end of a string. ^: Start of string when placed at the beginning of a regex pattern. To demonstrate these patterns, we’ll apply grep() to three headlines from the BBC. headlines &lt;- c(&quot;May to deliver speech on Brexit&quot;, &quot;Pound falls with May&#39;s comments&quot;, &quot;May: Brexit plans to be laid out in new year&quot;) print(headlines) ## [1] &quot;May to deliver speech on Brexit&quot; ## [2] &quot;Pound falls with May&#39;s comments&quot; ## [3] &quot;May: Brexit plans to be laid out in new year&quot; #Find elements that contain May at the beginning of the string grep(&quot;^May&quot;, headlines, value = TRUE) ## [1] &quot;May to deliver speech on Brexit&quot; ## [2] &quot;May: Brexit plans to be laid out in new year&quot; #Find elements that contain Brexit at the beginning of the string grep(&quot;Brexit$&quot;, headlines, value = TRUE) ## [1] &quot;May to deliver speech on Brexit&quot; 3.2.2.5 Exercises Personally identifiable information or PII is often a barrier to sharing information For the following financial record, anonymize records by removing age and name using gsub() and regmatches() to extract the amount of money John owes the bank. statement &lt;- &quot;John (SSN: 012-34-5678) owes $1004 to the bank at 49-29 Pewter Street.&quot; 3.3 Matrix and Data Frames Moving onto the macro-scale, individual values of data are often placed within vectors, matrices and data frames. We already know that vectors are a series of values of the same data type (e.g. strings, numerics, booleans) and matrices are vectors but in two dimensions (\\(n \\times m\\) dimensions). Data frames are a generalization of matrices that allow for each column of data to hold different data types as well as refer to individual columns by a user-specified name. When should each matrices and data frames be used? From a pure logistical perspective, data frames are more flexible with respect to its ability to store multiple data types. Some code libraries are built specifically for matrices and others for data frames. Ultimately, it is up to the data scientist to choose. Matrices Data Frames Pros Memory efficient. Good for advanced mathematical operations. Store mixed types of data types. Allows user to refer to columns by an explicit name. Cons Able to store one data type at a time – leads to slightly more work required to manage multiple matrices. Columns can only be referred to by index number. Not as memory efficient. In this section, we review among the most powerful functions for data manipulation: sort, reshape, collapse, and merge. A mastery of the logic and operations that guide matrix and data frame processing opens the possibilities to work with virtually any kind of data. To illustrate this, consider a data frame with a list of the top 25 male long jumpers and top 25 female long jumpers as found on Wikipedia. Each row contains information about one athlete such as with their record-setting longest jump, date of jump, location of jump among other features. Imagine the sort of tasks that one could do to munged the data into a usable shape. Note that data can be obtained directly from this URL (https://s3.amazonaws.com/dspp/long_jump_top25.csv): jumps &lt;- read.csv(&quot;https://s3.amazonaws.com/dspp/long_jump_top25.csv&quot;) print(jumps) or alternatively using the digIt() function designed for this textbook. library(digIt) jumps &lt;- digIt(&quot;long_jump_top25&quot;) print(jumps) Table 3.5: Example records from long jump data set sex rank athlete mark.meters date place male 1 Mike Powell 8.95 30 August 1991 Tokyo male 2 Bob Beamon 8.9 18 October 1968 Mexico City . . . . . . . . . . . . female 14 Ivana Španović 7.24 5 March 2017 Belgrade female 16 Helga Radtke 7.21 26 July 1984 Dresden 3.3.1 Indices and Subsetting In both matrices and data frames, individual and ranges of rows and columns can be extracted by calling their index number. The jumps data contains \\(n = 51\\) rows and \\(k = 6\\) features for a total of \\(306\\) data elements. Each row and each feature has a unique index number that starts from \\(1\\) and increases sequentially. In other programming languages, index numbers start from \\(0\\). To extract the second row from jumps, we simply type the number 2 before the commaa in square brackets. The line below essentially indicates that given a matrix or data frame, extract the second row and all columns. jumps[2,] ## sex rank athlete mark.meters date place ## 2 male 2 Bob Beamon 8.9 18 October 1968 Mexico City To extract multiple records by row index depends on whether the request is sequential or piecemeal. Below, the first line extracts a range of rows from the 2nd through 4th rows in jumps, whereas the second extracts two non-overlapping ranges that are included in a vector. jumps[2:4, ] # apply the index range to extract rows jumps[c(1:2,10:11), ] # specific indices The same notation can be used to extract all athlete names for all records by typing the number 3 after the comma in square brackets. The number 3 is the column index that contains athlete names. Keep in mind that extracting one column from a matrix or data frame results in a vector – the data structure is not retained. jumps[, 3] In addition, data frames provide a few additional methods of extracting the athlete column. jumps[, &quot;athlete&quot;] # extract column with &quot;athlete&quot; label jumps[[&quot;athlete&quot;]] # list syntax to extract column from data frame jumps$athlete #compact version of data column manipulation To extract two or more columns follows a familiar pattern, making use of either a range of column indices or a vector of column names. jumps[, 3:4] # extract multiple columns jumps[, c(&quot;athlete&quot;, &quot;mark.meters&quot;)] # multiple column labels To extract rows that meet one specific criterion requires the creation of either a vector of booleans than indicate if the criterion is TRUE or FALSE for each row (logical vector has the same number of rows as the data), or a list of row index numbers that meet the criterion. The example below performs operations on a data frame. Example 1: Create a vector of booleans over 8.8 meters, then select rows based on whether a row contains a TRUE value. group1 &lt;- jumps$mark.meters &gt; 8.8 # return vector of booleans for jumps over 8.8 meters jumps[group1, ] # select the indices Example 2: Find row indices that contain the place “Eugene”, then use the vector of matching row indices to extract rows from jumps. group2 &lt;- grep(&quot;Eugene&quot;, jumps$place) jumps[group2, ] Example 3: Find record setting jumps in Tokyo, Mexico City and New York City using the %in% operator. #Find place in vector of cities group3 &lt;- jumps$place %in% c(&quot;New York City&quot;, &quot;Mexico City&quot;, &quot;Tokyo&quot;) #Return matching entries jumps[group3, ] #Return non-matching entries jumps[!group3, ] Extract rows that meet two criteria is as simple as using logical operators. To search for two non-overlapping criteria, the pipe operator &quot;|&quot; should be used to represent OR, whereas &quot;&amp;&quot; represents AND. #Find athletes who jumped beyond 8.9m or below 7.2m group2 &lt;- (jumps$mark.meters &gt; 8.8) | (jumps$mark.meters &lt; 7.2) jumps[group2, ] # select the indices Rows and columns can be re-ordered simply by enumerating indices in a specified order. For example, columns can be re-ordered by passing a vector of column indices into a data frame. #Original order colnames(jumps) ## [1] &quot;sex&quot; &quot;rank&quot; &quot;athlete&quot; &quot;mark.meters&quot; &quot;date&quot; ## [6] &quot;place&quot; #Reorder according vector of column indices jumps &lt;- jumps[, c(2, 4, 3, 6, 1, 5)] #Check new order colnames(jumps) ## [1] &quot;rank&quot; &quot;mark.meters&quot; &quot;athlete&quot; &quot;place&quot; &quot;sex&quot; ## [6] &quot;date&quot; To sort a column can be done using order(), which returns a vector of row indicies in ascending order. order(jumps$mark.meters) ## [1] 49 50 51 47 48 43 44 45 46 41 42 39 40 38 37 34 35 36 33 32 31 30 29 ## [24] 28 27 26 24 25 22 23 21 19 20 17 18 16 15 14 13 12 11 9 10 8 5 6 ## [47] 7 4 3 2 1 This simple function can be included in the rows index to sort a entire data frame. To sort descending, add a &quot;-&quot; before the feature to be sorted. jumps[order(jumps$mark.meters), ] # order records by mark.meters jumps[order(-jumps$mark.meters), ] # order records by mark.meters, decreasing jumps[order(jumps$mark.meters, decreasing=TRUE), ]# order records by mark.meters, decreasing To sort on multiple columns is a matter of adding additional fields separated by commas. # first by rank, then sex, then place jumps[order(jumps$rank, jumps$sex, jumps$place), ] # first desc. by rank, then sex, then place jumps[order(-jumps$rank, jumps$sex, jumps$place), ] Exercises Extract records with female athletes who are ranked greater than rank #10. Extract records with jump marks greater than 7.250 meters and less than 8.625 meters. 3.3.2 Reshape Data usually takes on two basic shapes: wide and long. The data frame in the previous section is in long format in which each row represents an individual that is ranked among the top 25 within each sex. Each row has a value (mark.meters) associated with the characteristics that make that row unique (athlete, place, rank, sex, date). We will find that the wide form is also useful. For those who have familiarity with spreadsheet software, reshaping data essentially is the functionality behind a pivot table. Some features can be used to identify rows and others features can be used to stratify the data by discrete bins. The reshape() function can be used to convert a data frame such that each row contains values for each rank (1 to 25) placing the distance jumped by each sex in separate columns. reshape(&lt;data&gt;, idvar = &lt;ID variables&gt;, timevar = &lt;column variables&gt;, direction = &lt;direction&gt;) &lt;data&gt; is the data set &lt;ID variables&gt; is a variable or a combination of variables that serves as a unique identifier for each row &lt;column variables&gt; is a variable that will be used to define multiple measurements per row &lt;direction&gt; indicates if the data will be converted into long or wide form. To illustrate reshape(), the data is whittled down to a three feature set containing rank, sex and mark.meters. These features are then input into the reshape() to convert a long form data set into wide form. example &lt;- jumps[, c(&quot;rank&quot;, &quot;sex&quot;, &quot;mark.meters&quot;)] wide &lt;- reshape(example, idvar = &quot;rank&quot;, timevar = &quot;sex&quot;, direction = &quot;wide&quot;) head(wide, 5) ## rank mark.meters.male mark.meters.female ## 1 1 8.95 7.52 ## 2 2 8.90 7.49 ## 3 3 8.87 7.48 ## 4 4 8.86 7.43 ## 5 5 8.74 7.42 There are now 21 rows, where there were previously 51. Returning to the original long format is straightforward; but we aren’t left with the exact same data table. There are artifacts of the change-in-shape for both the column and row names. Sort of like data manipulation breadcrumbs. This will be cleaned up in the next subsection. Rename row and column headers The newly assigned column or row names may not match the meaning in the data. It is good practice to maintain the column headers at each stage of analysis, even if you don’t immediately use the intermediate data table. Otherwise editing code gets confusing, quickly. The column names are stored in an attribute of the data frame: #Two ways of obtaining data frame field names names(wide) ## [1] &quot;rank&quot; &quot;mark.meters.male&quot; &quot;mark.meters.female&quot; colnames(wide) ## [1] &quot;rank&quot; &quot;mark.meters.male&quot; &quot;mark.meters.female&quot; Renaming column headers using the built-in, base functions in R looks confusing. In words, the following code identifies the positions in names(wide) where the values are mark.meters.male and mark.meters.female. At the specified positions, the values is reassigned with new values male.mark and female.mark, respectively. names(wide)[names(wide) == &quot;mark.meters.male&quot;] &lt;- &quot;male.mark&quot; names(wide)[names(wide) == &quot;mark.meters.female&quot;] &lt;- &quot;female.mark&quot; head(wide, 5) ## rank male.mark female.mark ## 1 1 8.95 7.52 ## 2 2 8.90 7.49 ## 3 3 8.87 7.48 ## 4 4 8.86 7.43 ## 5 5 8.74 7.42 Alternatively, groups of fields can be renamed based on the index position in the names list. colnames(wide)[c(1,3)] &lt;- c(&quot;rank.num&quot;, &quot;female.mark&quot;) 3.3.3 Collapse Reshape and collapse are often used in conjunction in order to calculate summary statistics by group. The aggregate() function is the workhorse for summarizing data. It accepts three arguments: aggregate(&lt;x&gt;, by = list(&lt;groups&gt;), FUN = &lt;function&gt;)) &lt;function&gt; is the specific function that will be applied to the data. Commonly used functions included mean, sum, length (count), sd (standard deviation), among others. &lt;x&gt; is the R object on which the function will be applied. This generally should be a numerical value. &lt;groups&gt; is a variable that contains groups for which the mathematical function will be calculated. This needs to be provided as a list(). Consider this function to estimate the jump distance average by sex. To arrive at the answer, we will collapse the data frame by sex to create a new data frame with the average jump distance by sex. Notice that the group variable can be named within the list and the number of observations is reduced to only two. (out &lt;- aggregate(jumps$mark.meters, by = list(athlete.sex = jumps$sex), FUN = mean)) ## athlete.sex x ## 1 female 7.2850 ## 2 male 8.6588 How about tabulations? count medals by country Note that data can be obtained directly from this URL (https://s3.amazonaws.com/whoa-data/long_jump_olympics.csv): jumps &lt;- read.csv(&quot;https://s3.amazonaws.com/whoa-data/long_jump_olympics.csv&quot;) print(jumps) or alternatively using the digIt() function designed for this textbook. olympics &lt;- digIt(&quot;long_jump_olympics&quot;) head(olympics,3) The USA had won 56 olympic long jump medals through 2017. cty_medals &lt;- aggregate(olympics$country, by = list(country = olympics$country), FUN = length) head(cty_medals[order(-cty_medals$x),], 3) ## country x ## 33 USA 56 ## 32 URS 12 ## 12 GBR 8 The built-in R functions are effective, but new, memory efficient libraries have arisen to augment R’s capabilites. Exercises Load the iris dataset using the datasets package. Calculate the average and maximum sepal length for each Iris species. (Bonus: Write the commands without error messages.) Use the aggregate() function to count the number of observations of sepal width for each species. (Difficult) Create a data frame with the 35th observation of sepal width for each species. 3.3.4 Join Joining is one of the most critical steps in a data analytics project as it allows one data set to be augmented by another. Types of joins merge(x, y, by = ids, all.x = TRUE, all.y = TRUE)) x is the left-hand side data frame; y is the right-hand side data frame; by = ids is a string vector of variable names to be joined. Note that this is used when the variable names are identical in both x and y. If column names are different, use by.x to specify x column names and by.y for y column names. all.x and all.y are logical parameters (TRUE/FALSE) that control the type of join: Inner join: By default, both parameters are set to FALSE. Left outer join: set all.x = TRUE. Right outer join: set all.y = TRUE. Outer join: set all.x = TRUE and all.y = TRUE . To illustrate the power of joins, we apply basic data manipulation to answer a few exploratory questions. Which of the top 25 athletes hold an Olympic medal? This can be done by a left join. #Aggregate number of medals by athlete medals &lt;- aggregate(olympics$country, by = list(athlete = olympics$athlete), FUN = length) #Join jumps with medals inner_join &lt;- merge(x = jumps[, c(&quot;athlete&quot;,&quot;mark.meters&quot;)], y = medals, by = &quot;athlete&quot;) #See top 10 head(inner_join[!is.na(inner_join$x), ], 10) ## athlete mark.meters x ## 1 Bob Beamon 8.90 1 ## 2 Brittney Reese 7.31 1 ## 3 Carl Lewis 8.87 4 ## 4 Dwight Phillips 8.74 1 ## 5 Galina Chistyakova 7.52 1 ## 6 Greg Rutherford 8.51 2 ## 7 Heike Drechsler 7.48 3 ## 8 Inessa Kravets 7.37 1 ## 9 Irina Meleshina 7.27 1 ## 10 Irving Saladino 8.73 1 How many of the 51 top long jumpers did not earn an Olympic medal? Left outer join by specifying all.x = TRUE to keep all left side values. #Merge left_outer &lt;- merge(x = jumps[, c(&quot;athlete&quot;,&quot;mark.meters&quot;)], y = medals, by = &quot;athlete&quot;, all.x = TRUE) #Print the number of rows nrow(left_outer[is.na(left_outer$x), ]) ## [1] 27 How many Olympic medalists are not on the top 25 list? Right outer join by specifying all.y = TRUE to keep all right side values. #Merge right_outer &lt;- merge(x = jumps[, c(&quot;athlete&quot;,&quot;mark.meters&quot;)], y = medals, by = &quot;athlete&quot;, all.y = TRUE) #Print the number of rows nrow(right_outer[is.na(right_outer$mark.meters), ]) ## [1] 93 Exercises The datasets library is a base package in R that contains a number of data sets for learning and practice. Use the state datasets (take a look at the documentation) to calculate identify the state division (e.g. New England) that the state with the highest murder rate. What was the recorded population by region? Answers. Aggregate, then sort. #Set data as a data frame df &lt;- as.data.frame(state.x77) #Roll up by division, calculating max out &lt;- aggregate(df$Murder, by = list(state.division), FUN = max) #Re-order out &lt;- out[order(-out[,2]), ] out[1, ] ## Group.1 x ## 4 East South Central 15.1 Aggregate. #Set data as a data frame df &lt;- as.data.frame(state.x77) #Roll up by division, calculating max out &lt;- aggregate(df$Population, by = list(state.region), FUN = sum) out ## Group.1 x ## 1 Northeast 49456 ## 2 South 67330 ## 3 North Central 57636 ## 4 West 37899 3.4 Control Structures Much of data science requires developing specialized code to handle the eccentricities of a dataset. Re-running blocks of code is required, often times on multiple data samples and subpopulations. It’s simply not scalable to manually change variables and assumptions of the code everytime. Variables are typically treated differently based on their quality and characteristics. In order to accomplish analytical and programming tasks, control structures are used to determine how a program will treat a given variable given conditions and parameters. In this section, we will cover two commonly used control structures: if…else statements and for loops. 3.4.1 If and If…Else Statement If statements evaluate a logical statement, then execute a script based on whether the evaluated statement is true or false. If the statement is TRUE, then the code block is executed. budget &lt;- 450 if(budget &gt; 400){ #If statement true, run script goes here print(&quot;You&#39;re over budget. Cut back.&quot;) } ## [1] &quot;You&#39;re over budget. Cut back.&quot; In cases where there are two or more choices, if…else statements would be appropriate. In addition to the if() statement, an else statement is included to handle cases where the logical statement is FALSE. budget &lt;- 399 if(budget &gt;= 400){ #If statement true, run script goes here print(&quot;You&#39;re over budget. Cut back.&quot;) } else { #else, run script goes here print(&quot;You&#39;re under budget, but watch it.&quot;) } The complexity of these statements can be as simple as if(x &gt; 10){ print(&quot;Hello&quot;)} more complex trees: age &lt;- 23 if(age &lt;= 12){ print(&quot;kid&quot;) } else if(age &gt;12 &amp;&amp; age &lt;20) { print(&quot;teenager&quot;) } else if(age &gt;=20 &amp;&amp; age &lt;65) { print(&quot;adult&quot;) } else{ print(&quot;senior&quot;) } ## [1] &quot;adult&quot; 3.4.2 For-loops Loops can be used to run the a given statement of code multiple times for a specified number of times or a list of index value. This is a functionality that is available in most programming languages, but the programming syntax will be different. Conceptually, for loops can be likened to an assembly line in a car factory. In order to build a car, a series of well-defined, well-timed processes need to coordinated in a serial fashion. To build 500 cars, the process needs to be executed 500 times. For-loops are essentially the same: Given a well-defined, self-contained process, a process can be be iterativelyapplied to address repetive tasks. Let’s take the following example. The code block essentially says “print values for the range of 1 through 5”, where i is an index value. When executing the statement, R will push the first value in the sequence of 1:5 into the index (in this case, it’s the number 1), then the code block in between the {} (curly brackets) will be executed, treating i as if it’s the number 1. Upon executing the code without error, R will advance to the next value in the sequence and repeat the process until all values in the sequence have been completed. for(i in 1:5){ print(paste0(&quot;Car #&quot;, i)) } ## [1] &quot;Car #1&quot; ## [1] &quot;Car #2&quot; ## [1] &quot;Car #3&quot; ## [1] &quot;Car #4&quot; ## [1] &quot;Car #5&quot; We can do the same for a vector or list of values. In the example below, the vector news contains six terms. Using a for-loop, we can print out each word in the vector. news &lt;- c(&quot;The&quot;,&quot;Dow&quot;,&quot;Is&quot;,&quot;Up&quot;,&quot;By&quot;,&quot;400pts&quot;) for(i in news){ print(i) } ## [1] &quot;The&quot; ## [1] &quot;Dow&quot; ## [1] &quot;Is&quot; ## [1] &quot;Up&quot; ## [1] &quot;By&quot; ## [1] &quot;400pts&quot; For-loops has a few qualities that users should be aware. First, what happens within the for-loop is written to the R environment as global variables. That means that any object (e.g. calculations, models) that is created in the loop will be accessible in the programming enviromment even after the loop ends. This may be a good or bad, depending on the use case: Good if one wants to keep copies of the intermediate results of a loop iteration, but bad if the user is not careful to take note of the potential floor of extraneous objects that may effect downstream calculations. Second, one of the most common mistakes when using loops is failing to record the result of the loop. There are functions in R that are designed to log and package results from loops, but in plain vanilla loops, this is not the case. A common paradigm with for-loops is to iteratively execute repetitive tasks. For example, if a calculation needed to be applied to each of one million files and the results need to be logged, then for-loops are a good option. Typically, the paradigm proceeds as follows: Create placeholder object (e.g. a vector, matrix, or data frame); Initialize loop; and Add outputs to placeholder at the end of each loop iteration. This may be applied in a broad variety of cases such as processes each data set in a repository of many large data sets, calculating complex statistics for various strata and subsets within the data, among others. Best practices with loops start with initializing new placeholder objects to full length before the loop rather than increasing the object size within the loop25. In R, this is particularly important issue for efficient data processing. In the example below, we would like to calculate the minimum and maximum of each of 1000 randomly generated normal distributions with \\(\\mu = 1000\\) and \\(\\sigma = 10\\). To do this, a placeholder data frame x with three columns (iteration, min and max) is created with \\(n = 1000\\) rows for each of the random distributions to be generated. Then, we use Sys.time() to capture when the loop starts and end – a common practice for optimizing code. The loop is initiated for 1 to 1000 iterations to calculate the mininum and maximum. At the end of each iteration, the min and max results are overwritten to the row that corresponds to the iteration in the placeholder x. #Set placeholder data frame with n rows n &lt;- 1000 x &lt;- data.frame(iteration = 1:n, min = numeric(n), max = numeric(n)) #Loop start &lt;- Sys.time() for(i in 1:n){ y &lt;- rnorm(10000, 1000, 10) x$min[i] &lt;- min(y) x$max[i] &lt;- max(y) } Sys.time() - start ## Time difference of 0.95297 secs The above process required roughly 0.8 seconds to process. What happens if the placeholder length were not pre-specified? For the given parameters, the task normally may last between 1.2 and 1.5 seconds. This may not seem to be much time, but at scale with millions if not billions of records and iterations, the time does tend to add up. #Set placeholder data frame without dimensions n &lt;- 1000 x &lt;- data.frame() #Loop start &lt;- Sys.time() for(i in 1:n){ set.seed(i) y &lt;- rnorm(10000, 1000, 10) x &lt;- rbind(x, cbind(iteration = i, min = min(y), max = max(y))) } Sys.time() - start ## Time difference of 1.356999 secs 3.4.2.1 R-specific: apply For-loops are common across all languages, but the efficiency of their implementation will vary. As was described in the previous chapter, R is an interpretted language optimized for mathematical and statistical calculation – quite different than other languages. This means that programming in R is most optimal when vectorizing calculation – linear algebra calculations of vectors and matrices using operations such as +, -, *, %*%, among others. In R, the speed of for-loops may be improved using lapply() under certain circumstances. lapply(), or list apply Whereas the intermediate objects in for-loops are global variables, lapply() creates temporary local variables. #Set n n &lt;- 1000 #Loop start &lt;- Sys.time() x &lt;- lapply(1:n, function(i){ y &lt;- rnorm(10000, 1000, 10) return(cbind(iteration = i, min = min(y), max = max(y))) }) x &lt;- do.call(rbind, x) Sys.time() - start ## Time difference of 0.808532 secs 3.4.3 While Whereas for loops require a range or list of values through which to iterate, while() statements keep iterating until some condition is met. The while() statement is formulated as follows: while([condition is true]){ [execute this statement] } A simple case may involve drawing a random value \\(x\\) from a normal distribution (\\(\\mu = 1.0\\), \\(\\sigma = 0.5\\)) while \\(x\\) is greater than 0.01. x &lt;- 1 while(x &gt; 0.01){ x &lt;- rnorm(1, 1, 0.5) print(x) } ## [1] 0.6867731 ## [1] 1.091822 ## [1] 0.5821857 ## [1] 1.79764 ## [1] 1.164754 ## [1] 0.5897658 ## [1] 1.243715 ## [1] 1.369162 ## [1] 1.287891 ## [1] 0.8473058 ## [1] 1.755891 ## [1] 1.194922 ## [1] 0.6893797 ## [1] -0.1073499 print(&quot;done!&quot;) ## [1] &quot;done!&quot; Exercises Write an if-else statement that classifies a number as positive number as “up” and a negative number as “down”. Then, write a forloop to classify each record of x from x_2 to x_100 is up or down relative to the preceding record Then, use table() to tabular the number of up days versus down days. n &lt;- 500 series &lt;- sin((1:n)/100) + cos((1:n)/80) Fibonacci numbers are defined as \\(F_n = F_{n-1} + F_{n-2}\\), or numbers that are defined as the sum of the preceding two numbers. For example, given an initial sequence of 0, 1, the next five numbers are 1, 2, 3, 5, 8. Using a while() loop, find the Fibonacci number that precedes 1,000,000. Often times, data files are stored in smaller chunks to save space and enhance searchability. In some cases, data is stored in daily chunks. The National Oceanic and Atmospheric Administration (NOAA) releases data every day on environmental and atmospheric conditions, including storms. Download the data using digIt(&quot;hail_201601&quot;, download = TRUE) and unzip the files, then use list.files(), then write a loop to record the following measures in a data frame: month and year number of rows maximum hail size from the maxsize field Answers If-Else Statement. #define series n &lt;- 500 series &lt;- sin((1:n)/100) + cos((1:n)/80) #write if-else for i = 2 temp &lt;- c() if(series[2] &gt;= series[2-1]){ temp &lt;- c(temp, &quot;up&quot;) } else{ temp &lt;- c(temp, &quot;down&quot;) } #set empty vector temp &lt;- c() #loop through if-statement for(i in 2:length(series)){ if(series[i] &gt;= series[i-1]){ temp &lt;- c(temp, &quot;up&quot;) } else{ temp &lt;- c(temp, &quot;down&quot;) } } table(temp) ## temp ## down up ## 267 232 Fibonacci Sequence. #define variables n &lt;- 0 n0 &lt;- 0 n1 &lt;- 1 f &lt;- 0 s &lt;- c() #enter into loop while(f &lt; 1000000){ f &lt;- n0 + n1 n0 &lt;- n1 n1 &lt;- f n &lt;- n + 1 s &lt;- c(s, f) } #get result in the (n-1)th position print(s[n-1]) ## [1] 832040 Hail files. #Download the hail files to the current working directory library(digIt) digIt(&quot;hail_201601&quot;, download = TRUE) #Unzip the zip file unzip(&quot;compressed.zip&quot;) #Get all files that start with hail hail &lt;- list.files(pattern = &quot;^hail_\\\\d{6}&quot;) #create empty dataframe temp &lt;- data.frame() for(rec in hail){ df &lt;- read.csv(rec) maxhail &lt;- max(df$MAXSIZE) date &lt;- regmatches(rec,regexpr(&quot;\\\\d{6}&quot;, rec)) rows &lt;- nrow(df) temp &lt;- rbind(temp, data.frame(max.hail = maxhail, date = date, rows = rows)) } 3.5 Functions Functions are generalizable sets of code that can be used to calculate a single value, process an entire dataset, print graphs, among other things. A strong software engineering habit involves building narrowly defined functions and low-level functions that can be put together to do high-level tasks. A typical function is constructed as follows. The function name is assigned to an object, followed by a list of parameters that will be used as inputs into the function, followed by the script that will be executed using the input parameters. function1 &lt;- function(parameter1, ...){ #Script goes here return([output goes here]) } To execute the function, we will simply need to pass call the function and pass inputs. function1(input1) We can contextualize it by reconstructing a standard function such as the mean() method. mean() accepts accepts a vector vec, sums all values in vec, divides by the length of vec, then returns the result that is passed through return(). #Create dataset n &lt;- 1000 df &lt;- data.frame(id = 1:n, x1 = rpois(n,3), x2 = rpois(n,10), x3 = rpois(n,5), x4 = rpois(n,30), x5 = rpois(n,1), x6 = rpois(n,1), x7 = rpois(n,1), x8 = rpois(n,100)) #Set up Function meanToo &lt;- function(vec){ # # Desc: # Calculate mean of a vector of numeric values # # Args: # vec = vector of values # # Return: # Single mean # res &lt;- sum(vec)/length(vec) return(res) } #Execute meanToo(df$x1) ## [1] 3.023 But what if we wanted to obtain the mean for each row as opposed to each column? That can be achieved using the rowMeans() method, but we can also write a function to replicate the functionality. The function should: Accept the following parameters: data = the data frame, start = index value for the first column in range, end = index for the last column. steps: Create an empty vector output Loop through each row Use the mean.too() function from above, calculate the row mean, append to output Return output as result #Write function rowMeans2 &lt;- function(data, start, end){ # # Desc: # Calculate mean for each rown # # Args: # data = data frame or matrix # start/end = column indices of first and last columns in data # # Return: # Vector of row means # output &lt;- c() for(i in 1:nrow(data)){ output &lt;- c(output, meanToo(data[i, start:end])) } return(output) } #Run function df$means &lt;- rowMeans2(df, 2,9) head(df$means, 10) ## [1] 17.750 19.875 18.750 19.000 19.125 16.500 15.875 18.000 17.500 18.625 3.6 Etiquette Notice how we rely on the meanToo() function that was previously built? There are some guiding principles that’ll ensure that your code is clean, readable, and reusable: Plan your code. Write or draw all the steps that are required to achieve your data processing requirements. Go through each line and cluster the steps into small, discrete modfules that can be relied upon independent of the initial context. For example, an entire data cleansing workflow should be broken into smaller functions rather than be converted into one long function. Make your actions clear. Write your code in a manner that can be re-usable and interpreted by other humans. The code should be self-explanatory. Annotate to make the logic and coding choices clear. For each function, include at least three descriptors so others may use your code: Desc for description of the function, Args for arguments or parameters in the code along with defaults, and Return indicating the output form. For example: Pretty code is readable code. To make readable code, do: Indent lines: indent using two spaces to dependencies such as if-statements, loops, etc.; Spacing: add spaces before and after operators (e.g. 10 + 2 rather than 10+2); Use &lt;- instead of = except for when there are function calls; Limit each line of the code to a common page width (~140 characters). Name objects consistently. Name new data objects and functions should follow a naming convention, such as the Google R Guide. Variable names should be all lower case without punctuation or spaces. If a space is required, replace with a period “.”. Function names should follow the style of “functionName” – no spaces, the first letter of the second word (if any) is capitalized. Most importantly, name functions in a meaninful fashion. As an example of these rules in action: #Function for calculating a mean absolute percentage error mape &lt;- function(actual, predicted, nas = TRUE, text = FALSE){ # # Desc: # Calculates mean absolute percentage error, often used for forecasting # # Args: # actual = vector of original values # predicted = vector of predicted values # nas = logical (default = TRUE) to remove NA values # text = logical (default = FALSE) to return in textual percentage form (e.g. &quot;30%&quot;) # # Return: # A single MAPE value # #Calculate mape out &lt;- mean(abs((predicted / actual)-1), na.rm = nas) #If statement for output options if(text == TRUE){ #return percentage readable text return(paste0(out*100, &quot;%&quot;)) } else{ #return raw return(out) } } #Create data frame of example data df &lt;- data.frame(y = c(1,2,3,4,5), yhat = c(1, 1.2, 3, 3.5, 6)) #Calculate mape for df mape(df$y, df$yhat) Exercises Write a function that replicates the unique() method. N-grams are a sequence of n-number of words in a sentence that are commonly relied upon for natural language processing and text analysis. Take the following sentence from the great statistician John Tukey: “Seek simplicity and distrust it.” 2-grams from this sentence would include: &quot;seek simplicity&quot;, &quot;simplicity and&quot;, &quot;and distrust&quot;, &quot;distrust it&quot;. A 1-gram would be a vector of all words parsed by spaces. Write a function that can return n-grams for any sentence. Answers. Replicate unique(). uniq &lt;- function(vec){ # # Desc: # Deduplicate and return unique values in a vector # # Args: # vec = vector of values # # Return: # A vector of unique values # #create empty vector uniq &lt;- c() #loop through for(i in vec){ if(!(i %in% uniq)){ uniq &lt;- c(uniq, i) } } #return return(uniq) } #Try it out on a short series a &lt;- rep(c(1,2,3), 10) uniq(a) ## [1] 1 2 3 n-grams. ngrams &lt;- function(vec, delimiter, num.grams){ # # Desc: # Produce all unique sequential n-grams for each element in a string vector # # Args: # vec = vector of string values # delimiter = character that separates words # num.grams = number of grams (word combinations) # # Return: # A vector of n-grams # #split characters by vec2 &lt;- unlist(strsplit(vec, delimiter)) #create placeholder then loop through each word grams &lt;- c() for(k in num.grams:length(vec2)){ grams &lt;- c(grams, paste(vec2[k - 1], vec2[k])) } return(grams) } #Test it ngrams(&quot;Seek simplicity and distrust it.&quot;, &quot; &quot;, 2) ## [1] &quot;Seek simplicity&quot; &quot;simplicity and&quot; &quot;and distrust&quot; &quot;distrust it.&quot; 3.6.1 What can you do with control structures? Loops are a critical part of all parts of data science, enabling data cleaning, optimization, and automation. Loops are helpful when an function cannot be applied globally, meaning that each element, column, observation or iteration needs to be done on its own. For example, taking the sum of a random variable x can be done without looping as R is designed to operate with column-wise functionality. However, a moving average of 10 records would require a forloop. Example: Smoothing time series of EIA Gasoline Spot Price Data What if we had a time series dataset with a fair amount of random variability and swings in volume? This sounds very much like financial and economic data – it’s often filled with noise. Let’s take the US Energy Information Administration’s spot price data, specifically the retail gasoline data. An extract has been made available via the digIt() library or from the link https://s3.amazonaws.com/whoa-data/doe_spot_prices_readme.zip. #Call rio library to open library(digIt) df &lt;- digIt(&quot;doe_gas_price&quot;) ## doe_gas_price has been loaded into memory. ## Dimensions: n = 7903, k = 3 #Call rio library to open library(rio) df &lt;- import(&quot;https://s3.amazonaws.com/dspp/doe_spot_prices_readme.xls&quot;) #Inspect the data dim(df) ## [1] 7903 3 head(df,1) ## Date ## 1 1986-06-02 ## New York Harbor Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) ## 1 0.468 ## U.S. Gulf Coast Conventional Gasoline Regular Spot Price FOB (Dollars per Gallon) ## 1 0.445 #Clean and format data colnames(df) &lt;- c(&quot;date&quot;,&quot;ny.values&quot;,&quot;us.gulf.values&quot;) df$date &lt;- as.Date(as.character(df$date), &quot;%Y-%m-%d&quot;) #Plot the data using ggplot library(ggplot2) ggplot(df, aes(date,ny.values)) + geom_line() While here is a pre-built smoothing function known as smooth() that is optimized for this task, we will write a moving average function to illustrate control structures on the ny.values series. moving &lt;- function(vec, lag){ # # Desc: # Produce rolling average # # Args: # vec = numeric vector # lag = number of periods to add to lag # # Return: # A vector of n-grams # new.vec &lt;- rep(NA, lag - 1) #Loop range from *size* to number of rows in vec minus *size* for(i in lag:length(vec)){ #Extract values of *x* from positions i-size to i extract &lt;- mean(vec[(i - lag):i], na.rm = TRUE) #Calculate mean of *extract*, store to the ith value of *new* new.vec &lt;- c(new.vec, extract) } return(new.vec) } Now we can test moving() using a 14-day window and plot the ny.values versus the 14-day moving average. #Calculate 14-day moving average df$new &lt;- moving(df$ny.values, 14) #Plot result ggplot(df, aes(x = date, y = ny.values)) + geom_line(colour=&quot;grey&quot;) + geom_line(data = df, aes(x = date, y = new)) It’s also possible to use loops within loops. What if we wanted to compare multiple window sizes, we can nest one loop inside another. In this case, looping through different potential window sizes helps with identifying the optimal window size. #Vector of windows to be tested windows &lt;- c(7, 91, 182, 364) #Outer loop (index value = *size*) for(size in windows){ #Calculate moving average by window df$new &lt;- moving(df$ny.values, size) #Calculate correlation cor_val &lt;- round(cor(df$new, df$ny.values, use=&quot;complete.obs&quot;), 2) #Plot graph g &lt;- ggplot(df, aes(x = date, y = ny.values)) + geom_line(colour = &quot;grey&quot;) + geom_line(data = df, aes(x = date, y = new), colour = &quot;blue&quot;) + ggtitle( paste(&quot;window =&quot;,size,&quot;, rho =&quot;, cor_val)) #Assign new name for plot object to avoid overwriting results assign(paste0(&quot;g&quot;,size), g) } #Compare graphs. Requires gridExtra library to allow for graph juxtapositionx library(gridExtra) grid.arrange(g7, g91, g182, g364, ncol=2) Note that there are some pre-canned functions that can assist with smoothing; However, coding the function from scratch will provide you with greater flexibility to tackle the task at hand. 3.7 Getting into the mentality Writing just any piece of code is easy. The challenging thing is to ensure that it works well and is repeatable. To be successful in the wild requires the appropriate mindset and an embrace of data etiquette. Data processing is not like a vampiric feeding frenzy of the sort that might be seen on Buffy the Vampire slayer (the TV show): one simply does not arbitrarily chomp at and slice data without any thought as to what comes next. It requires discipline and finesse to acquire a raw piece of information and extract the empirical substance. Hypothetically, imagine that you have conducted a three-month analytics project, rushing to obtain to a result. If asked to provide backup, documentation, and a process, it may very well require an additional few months to decipher your passionate but hasty explorations. Where did this data come from? Why did I take a dot product in this equation? What happened to the raw file? Who is Bob mentioned in this email? Ultimately, the data scientist is in control and should be empowered to conduct data processing in a structured, scientific manner. So, what exact does that mean? Here are five guidelines that make for successful, scalable data science projects: Define a file structure and stick to it. Create a folder for scripts to store all code, another for raw data, a folder for outputs from data processing, and another for documentation. Choose any file structure, but make sure it is consistent and always followed. Think modularly. One of the greatest human inventions is the assembly line. Each step in the process is discrete, requiring workers to specialize in one set of well-defined tasks. As a worker becomes specialized, production speed increases as workers focus on a repetitive task as opposed to switching between tasks. Also, if any step in the process needs to be improved, revised, or replaced, all other steps in the process are not disrupted – only one step needs to be addressed. This standardization of processes is the only way to scale. Coding is the same. Each group of similar tasks can be rolled up into a function that is comprised of generic actions. For example, a function could transform involve downloading a file, unzipping it, loading into memory, and extract a specific set of fields, then saving out the extracted data. Adopt and stick to a coding style. Each file, data field, and function should follow a style so that the logic is cohesive, readable and traceable. The Google R Style Guide, for example, sets easy to follow rules. For example, variable names should be labeled as lower case characters with a &quot;.&quot; as a separator: variable.name. Functions should be named in proper case without spaces: NewFunction or MyFunction. Regardless of the style guide, file names should be meaningful and the only punctuation used should be &quot;_&quot; for spaces and &quot;.&quot; to accompany the file type: gdp_2014_10_02.json. Be vigilent with your code style as it may be the difference between a successful and a failed project. Work forward, not in circles. Raw meats should never be placed with cooked meats. It’s a one way street to keep things clean and sanitary. Data is also a one way street Start from the raw data, make a copy when working with the raw, do thy bidding on the data, then output results or processed data as a new file. Never overwrite the raw file as this is the equivalent to repeatedly refridgerating and re-cooking already cooked meat – the result may be less than desirable in the future. Version up. Each time code is written, modified, amended, or updated, a new version should be saved. Saving can be done in one many ways. If on a local computer, simply adding a version number to the code file name would suffice with an entry to a log file. The log file can be as simple as a text file with time stamps indicating the date fo the edit, the file name and the nature of the edit. Services such as Github are particularly useful for version control and working on code in groups. Following these basic guidelines is the difference between successful, scalable projects and ones that get stuck in the mud. 3.8 DIY 3.8.1 How do I auto-populate text and stences pro forma? In 2014, an automated script was the first produced by the L.A. Times was the first to report on an earthquake around Los Angeles. Everytime the United States Geological Survey issues an earthquake over a certain severity level, the robot is able to pick out key pieces of information and populate an article pro forma. In the wild, many well-structured data sources that are issued as alerts are well-suited for automation. What if we have the following statement. What if specific information could be auto-populated? original &lt;- &quot;Maria was a Category 5 with max wind speeds of 175 mph.&quot; We need to first find key items and replace with unique placeholders. The unique placeholders can be used as a standardized search. Notice that each modification to the original string original is assigned to the new object warn.line in order to avoid overwriting the gold copy. #Replace Category 5 with &lt;hurricane_level&gt; warn.line &lt;- gsub(&quot;Category \\\\d&quot;, &quot;Category &lt;hurricane_level&gt;&quot;, original) #Search for Maria as a five-letter word with &lt;name&gt; warn.line &lt;- gsub(&quot;\\\\b[[:alpha:]]{5}\\\\b&quot;, &quot;&lt;name&gt;&quot;, warn.line) #Replace 175 with &lt;wind_speed&gt; (warn.line &lt;- gsub(&quot;\\\\d{3}&quot;, &quot;&lt;wind_speed&gt;&quot;, warn.line)) ## [1] &quot;&lt;name&gt; was a Category &lt;hurricane_level&gt; with max wind speeds of &lt;wind_speed&gt; mph.&quot; As information becomes available, this sentence can be populated with event-specific information. Below is a list of a few hurricanes that occurred in the 2017 season. hurricanes &lt;- rbind(data.frame(storm.name = &quot;Irma&quot;, speed = 185, level = 5), data.frame(storm.name = &quot;Jose&quot;, speed = 155, level = 4), data.frame(storm.name = &quot;Katia&quot;, speed = 105, level = 2), data.frame(storm.name = &quot;Harvey&quot;, speed = 130, level = 4)) To auto-populate the sentence, we can write a simple function that accepts key inputs, then outputs a storm summary. Note that the function includes descriptive text to help future users understand how to use the code. warningText &lt;- function(model.sentence, storm.name, level, speed){ # # Converts key hurricane information into summary text # # Args: # model.sentence = model sentence with markers # storm.name = string containing storm name to replace &lt;name&gt; # level = hurricane level to replace &lt;hurricane_level&gt; # speed = maximum speed in mph to replace &lt;wind_speed&gt; # # Result: # Storm summary populated with storm details # statement &lt;- gsub(&quot;&lt;hurricane_level&gt;&quot;, level, model.sentence) statement &lt;- gsub(&quot;&lt;wind_speed&gt;&quot;, speed, statement) statement &lt;- gsub(&quot;&lt;name&gt;&quot;, storm.name, statement) return(statement) } With the function, we can loop through each row of the data frame to generate a sentence describing each storm. for(i in 1:nrow(hurricanes)){ print(warningText(model.sentence = warn.line, storm.name = hurricanes$storm.name[i], level = hurricanes$level[i], speed = hurricanes$speed[i])) } ## [1] &quot;Irma was a Category 5 with max wind speeds of 185 mph.&quot; ## [1] &quot;Jose was a Category 4 with max wind speeds of 155 mph.&quot; ## [1] &quot;Katia was a Category 2 with max wind speeds of 105 mph.&quot; ## [1] &quot;Harvey was a Category 4 with max wind speeds of 130 mph.&quot; 3.8.2 What is the overlap between these two lists? Motivation Bill de Blasio, the 109th Mayor of New York City, was not always known as Bill de Blasio. In fact, he has changed his name twice and he has had three legal names, including Warren Wilhelm Jr., Warren de Blasio-Wilhelm and Bill de Blasio.26 His current name first appeared in 1990 as a working name and was only officially with a court petition in 2001. In addition, his close network addresses him as Billy.27 While his entities are well-covered in the press and are common knowledge for New Yorkers, to others it may not be. Name changes are quite common throughout society. Some people may choose to change part of or their entire name at major life events, such as at joyous occassions like marriage or more clandestine efforts with witness protection. The way in which people refer to themselves colloquially also tends to differ such as nicknames (e.g. Bob = Robert, Dick = Richard, Jen = Jennifer) and stage names (e.g. George Michael = Georgios Kyriacos Panayiotou, Stevie Wonder = Stevland Hardaway Judkins). Organizations may use the same acronyms to refer to themselves. CIA can refer to the clandestine Central Intelligence Agency or the more gastronomically inclined Culinary Institute of America. In data science, entity resolution (ER) or the disambiguation of names and entities is paramount in the early stages of developing data sets for analysis and application. ER is also referred to as record linakge and as deduplication in certain contexts – essentially being able to systematically map aliases to a canonical identifier. This process goes beyond simply names and spellings, but other identiable information that can be used to triangulate upon an entity or identity. For example, a unique identifier in the US is the social security number (SSN). Composite identifiers can be developed using a number of pieces of information, such as the last four digits of the social security number (SSN4), a person’s last name, and birth date. For businesses, an identifier might be the tax identification number (TIN) or even commercial email address in certain cases. Through successfully resolving entities, fundamental business analysis problems can be solved: How many people do I have in these customer lists? How many customers return more than once per year? How many records overlap between these two lists? I have two data sets with data about people. How do I combine them to augment my knowledge about those people? Principles The goal of entity resolution is to get entity \\(A\\) in one set of information to equate \\(A\\) in another set of information. On one level, getting identifiers to line up, such as names, is a matter of ensuring that characters in two sets of information are the same. On another level, it is a matter of identifying the right combination of fields that have enough identifiable information to make the linkage. There are a broad set of complex techniques that can be applied to achieve this task. To start, the mastering the following can go a long way: Text may be processed to remove punctuation and spacing as well as standardize capitalization. For example, Bill de Blasio-Wilhelm and billdeblasiowilhelm are the same name. Misspellings and alternative spellings are a common problem. In NYC, Broadway has been known to be spelled as B-way, B’way, Bwy, Bdway, among others. Fuzzy matching can be used to find candidate matches among strings based on how they sound or are spelled words. Phonetic algorithms, such as Soundex, are useful with indexing names based on how they sound in English as opposed to their spelling. In Soundex, for example, names are represented as a letter and three numbers as derived by encoding rules28 The soundex for the name “Boston” is “B-235”. Other names that would have the same soundex are “Bostin”, “Bawstin”, and “Bastin” – all of which would be phonetic matches. When spelling matters, methods such as Levenshtein Distance or Edit Distances can be used to calculate the number of character insertions, deletions and substitutions are required to convert a source string into a target – essentially a character similarity measure. The Levenshtein Distance from “Boston” to “Bawstin” is 3 and to “Bostin” is 1. Note that matching based on Levenshtein Distance would require the user to specify an acceptable cutoff distance. Disambiguation is reliant on linking known names to a canonical name. Bill can be linked to Billy and Warren in de Blasio’s case, but not in the case of Bill Clinton or Bill Nye. Typically, these linkages are surfaced through manual investigation and keeping track of the “Also Known As” or “AKA” may accomplish more than any text manipulation ever could. A Worked example Publicly available PII is becoming increasingly abundant due to cybersecurity breaches. But, published PII is officially published on sanction lists and watch lists. A number of nations and governing bodies publically publish such lists of enemies of the state and their many aliases so that companies and people conducting global commerce can follow international sanctions. Indeed, the value of data here is one of a serious and grave nature, but without such lists, diplomacy and policy is hard to implement and enforce. For this example, four sanctions lists have been assembled: US Consolidated Screening List: http://2016.export.gov/ecr/eg_main_023148.asp UK Financial sanctions targets: list of all targets: https://www.gov.uk/government/publications/financial-sanctions-consolidated-list-of-targets/consolidated-list-of-targets UN Sanctions List: https://scsanctions.un.org/resources/xml/en/consolidated.xml EU Sanctions List: http://ec.europa.eu/external_relations/cfsp/sanctions/list/version4/global/global.xml For simplicity, we will focus only on the UN and EU lists to create a cleaned set of names in order to determine the number of unique entities in each list, then conduct matching to determine the overlap. To start, we will directly read the EU and UN data using the digIt() library. library(digIt) un &lt;- digIt(&quot;watch_list_un&quot;) eu &lt;- digIt(&quot;watch_list_eu&quot;) As a first step, we will examine a few records. There are quite a few fields that could be used for matching such as name, birth date, and citizenship. Typically, it is best to clean and prepare multiple fields for matching. Table 3.6: Comparison of EU and UN lists. EU Variables UN Variables Example id 13 rec_type individual legal_basis 1210/2003 (OJ L169) id 6908048 reg_date 2003-07-07 version_number 1 lastname Hussein Al-Tikriti firstname SADDAM firstname Saddam secondname HUSSEIN middlename thirdname AL-TIKRITI wholename Saddam Hussein Al-Tikri un_list_type Iraq gender M ref_num IQi.001 title listed_on 2003-06-27 func_role comments language designation birthdate 1937-04-28 citizen_country Iraq birth_place al-Awja, near Tikrit list_type UN List birth_country IRQ date_updated passport_number alias Abu Ali passport_country alias_quality Low citizen_country IRQ country pdf_link http://eur-lex.europa.e birthdate 1937-04-28 programme IRQ birthdate_type EXACT birth_place al-Awja, near Tikrit birth_country Iraq passport_number passport_country sort_key sort_key_mod For this example, we will rely largely on the EU wholename field and construct a similar fields from the UN list using firstname, secondname, and thirdname. A first step in preparation is to fill all NA values with an empty quotation as concatenating multiple NA values will be erroneously interpretted as a string value “NA” (e.g. “Saddam NA Hussein”). As a cursory check, we match both the eu and un data sets by the wholename field, which reveals that only four of hundreds of records are readily matchable. #PRE-PROCESSING eu[is.na(eu)] &lt;- &quot;&quot; un[is.na(un)] &lt;- &quot;&quot; #Concatenate names in UN set un$wholename &lt;- paste(un$firstname, un$secondname, un$thirdname) #Test straight up matches (only for records that match without any changes) base.merge &lt;- merge(eu, un, by = &quot;wholename&quot;) paste(&quot;Number of matched rows =&quot;,nrow(base.merge)) ## [1] &quot;Number of matched rows = 4&quot; To build out the entity resolution process, we will need to write two functions. The first function to be named cleanEntity() will clean, deduplicate and standardize a vector of names. Computers interpret characters as is, thus a capital “A” is not he same a a lower case “a” and one space is not the same as a tab indent. To clean, spaces and punctuation will be stripped out. In addition, all characters will be turned into lower case and trim excess white space from the beginning and end of each string. Deduplication will be conduct on the resulting cleaned string vector. This is a key step to ensure that matching is conducted as close to a 1:1 basis, otherwise we run the risk of a Cartesian Product (all duplicate matches can matched to one another, thereby multiplying the number of matched records). The result is a data frame containing both the original whole name and the cleaned whole name. #Write function to cleaning data cleanEntity &lt;- function(x){ # # Desc: # Accepts a vector of names, returns a cleaned, deduplicated, data frames # # Args: # x = a string vector # # Returns: # Data frame with two fields: Original string and a cleaned name x &lt;- x[!duplicated(x)] lower &lt;- trimws(tolower(x)) nopunct &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, lower) nospace &lt;- gsub(&quot;[[:space:]]&quot;, &quot;&quot;, nopunct) return(data.frame(original = x, cleaned = nospace)) } As a proof of concept, we run the cleanEntity() function and examine the first five records. For the most part, the names look fairly standardized. #Check to see that it works test.match &lt;- cleanEntity(eu$wholename) head(test.match, 5) Table 3.7: Cursory comparison of original and cleaned string vectors original cleaned Robert Gabriel Mugabe robertgabrielmugabe Saddam Hussein Al-Tikriti saddamhusseinaltikriti Qusay Saddam Hussein Al-Tikriti qusaysaddamhusseinaltikriti Uday Saddam Hussein Al-Tikriti udaysaddamhusseinaltikriti Abid Hamid Mahmud Al-Tikriti abidhamidmahmudaltikriti The next step is to write a function called canonicalNames() that is designed to find which raw, uncleaned name in one data set matches with a raw, uncleaned name in a second data set. This is accomplished by translating two vectors of names into a standardized form, then conducts matching. The function starts by using cleanNames() to standardize the two name vectors. Notice the modularity – how a user-defined function is designed to work as part of a more complex function. Upon transforming each vector, canonicalNames() conducts two rounds of matching: one with the uncleaned names as these contain arguably the highest quality matches, then another round on cleaned names. The result of this function is a data frame that contains matched, untransformed names that can be used as a key to join data between data sets. canonicalNames &lt;- function(a, b){ # # Desc: # Accepts two vectors of identifiers, returns matches # # Args: # a and b are string vectors of names # # Result: # A matched list with original names in each dataset #Clean Data a &lt;- cleanEntity(a) b &lt;- cleanEntity(b) #Change field names colnames(a) &lt;- paste0(&quot;a.&quot;,colnames(a)) colnames(b) &lt;- paste0(&quot;b.&quot;,colnames(b)) #Match on originals overlap1 &lt;- merge(a, b, by.x = &quot;a.original&quot;, by.y = &quot;b.original&quot;) overlap1$step &lt;- &quot;original&quot; overlap1$b.original &lt;- overlap1$a.original print(paste0(&quot;Original: # matches = &quot;, nrow(overlap1))) #Match on cleaned overlap2 &lt;- merge(a, b, by.x = &quot;a.cleaned&quot;, by.y = &quot;b.cleaned&quot;) overlap2$step &lt;- &quot;cleaned&quot; print(paste0(&quot;Cleaned: # matches = &quot;, nrow(overlap2))) #Create master master &lt;- rbind(overlap1[,c(&quot;a.original&quot;,&quot;b.original&quot;, &quot;step&quot;)], overlap2[,c(&quot;a.original&quot;,&quot;b.original&quot;, &quot;step&quot;)]) #De-dupe master &lt;- master[ !duplicated(paste(master$a.original), master$b.original), ] print(paste0(&quot;Total de-duplicated matches = &quot;, nrow(master))) return(master) } When we apply these functions to the UN and EU data, we find \\(n = 667\\) matches out of \\(n = 1046\\) in the UN data set and \\(n = 2016\\) in the EU data set. eu.un &lt;- canonicalNames(un$wholename, eu$wholename) A closer examination of matches reveals that punctuation and capitalization accounts for the majority of the differences between names. This, however, omits name disambiguation. tail(eu.un, 6) ## [1] &quot;Original: # matches = 4&quot; ## [1] &quot;Cleaned: # matches = 671&quot; ## [1] &quot;Total de-duplicated matches = 667&quot; Table 3.8: Comparison of Matched Records a.original b.original step 662 ZAFAR IQBAL Zafar Iqbal cleaned 663 ZAKARYA ESSABAR Zakarya Essabar cleaned 664 ZAKI-UR-REHMAN LAKHVI Zaki-ur-Rehman Lakhvi cleaned 665 ZIA-UR-RAHMAN MADANI Zia-ur-Rahman Madani cleaned 666 ZULKARNAEN Zulkarnaen cleaned 667 ZULKIFLI ABDUL HIR Zulkifli Abdul Hir cleaned #JOINING un.new &lt;- merge(un, eu.un, by.x = &quot;wholename&quot;, by.y = &quot;a.original&quot;, all.x = TRUE) eu.new &lt;- merge(eu, eu.un, by.x = &quot;wholename&quot;, by.y = &quot;b.original&quot;, all.x = TRUE) joint &lt;- merge(un.new, eu.new, by.x = &quot;b.original&quot;, by.y = &quot;wholename&quot;) Exercises Conduct the same process on the US and UK data sets using the functions that you have written for cleaning and resolution. Determine the total number of people who overlap between the UK and UN sanction lists, doing so also considering primary and secondary (AKA) aliases. In the UN file, aliases are listed under the alias field. In the UK list, entities are listed in long form, thus use a combination of the Alias.Type and Group.ID fields to identify unique individuals. Note that matching may require more than just character changes to names. 3.8.3 What do I do find trends in transactional or event-level data? Motivation Before entering the world of statistical models and algorithms, cursory analysis of data is reliant on aggregation. Patterns emerge when the most granular records are aggregated,29 meaning that as information is summed and averaged by some unit of analysis, some aggregated units will be unambiguously higher than others – a sustained deviation away from random. But how exactly are records aggregated into summary statistics? Investment analysts and in the earth scientists employ rolling averages – averaging data over a longer time window. The size of the window depends on what the data is being used for and the frequency of the data. Stock trades are refreshed in real-time, thus the rolling average window may be only minutes or hours. Satellite imagery on vegetation may be as frequent as daily or every two weeks, meaning the rolling average may be quite a bit wider such as a monthly-level average. For measures concerning the atmosphere, earth scientists will calculate a 30-year average that is known as a climate normal that shows the historically prevailing conditions. Demographers and economists count the number of people in the United States at the address of residence every 10 years. Since the data is personally identifiable information or PII, it is aggregated by discrete units of geography known as Census blocks, which is defined as a boundary that contains “at least 30,000 square feet (0.69 acre) for polygons bounded entirely by roads or 40,000 square feet (0.92 acres) for other polygons.”30 These blocks neatly roll up into higher units of analysis such as Census Block Groups, Census Tracts, Counties and States. It is easy to aggregate data in a way that emphasizes a misleading pattern. Perhaps the best known example is gerrymandering, which is the act of “drawing political boundaries to give your party a numeric advantage over an opposing party.”31 In theory in a representative democratic model, the proportion of the popular vote that is for a given party should be approximately equal to the number of elected congresspeople in the US House of Representatives. In 2012, while democrats had won roughly 49 percent of the House vote compared to 48.2 percent for Republicans, the scales swung in the favor of the Republicans winning a 234 seats and the Democrats with only 201 seats.32 We will thus be vigilant in ensuring that data analytics is free of gerrymandered insights. Questions this answers By aggregating data into more meaningful units, we can begin to answer sim - When is the most busy period of the year? - Which units move together? - Principles Better to obtain raw transactional data so that you have more control and options for aggregating data Keep in mind that when sample is not randomly sampled or is not the entire potential universe, aggregation can be misleading How you aggregate depends on the use of the data and the methods that one is comfortable with using. A Worked example The Washington Area Transit Authority (WMATA or colloquially known as “The Metro”) reported that its 2017Q1 performance attained a train arrival on-time rate of 69%. This is a cursory, high-level measure that does not provide much insight. What data underlies those estimates? Washington Metropolitan Area Transit Authority 2017Q1 Performance Report - Source: https://www.wmata.com/about/records/scorecard/index.cfm By scraping the WMATA website, it is possible to dive deeper into the delay patterns to develop a better understanding of delay patterns across the metro system, which in turn can help riders of the Metro gain insight about smart planning. The data set that has been assembled for this exercise was scraped for the period of July 2016 to July 2017. What answers should we attempt to answer through simple aggregation? How long are delays? When are delays most likely to occur? Is there a time of year when delays are likely? Which line has the longest delays To start, we load in the data from the digIt library. library(digIt) wmata &lt;- digIt(&quot;wmata&quot;) First we’ll take a cursory glance at the data. To answer the questions, we will need only a few fields, Table 3.9: Example delay records field 1 1000 3000 date July 30, 2016 August 17, 2016 December 09, 2016 dayofweek Saturday Wednesday Friday link https://www.wmata. https://www.wmata. https://www.wmata. line Red Red Green direction Glenmont Grosvenor Branch Avenue delay 7 11 20 time.occur 11:27AM 6:10AM 11:12PM text 11:27 am A Glenmo 6:10 am A Grosveno 11:12 pm A Branch reason.spacing 0 0 0 reason.medical 0 0 0 reason.police 0 0 0 reason.brake 0 0 1 reason.equipment 0 0 0 reason.door 0 0 0 reason.operational 0 0 0 reason.disabled.tr 0 0 0 reason.arcing.insu 0 1 0 outcome.offboard 0 0 0 outcome.singletrac 0 0 0 outcome.shuttle 0 0 0 in.station Shady Grove Friendship Heights Columbia Heights bw.station.1 bw.station.2 We’ll now create a date-time object. To do so, we’ll use paste() to concatenate date and time separated by a space and assign to a new vector. date.time &lt;- paste(wmata$date, wmata$time.occur) date.time[1] ## [1] &quot;July 30, 2016 11:27AM&quot; This new vector date.time is then processed using strptime() to convert strings into what is known as POSIXlt, a class of data objects that represent the number of seconds relative to the beginning of 1970 UTC.33 This makes it easier to keep track and calculate time at a granular level. The tricky part of this process is to accurately representing the time using conversion specifications. For the date above, the following specification is most appropriate: spec &lt;- &quot;%B %d,%Y %I:%M%p&quot; wmata$datetime &lt;- strptime(date.time, spec) From the new date-time object, we can use format() to extract parts elements from POSIXlt, such as months (%m) and hours (%H). wmata$month &lt;- as.numeric(format(wmata$datetime, &quot;%m&quot;)) wmata$hour &lt;- as.numeric(format(wmata$datetime, &quot;%H&quot;)) With the data prepped, we can proceed to roll up data by different dimensions. To start, We’ll calculate the average, standard deviation and sample size for each station. wmata.line &lt;- aggregate(wmata$delay, by = list(hour = wmata$hour), FUN = mean, na.rm = TRUE) colnames(wmata.line) &lt;- c(&quot;station&quot;,&quot;avg&quot;) options(knitr.kable.NA = &#39;&#39;) knitr::kable(wmata.line, caption = &quot;Average delay&quot;, digits = 1, row.names = FALSE, booktab = TRUE) Table 3.10: Average delay station avg 0 14.4 1 10.0 4 21.6 5 9.3 6 7.2 7 7.5 8 7.4 9 8.4 10 9.1 11 9.5 12 9.2 13 8.8 14 9.0 15 6.9 16 6.8 17 8.4 18 8.5 19 10.1 20 9.4 21 11.3 22 10.9 23 11.1 Exericse In 10 or fewer lines of code, write a function to extract the time at which train delays occurred and how long they lasted. The function should accept a string vector and return a data frame with two fields: a string field that contains the hour, minute, and time of day (AM or PM) time and a numeric field for the delay in minutes. Include standard annotation in the function to indicate how to use it. Test your function on the ‘text’ field in the wmata data set, then check the accuracy of results against the time and delay fields. Tip: Use regular expressions! 3.8.4 I have a lot of text. How do extract basic keywords? Motivation Tabular data can sometimes be a luxury. For it to exist, it requires someone to spend time and effort to meticulously collect and structure data into a clean, well-defined format. Textual data, in contrast, does not have structure. As is obvious to most people, text conveys meaning, but not all of the words in a document are necessary to understand its contents. We naturally search for keywords and groups of keywords, which then become structured information. From the structure information, we can glean what topics are contained in numerous documents. When done at scale, it becomes possible to traverse boundless amounts of textual information and sift the things that matter. This is the basic idea of text processing and natural language processing (NLP). Text and language can be manipulated to glean insight at scale, enabling one to answer questions such as: What are keywords best describe a text? (e.g. tagging) How do two texts relate to one another? Which other texts are similar? What’s the tone of a corpus of documents? What distinct topics are mentioned in the text and by whom? Principles NLP is a vast and growing field. Similar to the example presented at the beginning of the chapter, basic text manipulation starts with tokenization, or the process of parsing a character sequence or string into smaller pieces referred to as tokens. In the case of a sentence, a token may be a word, but it also may take on the form of a n-word phrase. The goal is to convert strings into smaller more comparable units. Thus, before tokenization is applied to a sentence, strings are converted into comparable formats such as capitalization (tolower() or toupper()), without punctuation (gsub()), among others. Upon cleaning the text, n-grams – a sequence of n-sequential tokens – can be derived from each sentence. For example, let’s suppose we were to tokenize the following two sentences: Maya is a physical scientist. Olivia is a data scientist. From these two sentence, we can derive a vector of uni-grams and bi-grams for each sentence: Table 3.11: n-grams for two sentences Sentence 1-gram 2-grams 1 Maya Maya is 1 is is a 1 a a physical 1 physical physical scientist 1 scientist 2 Olivia Olivia is 2 is is a 2 a a data 2 data data scientist 2 scientist This should result in a vector of many words and short phrases. Notice that short sentences quickly grow into a larger number of records. Imagine when a corpus of documents contains thousands of records that need to be analyzed. To whittle down the data to essential terms, stop words such as “is and”an&quot; can be removed, which remove words that are essentially the padding that makes language sound good. Removing stop words also reduces the storage and process requirements. From the vector of words, term frequencies can be tabulated for each sentence or document. In some cases, term frequencies can be used to identify topics or represent the absolutely significance of certain words. But, more often than not, the relative importance of words is more meaningful. Term Frequency - Inverse Document Frequency (TF-IDF), a simple term re-weighting calculation, can vastly improve ranking of importance of words by converting frequencies into values that reflect relative importance between textual documents. TF-IDF is widely used in information indexing and search systems to help re-rank documents based on terms The calculation is as follows: \\[\\text{TF-IDF} = \\frac{n_{it}}{n_i} \\times ln(\\frac{N}{N_t})\\] where \\(n_t\\) is the number of times term \\(t\\) appears in a document \\(i\\), \\(n\\) is the number of terms in document \\(i\\), \\(N\\) is the total number of documents, and \\(N_t\\) is the number of documents that contain term \\(t\\). This two step calculation compares the relative prevalence of a term in a document and scales it by that term’s prevalence in the corpus – a simple, but powerful analytical trick. From these fundamental processing steps, more advanced techniques can be built on top such as topic modeling, which is used to find latent groups of topics within documents, as well as build content-based recommendation engines that suggest products based on how the qualities of a product overlap with a consumer’s interest profile. There are plenty of libraries designed to make text processing easy, such as tm and tidyr, but to understand the underlying mechanics, we will illustrate basic processing with base R functionality. A Worked example Political scientists and journalists often times count the number of times Congress applauds the President when delivering the State of the Union (SOTU) Address as well as analyzes the number of times words are used. While it is not a clear science, applying data manipulation techniques to create an analyzable dataset can certainly be fascinating. To illustrate a real clean up workflow with some data manipulation, we will use the SOTU transcripts from the Obama Administration. An interesting attribute of the transcripts are that they reflect the number of applause breaks as planned for by the speechwriters and policymakers as opposed to the actual number. Using this data, we will answer the following three questions: How many breaks were planned in 2010 vs 2016? What were the top 10 words used in each of those state of the unions? Which words experienced relatively greater use? How many planned applause breaks in 2010 versus 2016? To start, let’s load just the 2010 data into memory and inspect one paragraph from the speech. Looking at the data, the speechwriters included queues for applause as denoted as (Applause.). The 2010 data can be accessed using the digIt() function library(digIt) speech &lt;- digIt(&quot;speech_2010&quot;) or using readLines() to import the data directly via from the Amazon Web Services server that hosts the data: speech &lt;- readLines(&quot;https://s3.amazonaws.com/dspp/speech_2010.txt&quot;) Upon loading the data, we remove blank lines and take a look at a randomly selected line in the speech. speech &lt;- speech[speech!=&quot;&quot;] speech[12] ## [1] &quot;It&#39;s because of this spirit -\\xd0 this great decency and great strength -\\xd0 that I have never been more hopeful about America&#39;s future than I am tonight. (Applause.) Despite our hardships, our union is strong. We do not give up. We do not quit. We do not allow fear or division to break our spirit. In this new decade, it&#39;s time the American people get a government that matches their decency; that embodies their strength. (Applause.) &quot; Using that piece of information, we can write a relatively short set of steps to match the Applause pattern. 57 breaks were planned in 2010, which is 11 more than the 46 breaks planned in 2016. While the same code was run two separate times, we will learn in a subsequent chapter how to automate repetitive tasks. #2010 #read in lines from the text speech10 &lt;- digIt(&quot;speech_2010&quot;) ## speech_2010 has been loaded into memory. ## Dimensions: # lines = 223 #remove any blank lines speech10 &lt;- speech10[speech10!=&quot;&quot;] #get string position of each Applause (returns positive values if matched) ind &lt;- regexpr(&quot;Applause&quot;, speech10) sum(attr(ind,&quot;match.length&quot;)&gt;1) ## [1] 57 #2016 speech16 &lt;- digIt(&quot;speech_2016&quot;) ## speech_2016 has been loaded into memory. ## Dimensions: # lines = 172 speech16 &lt;- speech16[speech16!=&quot;&quot;] ind &lt;- regexpr(&quot;Applause&quot;, speech16) sum(attr(ind,&quot;match.length&quot;)&gt;1) ## [1] 46 What were the top words in 2010 vs 2016 To do this, we’ll need to do some basic cleaning to start (e.g. remove punctuation, remove numbers, remove non-graphical characters like \\r), parse the words into a vector of words or ‘bag of words’, and aggregate words into word counts. #2010 #Clean up and standardize values clean10 &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,speech10) clean10 &lt;- gsub(&quot;[[:digit:]]&quot;,&quot;&quot;,clean10) clean10 &lt;- gsub(&quot;[^[:graph:]]&quot;,&quot; &quot;,clean10) #convert into bag of words bag10 &lt;- strsplit(clean10,&quot; &quot;) bag10 &lt;- tolower(trimws(unlist(bag10))) #Count the number of times a word shows up counts10 &lt;- aggregate(bag10, by=list(bag10), FUN=length) colnames(counts10) &lt;- c(&quot;word&quot;,&quot;freq&quot;) counts10$len &lt;- nchar(as.character(counts10$word)) counts10 &lt;- counts10[counts10$len&gt;2,] counts10 &lt;- counts10[order(-counts10$freq),] head(counts10, 10) ## word freq len ## 1478 the 338 3 ## 76 and 237 3 ## 1476 that 149 4 ## 1030 our 120 3 ## 87 applause 116 8 ## 603 for 84 3 ## 1649 will 61 4 ## 1493 this 60 4 ## 91 are 57 3 ## 695 have 53 4 #2016 clean16 &lt;- gsub(&quot;[[:punct:]]&quot;,&quot;&quot;,speech16) clean16 &lt;- gsub(&quot;[[:digit:]]&quot;,&quot;&quot;,clean16) clean16 &lt;- gsub(&quot;[^[:graph:]]&quot;,&quot; &quot;,clean16) bag16 &lt;- strsplit(clean16,&quot; &quot;) bag16 &lt;- tolower(trimws(unlist(bag16))) counts16 &lt;- aggregate(bag16, by=list(bag16), FUN=length) colnames(counts16) &lt;- c(&quot;word&quot;,&quot;freq&quot;) counts16$len &lt;- nchar(as.character(counts16$word)) counts16 &lt;- counts16[counts16$len &gt; 2,] counts16 &lt;- counts16[order(-counts16$freq),] head(counts16, 10) ## word freq len ## 1366 the 284 3 ## 57 and 193 3 ## 1364 that 146 4 ## 935 our 94 3 ## 70 applause 89 8 ## 530 for 59 3 ## 1377 this 41 4 ## 1521 who 41 3 ## 898 not 40 3 ## 182 but 39 3 Looking at the words above, it feels a bit unsatisfying. To improve the list, we’ll use a stop word list to remove words that hold little meaning (e.g. the padding language). #Import and remove stop words stopwords &lt;- digIt(&quot;stopwords&quot;) ## stopwords has been loaded into memory. ## Dimensions: n = 543, k = 1 stopwords &lt;- as.vector(stopwords) #Remove stop words counts10 &lt;- counts10[!(counts10$word %in% stopwords),] counts16 &lt;- counts16[!(counts16$word %in% stopwords),] In addition, the importance of words may not be well-represented using term frequencies. The words “America” and “Freedom” are likely to appear in many SOTU speeches, but do not reflect the distinct foci of each address. We can write a simple function to calculate TF-IDF in order to surface terms of relative importance with respect to each speech. tfidf &lt;- function(terms, freq, doc){ # # Desc: # Returns a TF-IDF index for a set of terms and documents # # Args: # terms = vector of terms # freq = vector of frequencies for each term # doc = vector of document membership # # Returns: # TF-IDF values for each term by document #Calculate components N &lt;- length(unique(doc)) Nt &lt;- aggregate(doc, by = list(terms), FUN = length) colnames(Nt)[2] &lt;- &quot;Nt&quot; nit &lt;- freq ni &lt;- aggregate(freq, by = list(doc), FUN = sum) colnames(ni)[2] &lt;- &quot;ni&quot; #Combine out &lt;- data.frame(terms, doc, N = N, nit = nit) out &lt;- merge(out, Nt, by.x = &quot;terms&quot;, by.y = &quot;Group.1&quot;, all = T) out &lt;- merge(out, ni, by.x = &quot;doc&quot;, by.y = &quot;Group.1&quot;, all = T) out$tfidf &lt;- (out$nit/out$ni) * log(out$N / out$Nt) #Return return(out[order(-out$tfidf), c(&quot;doc&quot;, &quot;terms&quot;, &quot;tfidf&quot;)]) } We can now reweight the terms of the two speeches. Note that TFIDF values will become more distinct with a greater diversity of textual documents. #Append data sets together master &lt;- rbind(data.frame(doc = 2010, counts10), data.frame(doc = 2016, counts16)) #Weight results reweighted &lt;- tfidf(terms = master$word, freq = master$freq, doc = master$doc) #Results head(reweighted[reweighted$doc==2010,], 10) head(reweighted[reweighted$doc==2016,], 10) doc terms tfidf doc terms tfidf 2010 lets 0.0017457 2016 isil 0.0013106 2010 bill 0.0015129 2016 doesn 0.0011650 2010 theyre 0.0015129 2016 him 0.0011650 2010 values 0.0013965 2016 voices 0.0011650 2010 cant 0.0012802 2016 don 0.0008737 2010 deficit 0.0011638 2016 opportunity 0.0008737 2010 home 0.0009310 2016 elected 0.0007281 2010 recovery 0.0009310 2016 needs 0.0007281 2010 americas 0.0006983 2016 planet 0.0007281 2010 house 0.0006983 2016 almost 0.0005825 {12pt} At this point, we’ve arrived at words that are more presidential sounding, but can still be whittled down to the core message. But that can be left for another day. Exercises Modify the code to compare the 2010 SOTU to the 2012 SOTU. Optimize the above code to import and process 2010 through 2016 SOTUs to estimate the TF-IDF values. 3.8.5 Edge detection of images? Motivation Ever wonder how an algorithm can effortlessly identify the outline of an object in a photograph? It may seem like magic, but it is really a matter of basic mathematical operations along a grid. To build up to the magic, we begin with a few basic concepts. A digital photograph is comprised of pixels, which are arrange at equal intervals in a grid. From a data perspective, pixels can hold multiple values. A photograph that is captured in RGB or Red-Green-Blue contains three channels of information based in different parts of the visible light spectrum. Each pixel in turn holds three values – one for each color, which can be brought together to represent what a camera witnessed at the time of capture. A grayscale image has only one channel that can take one of 255 unique values. A RGB photograph is essentially three overlaid matrices. A grayscale image is one matrix where each pixel is one cell in the matrix. Suppose we were to look at one row of pixels in a photograph. As we progress from one side to the other, the pixel values may change – sometimes due to noise and sometimes due to different objects that are captured when composing a photograph. A meaningful change in the pixel values along each row and each column can be viewed as an edge: an edge is defined as a point in an image in which values transition. The noise in the photograph make edge detection a challenge, thus noise should be removed by blurring the image, or averaging values around each pixel in some consistent manner. The before and after results for a given row or column of pixels may resemble the result below. ## Warning in c(rep(5, 20), 200/(1 + exp(20:-20))) + 30 - 30 * runif(41): ## longer object length is not a multiple of shorter object length ## Warning in c(rep(5, 20), 200/(1 + exp(20:-20))) + 30 - 30 * runif(41) + : ## longer object length is not a multiple of shorter object length Figure 3.1: Example of raw and smoothed greyscale values along a row of pixels Next, detecting an edge is a matter of calculating the gradient in the pixels, or how much change in values is observed around each pixel in both vertical and horizontal directions. Gradient operators are mathematical methods of calculating those gradients and are used to determine the strength of the gradient and its direction. The gradient for the example row of pixels is plotted below. Local maxima in the gradients can be used to define a edge threshold. ## Warning in `[&lt;-.factor`(`*tmp*`, 61:nrow(df), value = structure(c(2L, 2L, : ## invalid factor level, NA generated ## Warning: Removed 62 rows containing missing values (geom_path). ## Warning: Removed 62 rows containing missing values (geom_point). Figure 3.2: Gradient for smoothed pixel values There are a variety of edge detection methods used in the wild, among the most commonly used is the Canny Edge Detection algorithm34 The mathematics of edge detection can be fairly straight forward; However, there is a diversified toolkit of transformations that have been developed to improve the results. A Worked example To illustrate how edge detection works, we will use a high contrast photograph of Marine One flying over Washington D.C. Images that are “busy” (have many overlapping objects) may prove to be more challenging to identify clear cut edges. To start, we use the digIt() library to retrieve the image and render it using image(), specifying asp = 1 for an aspect ratio of 1:1. The helicopter is easy to identify with some propeller detail despite being a low resolution image with 134 x 157 pixels and 3 channels (RGB). library(digIt) require(raster) img &lt;- digIt(&quot;image_set_marine1&quot;) image(img, asp = 1, main = &quot;Marine One&quot;) Figure 3.3: Raw image of Marine One dim(img) ## [1] 134 157 3 To make the data manipulation simpler, the three color channels can be weighted to derive greyscale values.35 #convert to greyscale img1 &lt;- img[[1]]*0.2126 + img[[2]]*0.7152 + img[[3]]*0.0722 #render image image(img1, asp = 1, col =paste(&quot;gray&quot;,1:99, sep=&quot;&quot;)) Figure 3.4: Converted to greyscale Notice that the above image contains noise and pixelation, which may disrupt the detection of true edges in the photograph. A blur filter can be applied using any number of techniques. The Gaussian filter is a method that applies a kernel to an \\(n \\times n\\) areas around a given pixel, placing the greatest weight on the central pixel and less weight on pixels that are farther from the center. For the Canny approach, a \\(5 \\times 5\\) matrix – also known as a filter mask – is used as the basis to calculate a weighted average pixel value around each pixel: \\[\\begin{bmatrix} 2 &amp; 4 &amp; 5 &amp; 4 &amp; 2 \\\\ 4&amp; 9&amp; 12&amp; 9&amp; 4\\\\ 5&amp; 12&amp; 15&amp; 12&amp; 5\\\\ 4&amp; 9&amp; 12&amp; 9&amp; 4 \\\\ 2&amp; 4&amp; 5&amp; 4&amp; 2 \\end{bmatrix}\\] For a photograph of \\(100 \\times 100\\) pixels, the filter mask is applied to each of the 1000 pixels in order to estimate its blurred (smoothed) value. #(1) Blur Image --------------------------------- gaussianBlur &lt;- function(mat){ # Blur an image using a Gaussian kernel # # Args: # mat = image in matrix form # # Returns: # Blurred image # filter5 &lt;- matrix(c(2,4,5,4,2, 4,9,12,9,4, 5,12,15,12,5, 4,9,12,9,4, 2,4,5,4,2), ncol = 5, nrow = 5, byrow = TRUE) tot &lt;- sum(filter5) #placeholder matrix new &lt;- matrix(NA, ncol = ncol(mat), nrow = nrow(mat)) #loop through each cell by row and column for(i in 2:(ncol(mat)-2)){ for(j in 2:(nrow(mat)-2)){ new[j,i] &lt;- sum((mat[(j-2):(j+2), (i-2):(i+2)] * filter5)) / tot } } #resulting image is upside down and rotated, apply transformations to rectify new &lt;- apply(new, 2, rev) new &lt;- t(new) #return result return(new) } #Test blur function on Marine One new &lt;- gaussianBlur(img1) image(new, asp = 1, col = paste(&quot;grey&quot;, 1:99)) Figure 3.5: Gaussian blur applied to image to average out noise. Next, edges can be detected using a Sobel operator. The operator makes use of two masks – one for rows and one for columns: \\[\\text{kernel}_x=\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2&amp; 0&amp; 2\\\\ -1&amp; 0&amp; 1 \\end{bmatrix}, \\text{ kernel}_y=\\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0&amp; 0&amp; 0\\\\ -1&amp; 2&amp; -1 \\end{bmatrix}\\] For each pixel in the blurred image, each \\(\\text{kernel}_x\\) and \\(\\text{kernel}_y\\) are applied to produce the point estimate of the gradient \\(G\\) as the first derivative for each the x and y axes. The resulting values can be combined to estimate the gradient strength \\(G =\\sqrt{G_x^2+G_y^2}\\) and edge direction \\(\\theta = atan2(\\frac{|G_y|}{|G_x|}) \\frac{180}{\\pi}\\). Below, the Sobel operator is programmed as as a function sobel() that accepts a blurred image in matrix form. #(2) Gradient magnitudes sobel &lt;- function(mat){ # Apply a Sobel operator to image matrix, returns strength and direction of edges # # Args: # mat = blurred image in matrix form # # Returns: # A list object with two matrices: one for gradient strength and one for edges # #filters xkernel &lt;- matrix(c(-1, 0, 1, -2, 0, 2, -1, 0, 1), ncol = 3, nrow = 3, byrow = TRUE) ykernel &lt;- matrix(c(1, 2, 1, 0, 0, 0, -1, -2, -1), ncol = 3, nrow = 3, byrow = TRUE) #Placeholders strength &lt;- matrix(NA, ncol = ncol(mat), nrow = nrow(mat)) direction &lt;- matrix(NA, ncol = ncol(mat), nrow = nrow(mat)) #Calculate 1st derivative gradients for(j in 2:(nrow(mat)-1)){ for(i in 2:(ncol(mat)-1)){ #Calculate gradient valx &lt;- sum(mat[(j-1):(j+1), (i-1):(i+1)] * xkernel) valy &lt;- sum(mat[(j-1):(j+1), (i-1):(i+1)] * ykernel) #Combine into strength and direction strength[j,i] &lt;- sqrt((valx)^2 + (valy)^2) direction[j,i] &lt;- atan(abs(valy)/abs(valx)) * 180 / pi } } return(list(strength = strength, direction = direction)) } The result of applying the Sobel operator is a thick outline around Marine One. gradients &lt;- sobel(new) image(gradients$strength, asp = 1, col = paste(&quot;gray&quot;,1:99, sep=&quot;&quot;)) https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf↩ http://www.nydailynews.com/news/election/de-blasio-names-de-blasio-article-1.1463591↩ https://beta.prx.org/stories/81520↩ https://www.archives.gov/research/census/soundex.html↩ Source needed↩ https://www2.census.gov/geo/pdfs/reference/GARM/Ch11GARM.pdf↩ https://www.washingtonpost.com/news/wonk/wp/2015/03/01/this-is-the-best-explanation-of-gerrymandering-you-will-ever-see/?utm_term=.f820bf35a620↩ http://history.house.gov/Institution/Election-Statistics/Election-Statistics/↩ DateTimeClasses. https://stat.ethz.ch/R-manual/R-devel/library/base/html/DateTimeClasses.html↩ Canny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8(6):679–698 (1986)↩ ref required↩ "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Visually Detecting Patterns 4.2 How does this work? 4.3 Univariate data analysis 4.4 Multivariate Data 4.5 DIY 4.6 Exercises", " Chapter 4 Exploratory Data Analysis 4.1 Visually Detecting Patterns Mobile technologies have lowered the bar to using lightweight sensors that measure the physical world and have opened new applications of data in daily life. From a smart phone’s accelerometer, it’s possible to track distinct patterns in one’s activity based on the fluctuations in acceleration (\\(\\frac{m}{s^{2}}\\)). In fact, many of these technologies have become commonly available, enabling physical fitness activity monitoring to characterizing transportation quality. Below is a set of exercise measurements from an smartphone accelerometer that lasted approximately 6.5 minutes and graphed at a frequency of 5 hertz (five readings per second). Can you visually identify distinct patterns? What makes those patterns distinct? Figure 4.1: Net acceleration collected from a mobile phone, sampled at a rate of 5 hertz (readings per second). Over the short time sample, the graphs indicate four distinct types of acceleration patterns. If we manually extract samples from these periods, we can quantify the patterns in terms of their central tendencies. Idle periods have near zero acceleration, walking periods have acceleration around 0.2 with tight dispersion, running periods hover around 0.6 +/- 0.2, and descending stairs vary widely. Given how these basic insights, we can experiment with various methods of feature engineering, or ways to distill and represent useful signal from the raw data. We can also begin to formulate hypotheses for how to model and represent the patterns and relationships in the data. This is the basis of exploratory data analysis or EDA – the first look at a dataset. Much of exploratory data analysis is focused on formulating hypotheses, assessing data structures, and understanding the quirks and tolerances of data in order to develop useful and insightful data-driven applications. This is achieved by using visual techniques (e.g. histograms, other graphs) to identify outliers and assess the distribution of variables as well as statistical measures help to understand central tendency among other properties of the data. A well-conducted EDA would yield critical insights into how the data can and should be processed, methods for feature extraction to represent usable signal in the data, and options for operationalizing a strategy for solving the problem at hand. We can break EDA into a series of high-level goals, each of which is associated with analytical tasks that help piece together a clearer picture of what is contained in the data. Each of these questions can be answered through a graphical or numerical technical approach. Goal Common Questions Assess the data types - Are the data categorical, numerical, factor, strings, other? - What manipulations will you need to perform to get the data into usable shape? Understand the empirical distributions - Does the data fall into a commonly recognized shape? - Is it unimodal, bimodal? - Is there any indication of time-dependence? Detect outliers, missingness and errors - Are there anomalous values? - Do records spike or occur during odd times? - How complete is the data? - Which variables need to be standardized and cleaned? Check the assumptions - How exactly is the data collected? - Does the data reflect what would be expected? Identify important variables -Which variables are correlated with one another? Formulate data-bounded hypotheses - Which variables are most correlated? This chapter will equip you with the skills to explore the data in an efficient and thoughtful manner. There are three considerations that underlie effective discovery: data structure, statistical measures, and graphical summaries. We begin with an overview of elements of EDA, then reinforce by walking through an example workflow. 4.2 How does this work? The structure of the data dictates the amount and type of processing required to make data usable and wieldy. Typically, data should be in matrix or tabular form – a basic requirement for data to be manipulated and analyzed. However, at times, the raw data may be in an unstructured format (e.g. raw text) or in different structured formats (e.g. satellite imagery, long form), requiring processing, reshaping and/or feature extraction so that the data is usable. 4.2.0.1 Where to start To start, we’ll create a simulated dataset, containing five variables: sex, age, weight, program, and status. num_recs &lt;- 1200 long = expand.grid(seq(1,num_recs,1), c(&quot;sex&quot;,&quot;age&quot;, &quot;weight&quot;, &quot;program&quot;, &quot;status&quot;)) colnames(long) &lt;- c(&quot;person_id&quot;, &quot;field&quot;) long$values &lt;- c( round(runif(num_recs)), round(rnorm(num_recs,40,10)), sample(c(rep(-9999, num_recs), round(rnorm(num_recs,120,20))),num_recs), rep(c(&quot;a&quot;,&quot;b&quot;),num_recs/2), rep(c(&quot;in&quot;,NA,&quot;out&quot;),num_recs/3)) long &lt;- long[order(long$person_id),] head(long, 10) ## person_id field values ## 1 1 sex 0 ## 1201 1 age 43 ## 2401 1 weight 101 ## 3601 1 program a ## 4801 1 status in ## 2 2 sex 1 ## 1202 2 age 42 ## 2402 2 weight 98 ## 3602 2 program b ## 4802 2 status &lt;NA&gt; Using the str() method (below), we can produce a structural summary of each variable in a dataset. There are a few common things to keep in mind: Is the data in the right shape?. The data is ‘stacked’ or in ‘long’ form, which means that each row contains a value (values) that corresponds to a person-variable combination (person_id and field). Notice that while there are numeric and character values, all variables are coded as characters. In order to analyze the data, each field should be represented in a separate column. This process is known as reshaping from long to wide form and is pre-requisite. Are there discrete variables that are coded as integers?. Numeric codes often are used in order to keep the data files smaller. For example, sex is coded as a binary integer, but represents two values: 0 = Female, 1 = Male. In some case, it may be helpful to recode using the text value for ease of interpretation Are there missing values?. Missing values are often coded as NA or a large negative number in cases where values should be non-zero positive such as -9999. This will require some cleaning to standardize values and at times imputation. Which variables should be numeric?. At times, numeric variables will be formatted as strings and factors. str(long) ## &#39;data.frame&#39;: 6000 obs. of 3 variables: ## $ person_id: num 1 1 1 1 1 2 2 2 2 2 ... ## $ field : Factor w/ 5 levels &quot;sex&quot;,&quot;age&quot;,&quot;weight&quot;,..: 1 2 3 4 5 1 2 3 4 5 ... ## $ values : chr &quot;0&quot; &quot;43&quot; &quot;101&quot; &quot;a&quot; ... ## - attr(*, &quot;out.attrs&quot;)=List of 2 ## ..$ dim : int 1200 5 ## ..$ dimnames:List of 2 ## .. ..$ Var1: chr &quot;Var1= 1&quot; &quot;Var1= 2&quot; &quot;Var1= 3&quot; &quot;Var1= 4&quot; ... ## .. ..$ Var2: chr &quot;Var2=sex&quot; &quot;Var2=age&quot; &quot;Var2=weight&quot; &quot;Var2=program&quot; ... 4.2.0.2 Example of structural fixes Data should be in wide form. To do so, we can rely on the reshape() method to create a column for each of the field variables. head(long) ## person_id field values ## 1 1 sex 0 ## 1201 1 age 43 ## 2401 1 weight 101 ## 3601 1 program a ## 4801 1 status in ## 2 2 sex 1 wide &lt;- reshape(long, idvar = c(&quot;person_id&quot;), timevar=&quot;field&quot;, direction=&quot;wide&quot;) head(wide) ## person_id values.sex values.age values.weight values.program ## 1 1 0 43 101 a ## 2 2 1 42 98 b ## 3 3 0 46 108 a ## 4 4 0 36 165 b ## 5 5 1 49 97 a ## 6 6 1 32 -9999 b ## values.status ## 1 in ## 2 &lt;NA&gt; ## 3 out ## 4 in ## 5 &lt;NA&gt; ## 6 out When using colnames(), we can see that the reshaped dataset contains “values.” as a prefix. For conciseness, we’ll remove the prefix using gsub() to replace the string pattern with “” or blank. #get column names colnames(wide) ## [1] &quot;person_id&quot; &quot;values.sex&quot; &quot;values.age&quot; &quot;values.weight&quot; ## [5] &quot;values.program&quot; &quot;values.status&quot; #rename by removing &quot;values.&quot; colnames(wide) &lt;- gsub(&quot;values.&quot;,&quot;&quot;,colnames(wide)) #get column names colnames(wide) ## [1] &quot;person_id&quot; &quot;sex&quot; &quot;age&quot; &quot;weight&quot; &quot;program&quot; &quot;status&quot; Lastly, variables should be recoded and reformatted into the appropriate format. These basic fixes help ensure the usability of the dataset. #re-code sex wide$sex[wide$sex == &quot;1&quot;] &lt;- &quot;male&quot; wide$sex[wide$sex == &quot;0&quot;] &lt;- &quot;female&quot; #Character to numeric wide$age &lt;- as.numeric(wide$age) wide$weight &lt;- as.numeric(wide$weight) #Recode -9999 to NA wide$weight[wide$weight == -9999] &lt;- NA 4.3 Univariate data analysis The tools and techniques used to analyze can be distinguished into univariate (concerning one data series at a time) or multivariate (concerning two or more data series at a time). To start, we will focus on univariate techniques, starting with statistics, then moving into graphical methods. 4.3.1 Statistics for continuous variables The shape and properties of a continous variables will vary greatly. These moments, or common attributes of data, should influence how an analyst will treat the data. Four continuous distributions are plotted below: The first graph looks similar to a box, indicating that there is an equal chance that a value can take on any value between 0 and 1. The second graph follows a bell curve ( also known as a ‘normal distribution’ or ‘Gaussian distribution’) with a central peak and symmetrical tails. The third graph peaks to the left with a longer tail to the right. The fourth graph peaks to the right with a longer tail to the left. While graphical techniques are useful, and are the topic of the next section in this chapter, sample statistics can concisely summarize the contents of data in a way that are insightful and comparable. Figure 4.2: Histograms with distributions of varying shapes How can we quantitatively tell these variables apart? Sample statistics are particularly helpful with characterizing central tendency, spread, skewness and kurtosis. 4.3.1.1 Central tendency Data with any amount of signal typically tends towards a central value or location. This means that values will peak or cluster around a central point, but need not be symmetrically distributed around that central point. There are two common measures that are used to characterize central tendency: the mean and the median. The mean, also known as the arithmetic mean or average, is simply the sum of all values in x divided by the sample size n. Since the calculation takes into account all values in x, means are sensitive to outliers and extreme values. Often times, the mean is the main statistic used to describe what is typical of a sample or population as it is considered to be the expected value. For example: Average age of a cohort entering graduate school Average speed of vehicles traveling down a highway segment Average number of days to close a complaint The median or P50 is also used as a method of indicate central tendency based on indexed position in a variable. It is computed as the 50th percentile value: sort all values from lowest to highest, then find the value at the \\((\\frac{n+1}{2})^{th}\\) position. Essentially, the median is denotes the position at which 50% of values are above and below. Medians are robust to outliers, meaning that the change in the magnitude of values above or below the 50th percentile point may change, but the median may stay the same. Measure Formula Definition When to use R Function Example Mean or Average \\[ \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\] A measure of central tendency formulated as the sum of all values. Also known as the ‘expected value’. The general de facto choice mean() mean(c(1,3,5,7,9)) = 5 Median Value at \\[(\\frac{n+1}{2})^{th} \\] position A measure of central tendency based on the ‘middle value’ or 50th percentile of a random variable. When data appears to be skewed or asymmetrical median() median(c(1,3,5,7,9)) = 5 Typically, it’s helpful to consider and compare both median and mean values as they provide context. Revisiting the four distributions, we can compare the means and medians. In graphs 1 and 2, the distributions are symmetrical, thus one would expect the mean to be approximately equal to the mean. In graph 3, the longer tail to the right pulls the mean above the median, whereas in graph 4, the left tail pulls the mean below the median. Figure 4.3: Histograms with distributions of varying shapes 4.3.1.2 Spread Just as important as central tendency is the spread or dispersion, or measures that gauge the variability of data relative to a central point. There are a number of measures that commonly are focused upon, such as variance and the interquartile range, and all help piece together a comprehensive understanding of a given data series. Perhaps the simplest measures are those that are based on positions of records. The minimum and maximum are the smallest and largest values of a continuous variable. The minimum and maximum are also known as the P0 and P100. The arithmetic difference between those values is the range. Similar to the range is the Interquartile Range or IQR, which is the arithmetic difference in values between the P75 and P25. Examining the IQR gives a sense of the density of the center mass of a distribution, or the middle 50%. Together, these basic measures can contextualize the shape and density of a variable, especially comparing between subpopulations. For example, the income range of group A is $100,000 and group B is $2,000,000, indicating that the top and bottom of group A are closely distributed whereas there are large outliers in group B. In addition, the variance of a data series contains key information about variability around the mean. Variance is defined as the average squared difference between each value of a series and its mean. The differences are squared in order to (1) preserve the information as simply adding the differences would net to zero, (2) emphasize differences from the mean. While the variance is a key part of statistics, it’s not particularly interpretable without some additional transformations. The standard deviation is the square-root of the variance, which has the same units as the original data and can be used to contextualize dispersion of data relative to the mean. In fact, standard deviations are used as a unit of analysis. Under a normal distribution using the mean as a point of reference, +/- 1.96 standard deviations should contain 95% of records and +/- 3 standard deviations should contain 99.7% of records. These benchmarks are commonly used to identify outliers. Measure Formula Definition R Function Example Minimum \\(\\operatorname{argmin}(x_i)\\) The smallest value of a random variable. min() min(c(1,3,5,7,9)) Maximum \\(\\operatorname{argmax}(x_i)\\) The largest value of a random variable. max() max(c(1,3,5,7,9)) Range \\(\\operatorname{argmax}(x_i) - \\operatorname{argmin}(x_i)\\) Difference of its largest and smallest data values range() range(c(1,3,5,7,9)) IQR P75 - P50 Difference of the 75th percentile and 25th percentile value IQR() IQR(c(1,3,5,7,9)) Variance \\(s^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i-\\mu)^2\\) A measure of dispersion around the mean. var() var(c(1,3,5,7,9)) Standard Deviation \\(s = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i-\\mu)^2}\\) The square root of the variance. sd() sd(c(1,3,5,7,9)) 4.3.1.3 Skewness + Kurtosis Measure Formula Definition R Function Example Skewness \\(\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\) Measure of symmetry library(e1071) kurtosis() example Kurosis \\(\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\) Measure of peakedness relative to Gaussian distribution library(e1071) skewness() example 4.3.2 Graphical Approaches While sample statistics help to determine expected values and overall trends of continuous variables, graphical and visual approaches help to identify unexpected values – where are there outliers and unusual quirks of the data. 4.3.2.0.1 Histograms and kernel density graphs The most commonly used graph to visualize univariate data is the histogram. Essentially, histograms will break a continuous variable into equal sized bins based on the range of values, then graph the number of observations in each bin. This practice of binning or discretizing add a second dimension to a continuous variable that is needed to visualize patterns. Typically the number of bins is determined automatically when using functions such as hist(). While normal distributions are the best known probability distribution, a pure normal distribution is fairly rare in the wild. In social data, it’s more common to see long tails as seen below. Thus, data is often times transformed using mathematical functions in order to reshape the distribution of values. For data with large peaks to the left and long tails, common transformations include the natural logarithm (log()), logarithm base 2 (log2()) or logarithm base 10 (log10()). In the graph below, we use the log10() transformation to stretch the data to the right so that it is more normally or symmetrically distributed. #Set up format par(mfrow=c(1,2)) #Basic histogram hist(df$WAGP, main = &quot;Wages&quot;) hist(log10(df$WAGP), main = &quot;log10(Wage)&quot;) Figure 4.4: Two histograms - wages and log(wages) The hist() function provides rudimentary stylings for histograms. For more advanced stylings, the ggplot2 library offers more convenient and easier to use stylistic options. In the example below, the log10 transformation is applied using scale_x_log10(). ggplot(df, aes(WAGP)) + ggtitle(&quot;Histogram&quot;) + geom_histogram(colour = &quot;white&quot;, fill = &quot;navy&quot;) + labs(x = &quot;log10(Wage)&quot;) + scale_x_log10() Figure 4.5: Histogram of Log(Wage) An alternative to histograms is the kernel density plot, which applies a technique known as kernel-density estimation as opposed to binning. Using ggplot2, we can apply the geom_density() argument to use the kernel density plot. The plot is notably smoother and organic when compared to histograms. #K-density ggplot(df, aes(WAGP)) + ggtitle(&quot;Kernel Density&quot;) + geom_density(colour = &quot;white&quot;, fill = &quot;navy&quot;) + labs(x = &quot;log10(Wage)&quot;) + scale_x_log10() Figure 4.6: Kernel Density Plot Both histograms and kernel density graphs are helpful for developing a notional understanding of the shape of data. Due to binning, the individual bins of a histograms correspond to an actual number of observations whereas kernel densities can be used to illustrate the organic shape of the distribution. However, neither provide quantitative benchmarks such as the mean or quantiles. 4.4 Multivariate Data Much of data analysis focuses on the possibilities of combining two or more data series in order to uncover relationships and patterns as well as inform modeling hypotheses. In this section, we will focus on using correlation statistics and bivariate graphs to identify patterns. 4.4.0.1 Correlation of continuous variables In everyday rhetoric, the term correlation is relatively loosely used. In statistics, it commonly has a specific definition, specifically, the Pearson Product-Moment Correlation Coefficient or Correlation Coefficient for short – a measure of relatedness. It is defined as: \\[\\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma{_X}\\sigma{_Y}} \\] where \\(\\rho\\) denotes the correlation coefficient, cov is the covariance of two variables (X and Y) as defined as \\((X - \\mu_X)(Y - \\mu_Y)\\), and \\(\\sigma\\) is the standard deviation of each X and Y. The correlation coefficient is bound between -1 and 1, where: -1 indicates perfectly negative linear relationship (as X increases, Y decreases proportionally) 0 indicates no relationship (X and Y are not related) +1 indicates perfectly positive linear relationship (as X increases, Y increases proportionally) Figure 4.7: Varying Degrees of Correlation In practice, it’s fairly simply to calculate and explore relationships by using cor(). For example, what if we were to compare age (AGEP) with retirement income (RETP). Notice that we use the complete.obs option to use only records that have values for both AGEP and RETP. The correlation coefficient is +0.226 indicating only a slight correlation. cor(df$AGEP, df$RETP, use = &quot;complete.obs&quot;) ## [1] 0.2262218 How do we examine a much broader set of variables such as WAGP - wage income, PERNP - personal income, RETP - retirement income, SSIP - supplemental security income, PAP - public assistance income, SEMP - self-employment income, and OIP - all other income. The cor() function can accept a dataframe or matrix of continuous variables and outputs a k x k matrix of pairwise correlation coefficients. The diagonal will always be filled with 1 as a correlation of a variable with itself will be the highest possible value. Values above the diagonal are identical to values below. select &lt;- df[, c(&quot;AGEP&quot;, &quot;WAGP&quot;, &quot;PERNP&quot;, &quot;SEMP&quot;)] cor(select, use = &quot;complete.obs&quot;) ## AGEP WAGP PERNP SEMP ## AGEP 1.00000000 -0.11366644 -0.09416775 0.02141612 ## WAGP -0.11366644 1.00000000 0.91511883 0.02645639 ## PERNP -0.09416775 0.91511883 1.00000000 0.42725385 ## SEMP 0.02141612 0.02645639 0.42725385 1.00000000 The correlation matrix may be a bit overwhelming to decipher. We may use the corrplot package to visualize the size of correlations. In this case, we can see that the strongest correlations are PERNP, WAGP, and SEMP. library(corrplot) M &lt;- cor(select, use = &quot;complete.obs&quot;) corrplot(M, method = &quot;ellipse&quot;) Figure 4.8: Correlation plot 4.4.0.2 Bivariate plots: continuous vs continuous The simplest bivariate graph is the scatter plot, which graphs points placing one variable on the horizontal axis and another on the vertical axis. The intersection of the values fall on a two-dimensional canvas, exposing relationships between variables. Below, a simple scatter plot shows the non-linear relationship between log-transformed personal income and age, suggesting that wages may reach a ceiling as one gets older. plot(df$AGEP, log10(df$PERNP), cex = 0.5) Figure 4.9: Scatter plot of age and log(personal income) In data science, often times, the amount of the data will push the limits of traditional graphs and require more stylized ways to derive empirical insight: Graph (a) plots a traditional scatter plot with much of the plot saturated with points. This is generally good when strong linear trends are present or with relatively few data points. Graph (b) is another scatter plot with an adjusted alpha, also known as opacity. In graphs, changing the opacity in a graph without any overlapping points makes the entire image look faint. In cases where points overlap, the opacity values add up, thereby darkening areas with overlapping points. This style is particularly useful when there is a high volume of points. Graph (c) is a scatter plot, but uses a locally weighted smoother to show the direction of local relationships and the variability around the trend line. This style can be used to find candidate transformations and non-linear specifications for the modeling phase. Graph (d) is a hexbin plot, which converts a graphical canvas into equal hexagonal bins, counts the number of values in each discrete bin and color codes based on the counts. This style is particularly useful when there is a high volume of points. Graph (e) is a contour graph that bins the graph into distinct regions where concentric regions towards the middle are larger values. This is useful for geographic or spatial data, particulary for terrain data. #Base data for plot p = ggplot(df,aes(x=AGEP,y=log10(PERNP))) + xlab(&quot;Age&quot;) + ylab(&quot;log(earnings)&quot;) ##Scatter plot p1 = p + geom_point(size = 0.2) + ggtitle(&quot;(a) traditional scatter&quot;) ##Scatter plot with transparency p2 = p + geom_point(alpha = 0.1, colour=&quot;navy&quot;, size = 0.2) + theme_bw() + ggtitle(&quot;(b) scatter (alpha = 0.1)&quot;) #Scatter plot with regression line (locally weighted smoother) p3 = p + ggtitle(&quot;(c) scatter + regression line&quot;)+ geom_point(alpha = 0.1, colour=&quot;navy&quot;,size = 1) + geom_smooth() + xlab(&quot;Age&quot;) + ylab(&quot;log(earnings)&quot;) ##Hexbin p4 = p + stat_bin_hex(colour = &quot;white&quot;, na.rm = TRUE, alpha = 0.9) + scale_fill_gradientn(colours=c(&quot;lightgrey&quot;,&quot;navy&quot;), name = &quot;Frequency&quot;) + guides(fill=FALSE) + ggtitle(&quot;(d) hex-bin plot&quot;) #Contour graph p5 = p + ggtitle(&quot;(e) contour&quot;)+ geom_density2d() + theme_bw() #Arrange graphs grid.arrange(p1, p2, p3, p4, p5, ncol = 3) Figure 4.10: Comparison of Five Bivariate Plots 4.4.0.2.1 Comparing subpopulations A common task in EDA is the search for separability – when data cluster in such a way that can help distinguish one group from another. This helps with identifying qualities that distinguish one subpopulation from another and factors that may be included in a modeling strategy. Side-by-side graphical comparisons, such as violin and boxplots, and graph overlays are powerful tools in showing differences. To illustrate this, let’s create a variable for whether a respondent holds a bachelor’s degree. These two levels can be used to stratify a dataset into subpopulations for comparison. df$college &lt;- NA df$college[df$SCHL &lt; 21] &lt;- &quot;Less than bachelor&#39;s&quot; df$college[df$SCHL &gt;= 21] &lt;- &quot;With bachelor&#39;s&quot; The natural shape of kernel density plots lend themselves to easier comparison. As seen below, we two kernel density plots are overlaid. Using the fill and colour parameters, the kernel densities are color-coded for ease of inspection. Notice that distributions are differently distributed, with college graduates generally earning higher wages. ggplot(df, aes(WAGP, fill = factor(college), colour = factor(college))) + geom_density(alpha = 0.1) + scale_x_log10() + labs(x = &quot;log10(Wage)&quot;) + ggtitle(&quot;Kernel Overlay&quot;) + theme(plot.title = element_text(size = 10, hjust = 0.5)) Figure 4.11: Kernel Densities by Education Attainment What if there are many subpopulations? If there are more than a couple of sub-groups, other side-by-side comparisons lend themselves for easier interpretation and reduce clutter. df$SCHL2[df$SCHL &lt; 16 ] &lt;- &quot;1 - Less than HS&quot; df$SCHL2[df$SCHL &gt;= 16 &amp; df$SCHL &lt; 21] &lt;- &quot;2 - HS&quot; df$SCHL2[df$SCHL == 21] &lt;- &quot;3 - Undergrad. Degree&quot; df$SCHL2[df$SCHL &gt; 21] &lt;- &quot;4 - Grad. Degree&quot; df$SCHL2[is.na(df$SCHL)] &lt;- &quot;5 - N/A&quot; Two data scientist favorites are the violin plot and the boxplot. The violin plot (left) is a kernel density graph rotated onto a vertical axis with the probability density plotted symmetrically across the vertical axis. Boxplots display the distribution of the data following key summary statistics, namely the median, 25th percentile/first quartile and the 75th percentile/third quartile. On the right, the median is denoted by the gray horizontal line in each blue box, and the upper and lower edges of the box are the 75th and 25th percentiles, respectively, also referred to as the shoulders of the distribution. Points to the top and the bottom are outliers, which are points located outside the body of the distribution (usually a distance of over 1.5-times the IQR above 75th percentile and below the 25th percentile). Both graphs are quite useful. The violin plot shows the shape of the distribution, whereas the boxplot allows for comparison of center mass. #Violin vio &lt;- ggplot(df, aes(factor(SCHL2), WAGP)) + geom_violin(colour = &quot;navy&quot;, fill = &quot;navy&quot;) + labs(x = &quot;log10(Wage)&quot;) + ggtitle(&quot;Violin Plot&quot;) + scale_y_log10() + theme(plot.title = element_text(size = 10, hjust = 0.5)) #Boxplot box &lt;- ggplot(df, aes(factor(SCHL2), WAGP)) + geom_boxplot(colour = &quot;grey&quot;, fill = &quot;navy&quot;) + labs(x = &quot;log10(Wage)&quot;) + ggtitle(&quot;Box Plot&quot;) + scale_y_log10() + theme(plot.title = element_text(size = 10, hjust = 0.5)) #Plot side by side grid.arrange(vio, box, ncol = 1) Figure 4.12: Comparison of Violin Plot and Box Plot for log(Wage) 4.5 DIY 4.5.1 What’s a common exploratory data analysis workflow? To put EDA into context, we will rely upon the American Community Survey (ACS), which is one of the most relied upon public data sources in the United States. A survey that is produced by the U.S. Census Bureau, the ACS provides a highly detailed socioeconomic snapshot of households and communities, allowing for data-driven insight to inform public policy as well as business decisions. The data dictionary containing variable definitions and descriptions can be found here. Note: While each record is associated with a sampling weight, meaning that one record represents more than one record. For simplicity, we will treat each record with equal weight. We will also only focus on one state in this exercise – in this case, we have selected Iowa. The same analysis can be easily replicated for other states if not the entire United States. Get data To start, we will need to retrieve data from the Census website: https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_pia.zip. First, we create a temporary file using tempfile(), which will be used to hold a the zipfile. Then, we will need to use the download.file() method, specifying the mode = &quot;wb&quot; to ensure the file downloads the binary values – a simple trick to get around security problems associated with “https”. We download the zipfile to the temp file location. #Set link url &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_pia.zip&quot; #Create a temporary directory temp &lt;- tempfile() #Download file from url, save to temporary directory download.file(url, temp, mode = &quot;wb&quot;) #Unzip file and read in as csv unz &lt;- unzip(temp, exdir = getwd()) df &lt;- read.csv(unz[1]) Now that the data has been downloaded, we will need to unzip the file using unzip(), and place it in our current working drive getwd(). As the dataset is a .csv, we can use the read.csv() method to import the data. unz &lt;- unzip(temp, exdir=getwd()) acs &lt;- read.csv(unz[1]) Examine data structure Now, let’s take a look at the structure of the first 5 variables using the `str() method. “PUMA” and “ST” are codes for Public Use Microdata Areas and States, both of which are geographic units, yet the data represents them as integers. These variables as well as CIT (citizenship), SEX (sex) and a few others are also coded as integers although they represent factors. We will need to clean these up before using them for visual analysis. str(acs[,1:5]) ## &#39;data.frame&#39;: 31900 obs. of 5 variables: ## $ RT : Factor w/ 1 level &quot;P&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ SERIALNO: int 141 215 215 215 227 262 312 312 327 327 ... ## $ SPORDER : int 1 1 2 3 1 1 1 2 1 2 ... ## $ PUMA : int 1800 1500 1500 1500 200 1400 100 100 2100 2100 ... ## $ ST : int 19 19 19 19 19 19 19 19 19 19 ... Cut down data To make the analysis a bit more manageable, we will extract 14 demographic variables, limit the analysis to ages 16 and above. var_list &lt;- c(&quot;HICOV&quot;,&quot;RAC1P&quot;,&quot;MAR&quot;,&quot;SEX&quot;,&quot;ESR&quot;,&quot;CIT&quot;,&quot;AGEP&quot;,&quot;PINCP&quot;,&quot;POVPIP&quot;,&quot;WKHP&quot;,&quot;SCHL&quot;) df &lt;- acs[acs$AGEP &gt;= 16, var_list] #Healthcare coverage (target variable): #Create one binary variable for calculations df$coverage[df$HICOV == 1] &lt;- 0 df$coverage[df$HICOV == 2] &lt;- 1 #another with characters df$hicov2[df$HICOV == 1] &lt;- &quot;With Healthcare&quot; df$hicov2[df$HICOV == 2] &lt;- &quot;Without Healthcare&quot; #Gender df$sex2[df$SEX == 1] &lt;- &quot;Male&quot; df$sex2[df$SEX == 2] &lt;- &quot;Female&quot; #Race df$race2[df$RAC1P == 1] &lt;- &quot;White alone&quot; df$race2[df$RAC1P == 2] &lt;- &quot;Black or African Amer. alone&quot; df$race2[df$RAC1P == 3] &lt;- &quot;Amer. Indian alone&quot; df$race2[df$RAC1P == 4] &lt;- &quot;Alaska Native alone&quot; df$race2[df$RAC1P == 5] &lt;- &quot;Amer. Indian + Alaska Nat. tribes&quot; df$race2[df$RAC1P == 6] &lt;- &quot;Asian alone&quot; df$race2[df$RAC1P == 7] &lt;- &quot;Nat. Hawaiian + Other Pac. Isl.&quot; df$race2[df$RAC1P == 8] &lt;- &quot;Some other race alone&quot; df$race2[df$RAC1P == 9] &lt;- &quot;Two or more&quot; #Marital Status df$mar2[df$MAR == 1] &lt;- &quot;Married&quot; df$mar2[df$MAR == 2] &lt;- &quot;Widowed&quot; df$mar2[df$MAR == 3] &lt;- &quot;Divorced&quot; df$mar2[df$MAR == 4] &lt;- &quot;Separated&quot; df$mar2[df$MAR == 5] &lt;- &quot;Never Married&quot; #Employment Status df$esr2[df$ESR %in% c(1, 2, 4, 5)] &lt;- &quot;Employed&quot; df$esr2[df$ESR == 3] &lt;- &quot;Unemployed&quot; df$esr2[df$ESR == 6] &lt;- &quot;Not in labor force&quot; #Citizenship df$cit2[df$CIT %in% c(1, 2, 3, 4)] &lt;- &quot;Citizen&quot; df$cit2[df$CIT == 5] &lt;- &quot;Not citizen&quot; #School df$schl2[df$SCHL&lt;16 ] &lt;- &quot;Less than HS&quot; df$schl2[df$SCHL&gt;=16 &amp; df$SCHL&lt;21] &lt;- &quot;HS Degree&quot; df$schl2[df$SCHL==21] &lt;- &quot;Undergrad. Degree&quot; df$schl2[df$SCHL&gt;21] &lt;- &quot;Grad. Degree&quot; Making sense of discrete variables With the data in place, we now can run a few basic cross-tabulations. We’ll compare education attainment against healthcare coverage using the table() function to produce a tabulation by combination of levels. Then, use that table to feed into the prop.table() method, specifying \\(1\\) to indicate that we would like to see numbers presented as proportions of each row. We then will use the chisq.test() method to determine if people who have health care have statistically different levels of educational attainment. In the proportions table, we can see that 9.2% of people with less than a HS degree are without healthcare, which is notably more than the others. ##Comparison of education attainment vs. healthcare coverage tab &lt;- table(df$schl2, df$hicov2) #Get proportions by row prop.table(tab, 1) Table 4.1: Percent with healthcare by education attainment With Healthcare Without Healthcare Grad. Degree 0.988 0.012 HS Degree 0.954 0.046 Less than HS 0.908 0.092 Undergrad. Degree 0.978 0.022 We also can see from the chi-squared test, used to determine if there is a significant difference between observed and expected frequencies in one or more categories. In this case, we’re comparing if the distribution of education attainment are proportional among those who have and do not have healthcare coverage. Based on the test statistic, we see that the p-value is less than 0.01, allowing us to conclude that education attainment is different among those who have and do not have healthcare coverage. chisq.test(tab) ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 229.61, df = 3, p-value &lt; 2.2e-16 Using this basic process, we can easily loop through all the categorical variables in our abridged dataset. #Set the variables master &lt;- data.frame(var = c(&quot;esr2&quot;, &quot;mar2&quot;, &quot;race2&quot;, &quot;sex2&quot;, &quot;schl2&quot;, &quot;cit2&quot;), descrip = c(&quot;Employment&quot;, &quot;Marital Status&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;Education&quot;, &quot;Citizenship&quot;)) master[,1] &lt;- as.character(master[,1]) master[,2] &lt;- as.character(master[,2]) #Loop through each variable and print result for(i in 1:nrow(master)){ print(master[i, 2]) tab &lt;- table(df[, master[i, 1]], df$hicov2) print(prop.table(tab, 1)) print(chisq.test(tab)) } ## [1] &quot;Employment&quot; ## ## With Healthcare Without Healthcare ## Employed 0.95632330 0.04367670 ## Not in labor force 0.95899153 0.04100847 ## Unemployed 0.83806344 0.16193656 ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 192.94, df = 2, p-value &lt; 2.2e-16 ## ## [1] &quot;Marital Status&quot; ## ## With Healthcare Without Healthcare ## Divorced 0.92605887 0.07394113 ## Married 0.97285345 0.02714655 ## Never Married 0.91821862 0.08178138 ## Separated 0.82375479 0.17624521 ## Widowed 0.98907104 0.01092896 ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 511.98, df = 4, p-value &lt; 2.2e-16 ## ## [1] &quot;Race&quot; ## ## With Healthcare Without Healthcare ## Amer. Indian + Alaska Nat. tribes 0.82352941 0.17647059 ## Amer. Indian alone 0.75308642 0.24691358 ## Asian alone 0.90291262 0.09708738 ## Black or African Amer. alone 0.88560158 0.11439842 ## Nat. Hawaiian + Other Pac. Isl. 0.66666667 0.33333333 ## Some other race alone 0.79310345 0.20689655 ## Two or more 0.92549020 0.07450980 ## White alone 0.95898771 0.04101229 ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 300.79, df = 7, p-value &lt; 2.2e-16 ## ## [1] &quot;Sex&quot; ## ## With Healthcare Without Healthcare ## Female 0.96413470 0.03586530 ## Male 0.94459768 0.05540232 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 56.594, df = 1, p-value = 5.357e-14 ## ## [1] &quot;Education&quot; ## ## With Healthcare Without Healthcare ## Grad. Degree 0.98825602 0.01174398 ## HS Degree 0.95367693 0.04632307 ## Less than HS 0.90847578 0.09152422 ## Undergrad. Degree 0.97780713 0.02219287 ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 229.61, df = 3, p-value &lt; 2.2e-16 ## ## [1] &quot;Citizenship&quot; ## ## With Healthcare Without Healthcare ## Citizen 0.95791252 0.04208748 ## Not citizen 0.77983539 0.22016461 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 344.67, df = 1, p-value &lt; 2.2e-16 Making sense of continuous variables Continuous variables offer a lot more opportunity to visualize patterns. Typically, two general types of graphs are common: univariate distribution graphs and bivariate graphs. Univariate graphs Two univariate graphs are typically used: (1) histograms, and (2 kernel density diagrams. Histograms are a way to depict the distribution of continuous variables. For values between the minimum and maximum value of a variable, the data is partitioned into equal intervals or “bins”. For each equal interval, a count is taken of the number of records in each bin. Using the AGEP variable (age), we can plot a histogram using ggplot2. In ggplot2, we use the ggplot() method to specify the data frame and variables that will be used to create graphs, then add various geom arguments to specify the type of graph. For a histogram, we specify the dataset df and the \\(x\\) variable aes(x = AGEP), then tag on +geom_histogram(). library(ggplot2) ggplot(df, aes(x = AGEP)) + geom_histogram() Figure 4.13: Histogram of Age AGEP can be replaced with PINCP(personal income). Notice that the distribution occupies only a small part of the graph. ggplot(df, aes(x = PINCP)) + geom_histogram() Figure 4.14: Histogram of Personal Income This can be adjusted by adding the + scale_x_log10() argument in order to transform the \\(x\\) variable using a \\(log_{10}\\) transformation. Within the geom_histogram() arguement, we change the color of the histogram to “navy”, add a title using ggtitle() and adjust the size of the label text using theme(). ggplot(df, aes(x = PINCP)) + geom_histogram(fill = &quot;navy&quot;) + scale_x_log10() + theme(plot.title = element_text(size = 10, hjust = 0.5)) Figure 4.15: Histogram of log(Personal Income) Another type of univariate graph is the kernel density graph. Whereas the histogram is boxy, the kernel density graph applies a rolling window approach to calculate the count of values at small but equally spaced intervals, weighing the count using a bandwidth – a threshold that determines how wide or narrow the rolling window should be. ggplot provides facility to use a kernel density graph using the + geom_density(). Kernel density diagrams provide a more organic representation of a distribution and can better illustrate small fluctuations at different magnitudes of the variable in question. This is useful for finding more discrete natural breaks in the data that can be used for classification. Furthermore, kernel densities are generally easier to compare when overlaying two more or more distributions. ggplot(df, aes(x = PINCP)) + geom_density(fill = &quot;navy&quot;) + scale_x_log10() + theme(plot.title = element_text(size = 10, hjust = 0.5)) Figure 4.16: Histogram of log(Personal Income) To compare two or more groups, simply set the colours and fill arguments to a categorical variable. For example, we can compare the ages of Iowans by employment status. As may be expected, most Iowans who are not in the labor force (not seeking employment) are past retirement age while there is a slight bump in unemployment among younger members of society. ggplot(df, aes(x = AGEP, colours = esr2, fill = esr2)) + geom_density(alpha = 0.2) + ggtitle(&quot;Employment Status by Age&quot;) + theme(plot.title = element_text(size = 10, hjust = 0.5)) Figure 4.17: Employment status and age Likewise, comparing healthcare coverage by age, younger Iowans are a greater proportion of people without healthcare. ggplot(df, aes(x = AGEP, colours = hicov2, fill = hicov2)) + geom_density(alpha = 0.2) + ggtitle(&quot;Healthcare coverage and Age&quot;) + theme(plot.title = element_text(size = 10, hjust = 0.5)) Figure 4.18: Healthcare coverage and age Bivariate graphs Scatter plots are among the most standard bivariate graphs and can be easily called by adding geom_point() to the base ggplot() method. They also can be easily stylized. Below, we plot x = AGEP and y = PINCP. The result is a mess of points without a clear trend. ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point() Figure 4.19: Scatter plot: log(personal income) and age. To move the mass of points above the x-axis, we can apply a \\(log_{10}\\) transformation to the PINCP variable. In addition, changing the alpha to a value less than 1.0 will allow us to take advantage of additive transparency – that is allowing the transparencies of multiple overlapping points to darken areas of creater density. We now can see that income generally rises quickly before the age of 25, then plateaus. ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point(alpha = 0.01) + scale_y_log10() Figure 4.20: Scatter plot with style. log(personal income) and age. In addition, we can apply a kernel density smoothed line to show the moving average trend of income as a function of age. ggplot(df, aes(x = AGEP, y = PINCP)) + geom_point(alpha = 0.01) + scale_y_log10() + geom_smooth() Figure 4.21: Scatter plot with smoothed line: log(personal income) and age. Using the same principles, we can find the bivariate relationship between age and health coverage. As health coverage (coverage) is coded as a binary, the smoothed estimate is equivalent to a locally estimated probability of no healthcare coverage. First up is coverage versus age. Lack of coverage spikes around AGEP = 25 at a value of about 9%, then drops for older ages. Notice that the grey region around the smoothed mean estimate hugs the line fairly closely, indicating that the estimates are fairly certain with low variability. ggplot(df, aes(x = AGEP, y = coverage)) + geom_smooth() + labs(x = &quot;Age (years)&quot;, y = &quot;Pct w/o Coverage (1.0 = 100%)&quot;) Figure 4.22: Percent without healthcare coverage by age. A comparison of coverage and income yields challenging results to interpret due to the notably large uncertainties at the very low and very high incomes, largely due to lower densities of records. ggplot(df, aes(x = PINCP, y = coverage)) + geom_smooth() + labs(x = &quot;log(Personal Income)&quot;, y = &quot;Pct w/o Coverage (1.0 = 100%)&quot;) + scale_x_log10() Figure 4.23: Percent without healthcare coverage by log(personal income). Hours worked per week sheds some interesting insights on coverage patterns, particularly around the 20 to 32 hours a week range as well as beyond 50 hours a week. This may be due to the nature of part-time work or multiple jobs. ggplot(df, aes(x = WKHP, y = coverage)) + geom_smooth() + labs(x = &quot;Hours Worked Per Week&quot;, y = &quot;% w/o Coverage (1.0 = 100%)&quot;) Figure 4.24: Percent without healthcare coverage by hours worked per week. Lastly, there is less coverage among those who are closer to and below the coverage line. ggplot(df, aes(x = POVPIP, y = coverage)) + geom_smooth() + labs(x = &quot;Poverty Level (100 = at level)&quot;, y = &quot;% w/o Coverage (1.0 = 100%)&quot;) Figure 4.25: Percent without healthcare coverage by poverty level. 4.6 Exercises Train your eyes to identify strong correlations. Play the Guess The Correlation Game for 5 minutes. This game challenges players to guess the coefficient correlation based on randomly assigned scatter plots of two continuous variables. "],
["similarity.html", "Chapter 5 Similarity 5.1 Distances 5.2 Correlation 5.3 Linguistic Distances 5.4 Entropy 5.5 DIY", " Chapter 5 Similarity We all have heard people react with the following remarks: “That idea sounds awfully like…”; “Did you mean to say…”; “That looks like…”; “That’s pretty close to…”; “That seems to trend with…”; “You might actually like this as well…”; Humans need to contextualize the world around us. We are creatures of experiential learning, equipped with a powerful ability of cognition to quickly relate one thing to another in abstract terms. While we are endowed with strong cognitive abilities, our abilities are not scalable. For the most part, humans take things one at a time. Even multi-taskers are not truly multi-taskers as each task is serially initialized. What if we wanted to scale our abilities so that it can be applied more widely. This is very much part of the data science mantra. So, how would one quantify the underlying logic behind the above reactions? “That idea sounds awfully like…” is a matter of intersection of two ideas, which in part can be quantified as the overlap of words in two descriptions. “Did you mean to say…” may be simply due to associated with a spelling difference. “That looks like…” relates physical and latent qualities of two objects, whether color, shape or size. “That’s pretty close to…” is an abstraction of distance; “That seems to trend with…” is a matter of co-occurrence over time “You might actually like this as well…” matches two entities to one another. If basic everyday comparisons can be parameterized, then a bold new set of questions can be asked to support policy and strategy such as “Find all documents that relate to X” or “Which other services might a user of Y need?”. In short, these everyday responses can be operationalized in simple measures of similarity – some are distance related, some are volumetrically related, and others are linguistically related. Together, they provide a versatile set of methods to guide scalable tasks. Each type of similarity plays a part in algorithms, from simple variance estimators in regression models to split criteria in non-linear tree methods. In this section, we introduce a set of common measures that will appear and re-appear throughout the rest of the book, and we will tie each method to a real world use. 5.1 Distances If a resident of a neighborhood searches the internet to find the nearest police station from a list of a dozen precincts, what happens in the background? The search site will ask for the user’s address, then convert that into latitude and longitude coordinates (\\(x_0, y_0\\)). The search site also contains a list of known police stations and has already converted the coordinates into a table of coordinates such that police station 1 is (\\(x_1, y_1\\)), police station 2 is (\\(x_2, y_2\\)), Based on this information, a simple dissimilarity matrix may be calculated – basically a \\(n \\times n\\) matrix of distances among items in the list of police stations and the user coordinate: \\[\\begin{bmatrix} 0 \\\\ 2 &amp;&amp; 0 \\\\ 1 &amp;&amp; 10 &amp;&amp; 0 \\\\ 4 &amp;&amp; 3 &amp;&amp; 23 &amp;&amp; 0 \\end{bmatrix}\\] where the diagonal represents distance from a coordinate to itself. If the first column is the user’s coordinate, then the closest distance is in the 3rd row of the matrix. This is known as a nearest neighbor search – within a metric space, which entities are the closest (covered in supervised learning). Distance implies a measure in space, but distance can be represented in a number of ways. Perhaps the most generalizable form of distance is the Minkowski Distance, which is given in math-fabulous as follows: \\[ d(x_1,x_2) = \\sqrt[p]{|a_1-a_2|^p +|b_1-b_2|^p + ... +|z_1-z_2|^p}\\] where \\(x_1\\) and \\(x_2\\) are n-dimensional vectors (a set of coordinates). In layman terms, the \\(x_1\\) and \\(x_2\\) are each lists of equal length containing coordinates that identify locations \\(1\\) and \\(2\\). \\(x_1\\) for instance can be represented as \\(x_1 = \\begin{bmatrix} a_1 \\\\ b_1 \\\\ \\vdots \\\\z_1 \\end{bmatrix}\\). For each corresponding dimension of \\(x_1\\) and \\(x_2\\), we take the absolute distance to the \\(p\\) power, sum up the absolute dimensions, then take the \\(p^{th}\\) root. The disimilarity matrix described before is comprised of the above equation, repeated for each pair of coordinates. One thing to note is that calculating \\(d(x_1, x_2)\\) and \\(d(x_2, x_1)\\) is the same, thus when computing the matrix, only slightly more than half of the matrix (above the diagonal or below the diagonal) contains unique information. It does not take much to see that Euclidean distance – “straight-line distance”, “ordinary distance”, \\(L_2\\) distance – is a case where \\(p = 2\\) and is the most likely candidate for the geographic nearest neighbor search: \\[ d(x_1,x_2) = \\sqrt[2]{|a_1-a_2|^2 +|b_1-b_2|^2 + ... +|z_1-z_2|^2}\\] where subscripts \\(x\\), \\(y\\), and \\(z\\) represent dimensions (continuous variables). Euclidean distances make the most sense when the coordinates are given in similar units. If the same resident from the above example wanted to find the closest police station that is located within half a mile of a train station, then adding an additional dummy variable as a dimension would not be appropriate as binary and coordinates are not in the same scale. When \\(p = 1\\), the Minkowski Distance gives rise to the \\(L-1\\)-norm, “Manhattan distance” or “taxicab distance”, which resembles distance along the edge of the blocks of a grid. \\[ d(x_1,x_2) = |a_1-a_2| +|b_1-b_2|+ ... +|z_1-z_2|\\] The Hamming Distance is a special case of the \\(L-1\\)-norm in which the coordinates are binary, such as binary variables that indicate level of education, demographics, prior service use, etc. In cases where binary is the only available information, Hamming distance offer a convenient strategy to similarity in terms of “bits”. The big picture with distance is to measure similarity or its inverse dissimilarity among records, which can then form the basis of: finding clusters of similar records (clustering and unsupervised learning) defining bandwidths – or the number of records to be included in a calculation selecting variables to be included in a model (regularization) Exercise Write a function that calculates the Minkowski Distance. The input parameters should accomodate two vectors \\(x\\) and \\(y\\) along with \\(p\\) for the \\(p^{th}\\)-root. minkowski &lt;- function(x, y, p){ # Returns Minkowski distance for the pth power # # Args: # x, y = numeric vectors # p = power # # returns: # l-p distance # return(sum(abs(x - y)^p) ^ (1/p)) } #Try a Euclidean distance (two continuous vectors) a1 &lt;- c(1, 3, 5) b1 &lt;- c(2, 20, 10) minkowski(a1, b1, 2) ## [1] 17.74824 #Try a Hamming distance (two binary vectors) a1 &lt;- c(0, 1, 1, 0) b1 &lt;- c(0, 1, 0, 1) minkowski(a1, b1, 1) ## [1] 2 5.2 Correlation Research on the effect of temperature on crime is fairly common. For instance, the findings from a Field (1992) describes this relationship in the United Kingdom36: An analysis of annual, quarterly, and monthly data for recorded crime in England and Wales yielded strong evidence that temperature has a positive effect on most types of property and violent crime. The effect was independent of seasonal variation. No relationship between crime and rainfall or hours of sunshine emerged in the study. The main explanation advanced is that in England and Wales higher temperatures cause people to spend more time outside the home. Time spent outside the home, in line with routine activity explanations for crime, has been shown to increase the risk of criminal victimization for most types of crime. The results suggest that temperature is one of the main factors to be taken into account when explaining quarter-to-quarter and month-to-month variations in recorded crime. The results are correlative when temperature increases, crime also increases. As has been covered in Exploratory Data Analysis, correlation is commonly measured using Pearson’s Correlation Coefficient, which defined as: \\[\\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma{_X}\\sigma{_Y}} = \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}\\sqrt{\\sum_{i=1}^n{(y_i-\\bar{y})^2}}}\\] Which is the covariance of X and Y divided by the product of the standard deviation of X and Y. The measure is bound between -1 and 1, where 1 indicates that two quantities move together all the time and -1 indicates two quantities move in exact opposite direction. To make the most of the correlation coefficient, it makes the most sense to use this measure with continuous variables. The idea of correlation can be generalized to other kinds of similarity. Suppose an organization, such as an online vendor or even a government agency, offers a portfolio of products and services. Typically, the needs and consumption of goods and services by customers are tracked for administrative and budgetary purposes. From the field of economics, it is common to observe that within a basket or portfolio of goods, two or more items may be complements: Someone who calls a government office to report noise pollution may also be in a place that may need to be checked for building structural problems (prioritization) Someone who buys hot dogs may also need to buy top-sliced buns (product recommendaiton) Someone who listens to Lenny Kravitz may also want to listen to Jimi Hendrix (product recommendaiton) Someone who reads a specific document in a database should also read a set of other documents (information retrieval) This is the fundamental idea behind recommendation engines.37 A customer of product A may be interested or may benefit from products X, Y, and Z. This can be inferred by how often two or more products are purchased together – essentially the Amazon, Netflix and general e-commerce experience. The basis of the technique known as item-item collaborative filtering starts with cosine similarity. Cosine similarity measures the angle between two vectors – basically if the vectors are going in the same direction. It is given as: \\[ cos(\\theta) = \\frac{\\sum_{i=1}^n{(X_i Y_i)}}{\\sqrt{\\sum_{i=1}^n{X_i^2}}\\sqrt{\\sum_{i=1}^n{Y_i^2}}}\\] Unlike the correlation coefficient, the input vectors should contain positive real numbers and returns a value between 0 and 1, where 1 indicates that two vectors are perfectly aligned. While it is easy to write the underlying cosine similarity function, fast matrix implementation of cosine similarity is available in the coop package with the function cosine(). In practice, how does this work? Take a look at the DIY recommendation engine example later in this chapter. Jaccard Similarity Coefficient Plagerism is a problem in academic and professional settings. With information ever more accessible via the internet, it becomes easier to simply copy and paste information. Imagine a case where 1,000 submit essays in a freshman English seminar – how can the uniqueness of essays be checked? Or how can news articles be monitored in order to find dependence between news agencies (e.g. one news outlet citing an article from another outlet)? In an online setting, new users of web applications may be prompted to provide their preferences to populate their profile so that they can be connected to relevant products and services. How are users matched to recommendations? These are basic cases in which the Jaccard Similarity Coefficient is a good fit. Given two vectors \\(X\\) and \\(Y\\), the Jaccard coefficient is the intersection \\(X \\cap Y\\) divided by the union of the two vectors \\(X \\cup Y\\): \\[ J(X,Y) = \\frac{|X \\cap Y|}{|X \\cup Y|} = \\frac{|X \\cap Y|}{|X| + |Y| - |X \\cap Y|}\\] In other words, it is a measure of how much two vectors overlap. Thus, the word frequencies of two or more documents can be compared and also proves to a convenient method of checking for the uniqueness of programming scripts. 5.3 Linguistic Distances Spelling errors are common and are a continuous challenge in entity resolution. Edit distances are methods of comparing two strings to determine their similarity. The Levenshtein Distance is a similarity measure that counts the number additions, substitutions, or deletions that are required to transform one string to another string.38 For example: the difference between the name “Jeff” and “Geoff” is 2: (1) substitute “J” for “G”, and (2) delete the “o”. In R, one function that implements Levenshtein Distance is adist(x, y), where x and y are string vectors. Below is a stylized output from adist() that provides the similarity measure for each string compbination. # Two sets of names x &lt;- c(&quot;Bill&quot;, &quot;Warren&quot;) y &lt;- c(&quot;Billy&quot;,&quot;Wally&quot;,&quot;Billie&quot;, &quot;Golly&quot;, &quot;William&quot;) # Calculate Levenshtein distances dist = adist(x, y) #Rename columns and rows row.names(dist) &lt;- x colnames(dist) &lt;- y #Print out print(dist) Table 5.1: Levenshtein distances for “Bill” and “Warren” Billy Wally Billie Golly William Bill 1 3 2 3 4 Warren 6 4 6 6 6 Levenshtein distances are quite useful with textual data, especially for surfacing potential alternative spellings. However, the choice of a “close match” is generally subjective. Another method of measuring linguistic distances is phonetically. Phonetic Algorithms index strings by sounds with respect to a target language. The soundex algorithm, for example, was developed to identify homophones – names that are pronounced the same but may have different spellings. This is done by encoding characters in names in a particular way that retains certain comparable sounds39: Keep the first letter of a string. Disregard the letters A, E, I, O, U, H, W, and Y. For each of the following groups of letters, replace with the associated number: 1 [B, F, P, V] 2 [C, G, J, K, Q, S, X, Z] 3 [D, T] 4 [L] 5 [M, N] 6 [R] If a name has double letters, drop one (e.g. Keep one r in Torres) If sequential number encodings are the same, keep only one. Examples from the National Archives: Pfister is coded as P-236 (P, F ignored, 2 for the S, 3 for the T, 6 for the R). Jackson is coded as J-250 (J, 2 for the C, K ignored, S ignored, 5 for the N, 0 added). Tymczak is coded as T-522 (T, 5 for the M, 2 for the C, Z ignored, 2 for the K). Since the vowel “A” separates the Z and K, the K is coded. If a name has a prefix (e.g. Van, Di) that is separated by a space, encode the name with and without the prefix and use both sets of encodings for search purposes. Example: Van Doren, Di Caprio, etc. Consonant separators (two rules). If the letters “H” or “W” separate two consonants with the same soundex code, the consonant to the right of the vowel is not coded.40 If a vowel (A, E, I, O, U) separates two consonants that have the same soundex code, the consonant to the right of the vowel is coded.41 The result of a soundex is a four character code in which the first character is a letter proceeded by three numbers. The phonics library enables the use of common phonetic algorithms, which can help facilitate search and matching of names. In the example below, different spellings of John and Eric are compared using soundex encodings. Notice how names that sounds the same are successfully mapped to the same encodings. #Load phonics package library(phonics) #John soundex(c(&quot;John&quot;, &quot;Jon&quot;, &quot;Jonathan&quot;)) ## [1] &quot;J500&quot; &quot;J500&quot; &quot;J535&quot; #Eric soundex(c(&quot;Eric&quot;, &quot;Erik&quot;, &quot;Erich&quot;, &quot;Enrique&quot;)) ## [1] &quot;E620&quot; &quot;E620&quot; &quot;E620&quot; &quot;E562&quot; Soundex is best for English language names and may not be adapted for all names. A number of variants have arisen with varying degrees of flexibility. Exercise Given the following rules, write a function to convert a string vector into soundex encodings using the first five rules. 5.4 Entropy Humans are creatures of habit, thus our range of actions are fairly predictable. Let’s suppose a restaurants in a city need to follow nearly a thousand statutes to remain in good standing. Chances are that a health inspector is not likely to know all statutes and will rely on a smaller set of violation types when conducting inspections. There is always the chance that the inspector will use extraneous and usual violations for unethical purposes, thus inspectors should generally issue a consistent set of violations. With this in mind, how does one detect abuse? Similarly, given a set of student characteristics, what constitutes better information? Information must contain signal that differentiates one idea/thing/topic from another in a consistent manner. Given a dummary variable indicating if a student has a SAT score above 1500 and another dummy variable indicating if a student’s toe nail is longer than 5 inches, which is better determinant of college admissions? Logically, it would be whichever measure that is able to partition the admissions pool into high success and low success. Enter entropy. With it origins in physics, entropy is a measure of randomness – a way to compare how many states a particle holds over some unit time. In statistics, it is a common method for identifying outliers and useful information of a system, given as: \\[\\text{entropy} = -\\sum_i^{n}p_i \\times log_np_i\\] where subscripts \\(p_i\\) is the proportion of a system that occupied a state or condition \\(i\\). To contextualize this, let’s take a simple case of three inspectors in which each state is a type of violation that had been issued: #The volume of each type of violation that has been issued inspector.1 &lt;- c(2, 10, 20, 10, 10, 30, 20, 5, 2, 5, 2, 3, 2) inspector.2 &lt;- c(50, 20, 20) inspector.3 &lt;- c(30, 20, 30, 20) To find the inspector who has not been consistent with their violation issuances, we can calculate entropy: entropy &lt;- function(x){ # Returns entropy of a vector # # Args: # x = vector of states # # returns: # Raw entropy # tot &lt;- sum(x) prop &lt;- x/tot return(-sum(prop * log(prop))) } And apply the function to each inspector. The inspector with the highest entropy is inspector 1. This does not mean that inspector 1 did anything wrong, but perhaps should be evaluated a bit closer. entropy(inspector.1) ## [1] 2.185227 entropy(inspector.2) ## [1] 0.995027 entropy(inspector.3) ## [1] 1.366159 An another way of viewing entropy is as a measure of homogeneity. Relative to inspector 1, inspectors 2 and 3 were relatively consistent in their issuance history, thus there is little indication of deviant behavior. As we will see later in the book, entropy plays a central role in decision tree learning methods for classification. 5.5 DIY 5.5.1 Given product [A], which other products [X, Y, Z] should I recommend? Motivation Principles Two types of Item-to-Item collaborative filtering A Worked Example Consumer-level purchasing behavior is mostly collected on e-commerce proprietary systems or available for purchase from data aggregators. However, there is an anonymized, publically available, person-level data set that is published by the U.S. Bureua of Labor Statistics (BLS). The BLS Consumer Expenditure Survey (CEX) provides an in-depth view into Americans’ purchasing patterns. The survey is divided into two parts. The first is an interview for larger purchases and the second is a diary survey focuses on frequently purchased items. These surveys are primarily aimed at informing the market basket of the Consumer Price Index (CPI). From an applied perspective, this data can be used to illustrate the mechanics of producing an item-to-item matrix through collaborative filtering (CF). Data for 2010 through 2016 has been aggregated and processed into a convenient format where each row represents a consumption unit (essentially a household) and each column represents a different type of item. Each household collects data on two separate weeks in the same year, but for the purpose of this exercise, the two weeks’ purchases are collapsed into one. To kick off this exercise, we will first load the data using the digIt() function. The data is also avalable at https://s3.amazonaws.com/dspp/cex_binary_matrix.Rda. library(digIt) purchased &lt;- digIt(&quot;cex_binary&quot;) In total, the dataset contains \\(n = 82809\\) records with \\(k = 548\\) purchase items plus a household ID. Each feature provides a highly detail description with a broad range of products such as candy, bicycles, and wine. Table 5.2: 40 types of purchase items from the BLS CEX data set visual goods oranges postage used motorcycles material &amp; suppli… salt spices other… rent wine at vending m… clocks &amp; other ho… gas tank repair r… landscaping items… other serving pie… round steak shoe repair &amp; oth… smoking access drivers license candy &amp; chewing g… girls skirts pant… toys games arts &amp;… clothing storage drive shaft &amp; rea… potato chips &amp; ot… docking &amp; landing… apparel laundry &amp;… snacks &amp; nonalcoh… vcr s &amp; video dis… noncarbonated fru… canned misc veget… appliance repair … eyeglasses &amp; cont… boys access bicycles other pork athletic gear gam… jams preserves ot… pet food olives pickles re… womens sportcoats… washers &amp; dryers sewing machines Each feature is a binary indicator of whether a given household had purchased a given item during the data collection period. It becomes easy to see that overlaps between households may serve as the basis of finding items that may be purchased together. For example, an examination of the overlap between items would indicate the apples and citrus are more related than biscuits. Empirically, we find that the cosine similarity \\(cos(X,Y) = \\frac{\\sum{(X Y)}}{\\sqrt{\\sum{X^2}}\\sqrt{\\sum{Y^2}}}\\) of apples and biscuits is 0.58 and apples and citrus is 0.77, confirming our initial observation. Table 5.3: View of six random observations and three randomly selected items unit id apples biscuit &amp; rolls citrus fruits excl. oranges 1042252 1 1 1 1078541 0 0 1 1221502 1 0 1 3452592 0 0 1 3543762 1 0 1 2895591 0 0 0 Imagine the pain of manually calculating and modifying a formula to calculate the cosine similarity. For a matrix of 500+ items, the calculation will need to be performed up to 250,000 times. To streamline the process, we write two functions to facilitate computation. The first cosSim() calculates the cosine similarity for two vectors. The second cosSimMat() produces an item-item recommendation list. cosSim &lt;- function(a, b){ # Desc. # Returns cosine similarity for two vectors # # Args. # Two numeric vectors # # Returns. # A numeric value between 0 and 1 # complete &lt;- !is.na(a) &amp; !is.na(b) a &lt;- a[complete] b &lt;- b[complete] z &lt;- sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2))) return(z) } To test out the function, we correlate “sirloin.steak” and “sauces.and.gravies”. cosSim(purchased$sirloin.steak, purchased$sauces.and.gravies) ## [1] 0.2114076 As a next step, we whittle down the data set to items that were purchased more than 3000 times across all households. This is done for convenience as calculating all pairs of cosine similarity values is computationally costly. Reduce the sample just for illustrative purposes #Items items &lt;- purchased[,2:ncol(purchased)] #Calculate the number of times an item was purchased (sum by column) column.sums &lt;- apply(items, 2, sum) #Keep items that were purchased at least 3000 times items &lt;- items[, column.sums &gt; 3000] With the item list cut down to a manageable set, a new function cosSimMat() is written to populate the similarity matrix in long form. While there are far faster methods of calculating the similarity matrix such as the cosine() function in the coop package, there is no better way of gaining an expert understanding of such problems than through writing the functions from the ground up. cosSimMat &lt;- function(items){ # # Desc: # Constructs item-to-item similarity matrix # # Args: # items = matrix of items purchased by consumer # # Returns: # A data frame with the similarity score for each item-item pair # ##Find all unique item-item combinations ##as data frame of column index pairs combos &lt;- expand.grid(x = 1:ncol(items), y = 1:ncol(items)) index &lt;- !duplicated(t(apply(combos, 1, sort))) combos &lt;- combos[index, ] #Loop through each combination based on combo index out &lt;- lapply(1:nrow(combos), function(i){ left &lt;- combos[i, &quot;x&quot;] right &lt;- combos[i, &quot;y&quot;] score &lt;- cosSim(items[, left], items[, right]) return(data.frame(x = left, y = right, score = score)) }) #Populate item names df &lt;- do.call(rbind, out) df$x &lt;- colnames(items)[df$x] df$y &lt;- colnames(items)[df$y] return(df) } With the function fully built, run cosSimMat() on the items matrix, then examine the first six items as a check. Notice that the output may be challenging to navigate. recs &lt;- cosSimMat(items) head(recs, 6) Table 5.4: Example recommendations x y score apples apples 1.0000000 bacon apples 0.2417474 baking.needs.and.misc.products apples 0.2444923 bananas apples 0.4603425 beer.and.ale apples 0.1621774 beer.and.ale.at.full.service.restaurants apples 0.1317498 Thus, to make navigating the matrix an accessible task, we create a simple utility function. findItem &lt;- function(recs, item.name){ # # Desc: # Conducts keywords search for item names # # Args: # recs = data frame output from cosSimMat # item.name = string containing part of item name to be searched # # Returns: # String vector of matched names # itemList &lt;- unique(c(grep(item.name, recs[[&quot;x&quot;]], value = TRUE), grep(item.name, recs[[&quot;y&quot;]], value = TRUE))) return(itemList) } We now test the findItem() function with the term “veg”. findItem(recs, &quot;veg&quot;) ## [1] &quot;canned.misc.vegetables&quot; &quot;dried.misc.vegetables&quot; ## [3] &quot;fresh.and.canned.vegetable.juices&quot; &quot;frozen.vegetables&quot; ## [5] &quot;other.fresh.vegetables&quot; To retrieve the top 10 results from the recommendations, the getRec() function is developed and returns results in a readable format. getRec &lt;- function(recs, item.id, len = 10){ # # Desc: # Returns top recommended items # # Args: # recs = data frame output from cosSimMat # item.id = matched series name (use getItem to find items) # len = number of results. Default = 10 # # Returns: # Data frame of most associated items from recs file # ##Get matching records index &lt;- unique(c(grep(item.id, recs$x), grep(item.id, recs$y))) results &lt;- recs[index, ] ##Clean up output results &lt;- results[results$x != results$y, ] results$x[results$x == item.id] &lt;- results$y[results$x == item.id] results &lt;- results[order(-results$score),] # results$x &lt;- gsub(&quot;\\\\.&quot;, &quot; &quot;, results$x) results$x &lt;- substr(results$x, 1, 40) results &lt;- results[1:len, c(&quot;x&quot;,&quot;score&quot;)] colnames(results) &lt;- c(&quot;item&quot;,&quot;score&quot;) return(results) } The value of the retrieval function is clear – if a consumer has purchased fresh and canned vegetable juices, then perhaps there is an opportunity to canned and bottled fruit juice and milk to a lesser degree. getRec(recs, &quot;fresh.and.canned.vegetable.juices&quot;) Table 5.5: Recommendations for (1) Beer &amp; Ale and (2) Frozen Vegetables Matches: Beer &amp; Ale Score Matches: Frozen Vegetables Score dinner at full service restaurants 0.396 other fresh vegetables 0.428 other alcoholic beverages at full servic 0.329 cheese 0.421 wine 0.317 fresh milk all types 0.406 gasoline 0.304 potato chips and other snacks 0.401 snacks and nonalcoholic beverages at ful 0.284 misc prepared foods 0.401 wine at full service restaurants 0.282 sauces and gravies 0.395 other fresh vegetables 0.276 other fresh fruits 0.387 lunch at fast food take out delivery con 0.276 bread other than white 0.383 fresh milk all types 0.269 bananas 0.379 lunch at full service restaurants 0.266 ready to eat and cooked cereals 0.369 This example is straight-forward from an technical perspective, but there are moving pieces and conditions that effect the ultimate success of a recommendation engine. In an item-item recommender, data is needed – an obvious necessity. Thus, an item-item strategy is only possible when a system has been collecting data for some time. If a recommender is absolutely required, the cold start problem will force one to make certain assumptions on how to recommend information in the absence of observed human behavior. An alternative strategy is content-based, which requires that the qualities of products. in an inventory (e.g. type of product, price, characteristics) are articulated as generalizable features and users are asked to provide their preferences, which in turn map to the product features. Then, the features for each product and customer preferences are compared using a Jaccard Similarity Coefficient – essentially looking for the overlap of qualities. There is much more that goes into a recommendation engine than just the calculation. How the recommendations are surfaced to consumer is a matter of user experience and interface design, which may have a large effect on whether the recommendations are acceepted by users. More often than not, the construction of a recommender system is a team effort, requiring data scientists, web developers, data engineers and product managers to create a cohesive, user-friendly but technically sound product. Exercises Write a new function jaccardSim() to calculate the Jaccard Similarity Coefficient: \\[J(X,Y) = \\frac{|X \\cap Y|}{|X \\cup Y|} = \\frac{|X \\cap Y|}{|X| + |Y| - |X \\cap Y|}\\] Then reconstruct the similarity matrix using the same steps as shown in the DIY. Compare the matches from each cosine similarity and Jaccard similarity results for each Beer &amp; Ale and Frozen Vegetables. 5.5.2 Which texts are saying similar things? https://academic.oup.com/bjc/article-abstract/32/3/340/319313?redirectedFrom=PDF↩ https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf↩ Ref required↩ https://www.archives.gov/research/census/soundex.html↩ https://www.archives.gov/research/census/soundex.html↩ https://www.archives.gov/research/census/soundex.html↩ "],
["clustering.html", "Chapter 6 Clustering 6.1 Everything is related to everything else 6.2 Technical Foundations 6.3 DIY", " Chapter 6 Clustering 6.1 Everything is related to everything else In a 1970 article, Geographer Waldo Tobler wrote “Everything is related to everything else, but near things are more related than distant things.”42 Tobler was getting at the idea that people and phenomena tend to cluster together – things that are clustered have short distances from one another relative to other things. Perhaps the easiest way to see the effect of spatial dependence is in night time satellite imagery. Across the US’ 35 largest cities, the urban landscape takes shape with lights clustered along streets and in certain parts of town, sometimes clustering in the main thoroughfares and other cases in residential areas and major roadways. We can see the clusters, but how does one measure it? Night time imagery from the NOAA-NASA Suomi NPP Satellite’s Visible Infrared Imaging Radiometer Suite (VIIRS) The idea of clustering can extend beyond just time and space. In marketing, consumers are regularly grouped into clusters that represent distinct behaviors and preferences. For example, hotel-goers of high end resorts will be more likely part of a specific affluent customer segment than those who choose to stay at a budget motel, which in turn form the basis of characterizing demand segments. In looking at markets, certain industries may be viewed as a cluster of economic activity as they rise and fall together due to their dependence on one another or their products are complements in the market. In epidemiology, outbreaks of a disease tend to be physically clustered together. In some law firms, data scientists may develop topic modeling algorithms to automatically tag and cluster hundreds of thousands of documents for improved search. Unsupervised learning can help. It is a branch of machine learning that deals with unlabeled data to identify statistically-occurring patterns – let the data fall where they may. Building upon measures of similarity and distance, this chapter provides a short survey of types of unsupervised learning and its uses. 6.2 Technical Foundations The fundamental idea of clustering methods is to express a set of attributes in two or more discrete groups. A visual inspection of the probability distribution of a data series often will give clues as to what natural clusters may lie within. For example, the bi-modal and quad-modal distributions below can be easily grouped into clear groups. Visually, the goal is to find the center of mass of sub-distributions, then assign values that are closest to a proposed center. Figure 6.1: A multi-modal distribution naturally yield two clusters The same visual process can easily guide clustering in two and three dimensions. Generally, greater distance between the masses – or separability – allows for less ambiguous cut offs between two groups. Figure 6.2: Two- and three- dimensional clusters. The task of clustering becomes complicated when subdistributions overlap in space – how to tell one from another? Imagine attempting to find clusters in four-dimensional space let alone n-dimensional space; the visual approach is no longer an option. Figure 6.3: Case of mixed distributions Clustering algorithms are designed to explore underlying patterns when labeled data are not available. The number of strategies used to cluster data points is as numerous as the approaches used to characterize similarity. Some methods such as k-means are focused on finding a fixed number of centroids, or finding center masses of clusters. More agglomerative approaches like hierarchical clustering examine pairwise distances between all points and group points together first in order to capture a hierarchy of relationships. While there are many other techniques, we focus on these two methods given their ease of use and versatility. 6.2.1 K-Means The k-means clustering is a technique to identify clusters of observations by treating features as coordinates in n-dimensional space. The k in k-means is specified by the analyst – it is the number of clusters that will be returned upon running the algorithm. k is not a known quantity and will need to be optimized by the analyst. The technique is fairly straight forward to optimize and is one that is iterative as shown in the pseudocode below: Initialize k centroids Repeat following until convergence: Calculate distance between each record n and centroid k Assign points to nearest centroid Update centroid coordinates as average of each feature per cluster The first step involves selecting \\(k\\)-number of random centroids from the feature space and giving each centroid a label. For each observation in the data, calculate the Euclidean distance ($ d(x_1,x_2) = $) to all initial centroids, then assign each point to the closest centroid. This is known as the assignment step – all points take the label of its closest centroid. It is unlikely that this initial assignment is likely suboptimal, thus the algorithm will update the centroid coordinates by calculating the mean value of each feature within each cluster. Upon doing so, this assignment-update procedure is iteratively repeated until the centroid coordinates no longer change between iterations (see illustration below). Figure 6.4: Illustration of k-means algorithm from initialization to convergence Central to algorithm is goal to find some set of \\(k\\) coordinates that minimize the within-cluster sum of squares (WSS): \\[arg min \\sum_{j=1}^k\\sum_{i=1}^n ||x_{i,j} - \\mu_j||^2\\] where the sum of the distance \\(x\\) of each point \\(i\\) in cluster \\(j\\) to its corresponding centroid of \\(j\\). Distance is calculated in terms of all input features \\(x\\) and the \\(j^{th}\\) cluster centroid \\(\\mu\\). Assumptions While k-means is a simple algorithm, its performance and effectiveness is guided by a number of key assumptions at each step of computation. Scale. As k-means treats features as coordinates, each feature is assumed to have equal importance, which in turn means that results may be inadvertently biased simply by the scale and variances of underlying features. To remedy this problem, input features should be mean-centered standardized (\\(\\frac{x_i-\\mu}{\\sigma}\\)) or otherwise transformed to reduce scaling effects. Note, however, that the influence of scaling may not always be removed. For example, a data set containing both continuous and binary features would likely perform quite poorly as Euclidean distances are not well-suited for binary. Thus, where possible, apply k-means when the formats are homogeneous, doing so using Euclidean L2-distances for continuous and binary distances for matrices of discrete features. Missing Values. K-Means do not handle missing values as each data point is essentially a coordinate. Thus, often times k-means models are usually reserved for complete data sets. Stability of Clusters. The initialization step of the algorithm chooses \\(k\\) initial centroids at random. The initial random selection is known to lead to suboptimal and unstable clusters. The instability in the results can be observed when running the algorithm for some value of \\(k\\) multiple times, sometimes leading to different cluster composition: holding \\(k\\) constant between model runs, a record \\(i = 1\\) may be in the same cluster as \\(i = 10, 23, 40\\) in one set of results, but only with \\(i = 23\\) in another model run. The stability of clusters may be due to a number of things, such as a suboptimal choice of \\(k\\), a high number features that add noise to the optimization process, among others. Figure 6.5: Comparison of a suboptimal result and optimal result Choice of K. Selecting the best value of \\(k\\) is arguably a subjective affair: there is a lack of consensus regarding how to identify \\(k\\). One method known as the Elbow method chooses \\(k\\) at the inflection point where an additional cluster does not significantly reduce the variance explained or reduction of error. The simplest method of identifying the inflection point can be seen by plotting the percent WSS over all values of \\(k\\) that were tested. This approach is deceptively simple as the inflection point might not manifest itself in some data sets. Figure 6.6: Elbow method: Choose k at the inflection point An alternative, but far more computationally intensive approach involves calculating the silhouette, which is compares estimates the similarity of a given observation \\(i\\) as compared to observations within and outside the cluster. The silhouette \\(s(i)\\) is defined as: \\[s(i) = \\frac{b_i-a_i}{max(a_i,b_i)}\\] where \\(a_i\\) is the Euclidean distance between a point \\(i\\) to other points in the same cluster, \\(b_i\\) is the minimum distance between \\(i\\) and any other cluster the sample. The values of \\(s(i)\\) fall between -1 and 1, where 1 indicates that an observation is well-matched with its cluster and -1 indicates that fewer or more clusters may be required to achieve a better match. Note that silhouettes do not scale well with very large data sets as a \\(n \\times n\\) similarity matrix (e.g. distance between all points to all points). Often times, a smaller sample should be used to enable the use of this method. For a step-by-step walkthrough of the application of the k-means algorithm, see How much of the ground is covered in [vegetation/buildings/economic activity]? in the DIY section of this chapter. 6.2.2 Hierarchical clustering Whereas k-means initializes on random centroids, hierarchical clustering take a more computationally costly ground-up approach: Calculate distance d between all points All points are start as their own clusters (singletons) Do until there is only one cluster: Find the closest pair of clusters in terms of linkage distance Merge into a single cluster Recalculate distances from new cluster to all other clusters Stop when all points are in one cluster For a step-by-step walkthrough of the application of the hierarchical clustering algorithm, see How do I characterize the demand for [products/services]? in the DIY section of this chapter. 6.3 DIY 6.3.1 How much of the ground is covered in [vegetation/buildings/economic activity]? Photographs contain data. Some are more structured than others. Satellite imagery, for example, can be directly used to infer patterns on the ground, especially relating to natural phenomena like agriculture. Free imagery is readily available from various satellite instruments such as Aqua/Terra MODIS (NASA), ASTER (NASA/Japan), VIIRS (NASA/NOAA), Landsat Operational Land Imager (NASA/USGS), among others. Private firms such as Digital Globe and Planet also operate their own satellites and provide commercial data services. Suppose there is a need to know how much healthy vegetation is present in farm lands in the US heartland. Satellite imagery can easily be used to support this task. A U.S.-Japan team used the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) instrument on the Terra satellite to capture images of crop fields in Kansas. The image below captures a 37.2-km x 38.8-km area where green areas indicate healthy vegetation.43 #Library library(raster) library(digIt) img &lt;- digIt(&quot;color_segment_kansas&quot;) plotRGB(img) Figure 6.7: Crops in Finney County KS. Via NASA/GSFC/METI/Japan Space Systems, and U.S./Japan ASTER Science Team Suppose the following question were asked: How much of the crop field is covered in healthy vegetation? In earth science, satellite imagery can be converted into vegetation indices, then cutoffs can be applied. The choice of a cutoff runs the risk of subjective biases. The alternative is to use k-means clustering to conduct color quantization, which is a process that reduces the number of colors in an image into fewer distinct colors. Photographs are comprised of a three-dimensional array that essentially resembles three matrices sandwiched together. Each matrix is an \\(n \\times m\\) matrix for each red-green-blue (RGB). The goal is to cluster on the colors, which requires the each of the \\(n \\times m\\) matrices to be transformed into a two dimensional matrix with 3 columns (one for each color) and of length \\(nm\\). K-means is applied to this matrix to obtain color groups. Getting Started In the wild, the digIt() function is not available. An image would normally need to be downloaded and loaded as a brick() using the raster package. For simplicity, we use the digIt library to download and load the ASTER data. #Library library(raster) library(digIt) img &lt;- digIt(&quot;color_segment_kansas&quot;) The image is converted into a matrix containing three vectors of equal length: one for each RGB value. #Dimensions dim(img) ## [1] 2481 2589 3 #Convert image into columns data &lt;- cbind(as.vector(img[[1]]), as.vector(img[[2]]), as.vector(img[[3]])) With the data in the right shape, k-means can be applied. In this example, we use the kmeans() function that is built into R: kmeans(x, k) where: x is a data frame or matrix of numerical values k is the number of clusters The result of the kmeans() function contains a number of attributes such as the cluster assignment of each observation. To evaluate the fitness of the cluster, a silhouette statistic can be calculated using the silhouette() function in the cluster library: silhouette(cluster, distance) where: cluster is the cluster assignment. distance is a dissimilarity matrix of input features produced by dist(). Given the size of the input matrix (\\(n = 2481 \\times 2589 = 6423309\\)), the dissimilarity matrix is produced on a sample of \\(n = 20000\\). Below, k-means is tested for values of \\(k = 2\\) to \\(k = 10\\) using a random sample of \\(n = 20000\\). Before the loop, the sample is taken, then the dissimilarity matrix is calculated using the dist() function. Within the loop, k-means results are assigned to the object res from which the cluster assignments are extracted. The silhouette is then calculated and assigned to the sil object, from which the mean silhouette is estimated from observation level silhouettes (third column). #Load cluster library library(cluster) #Calculate distance object using sample of n = 10000 set.seed(10) subdata &lt;- data[sample(data, 20000),] d &lt;- dist(subdata) #Set up placeholder for silhouette values sil.out &lt;- data.frame() #Loop through values of k for(k in 2:10){ set.seed(20) #Run k-means, save to o res &lt;- kmeans(subdata, k) #Get silhouette sil &lt;- silhouette(res$cluster, d) #Get summary values of silhouette temp &lt;- data.frame(k.level = k, avg = mean(sil[,3])) sil.out &lt;- rbind(sil.out, temp) } In color quantization exercises, lower values of \\(k\\) should be used. In the case below, the grid search suggests that \\(k = 2\\) provides the most favorable cluster results. #Plot result plot(sil.out[, c(&quot;k.level&quot;, &quot;avg&quot;)], type = &quot;l&quot;, col = &quot;orange&quot;, ylab = &quot;Mean Silhouette&quot;, xlab = &quot;k&quot;) points(sil.out[, c(&quot;k.level&quot;, &quot;avg&quot;)], pch = 19, col = &quot;orange&quot;) Figure 6.8: Mean silhouette by k The k-means model is then estimated on the entire data set for \\(k = 2\\). To visually check our results, we need to convert the vector of cluster assignments to a matrix with the same dimensions as the original image img. The matrix contains all the same information as an image, but is not in the right data class. Using raster(), the matrix can be converted into an raster image format containing cluster assignments. #K values set.seed(123) res &lt;- kmeans(data, 2) #Convert cluster labels into matrix mat &lt;- matrix(res$cluster, ncol = ncol(img), nrow = nrow(img), byrow = TRUE) img2 &lt;- raster(mat) With the data in the right form, the cluster assignments are rendered as an image. Notice that the healthy green areas are coded in green, which corresponds with cluster #2. plot(img2, box=FALSE, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, frame.plot = FALSE, col = c(&quot;black&quot;, &quot;green&quot;)) Figure 6.9: Color quantization for k = 3 To calculate the proportion of the land that is covered in healthy vegetation as well as approximate land area, we can use the following calculation: prop &lt;- mean(mat == 2) print(paste0(&quot;%Area = &quot;, prop)) ## [1] &quot;%Area = 0.482713349147612&quot; print(paste0(&quot;km2 = &quot;, 37.2 * 38.8 * prop)) ## [1] &quot;km2 = 696.729139625697&quot; 6.3.2 How do I characterize the demand for [products/services]? Clustering algorithms are useful for more exploratory purposes, especially for characterizing types of demand for services. In private industry, data on product consumption can be used to group types of customers and their preferences together, which in turn form the basis of customer segments. In the public sector, this is not the norm, but just because it is uncommon does not prevent it from being the norm in the future. 311 Call Centers have become common place in US cities. These citizen-facing centers triage requests for local government services and dispatch resources to address needs. 311 also has become a rich source of data on what constituents need. In New York City, millions of calls and hundreds of types of requests are logged and made public via the open data platform. Suppose the following question were asked: How do I characterize the demand for [products/services]? or otherwise stated: Which constituents share similar concerns? Using NYC’s data, the millions of 311 requests were reprocessed into grid points in Lat/Lon with precision to three places (e.g. lat = 40.552, lon = -74.212). The data are available using the digIt library: library(digIt) nyc311 &lt;- digIt(&quot;nyc311_gridded&quot;) Overall, the data set contains n = 57337 and k = 139 with features such as “general.construction/plumbing” and “sweeping/inadequate” dim(nyc311) ## [1] 57337 139 colnames(nyc311)[1:20] ## [1] &quot;lat&quot; &quot;lon&quot; ## [3] &quot;adopt-a-basket&quot; &quot;air.quality&quot; ## [5] &quot;animal.abuse&quot; &quot;animal.in.a.park&quot; ## [7] &quot;appliance&quot; &quot;asbestos&quot; ## [9] &quot;beach/pool/sauna.complaint&quot; &quot;best/site.safety&quot; ## [11] &quot;bike.rack.condition&quot; &quot;bike/roller/skate.chronic&quot; ## [13] &quot;blocked.driveway&quot; &quot;boilers&quot; ## [15] &quot;bridge.condition&quot; &quot;broken.muni.meter&quot; ## [17] &quot;broken.parking.meter&quot; &quot;building/use&quot; ## [19] &quot;bus.stop.shelter.placement&quot; &quot;city.vehicle.placard.complaint&quot; Although the sample size is modest, dissimilarity matrix would yield 3.3 billion data elements (57337\\(^2\\)). For simplicity, we sample only \\(n = 15000\\) records. nyc311.short &lt;- nyc311[sample(1:nrow(nyc311), 15000), ] The data should be on the same scale with the same mean (0) and unit variance. We can use the scale() function to scale all features, then use the dist() function to produce a dissimilarity matrix: dist(x, method) where: x is a matrix of continuous values. method is a string value that indicates the type of dissimilarity used, which can include “binary”, “minkowski”, “euclidean” among others where the latter is the default. For cases where the data are all binary or discrete, a binary distance may be more appropriate. For continuous values, Euclidean is the best bet. For the 311 data, the dissimilarity matrix is based on Euclidean distance, then assigned to the object dis.mat. #Scale columns nyc.short &lt;- scale(nyc311.short[,3:ncol(nyc311.short)]) #Create dissimilarity matrix using Euclidean distances dis.mat &lt;- dist(as.matrix(nyc.short), method = &quot;euclidean&quot;) Finally, the hierarchical clustering algorithm can be run using the hclust() command: hclust(d, method) where d is a dissimilarity matrix from dist() method is a string value specifying the agglomeration method, such as “single”, “complete”, “average”, “centroid”, “ward.D” among others. Note that the time to processing a data set is dependent on the complexity of the method. Below, we pass the dis.mat object to the hclust() function is choose Ward’s D to guide agglomeration. #Run hierarchical clustering hc &lt;- hclust(dis.mat, method = &quot;ward.D&quot;) The results can be easily plotted as a dendrogram, which shows the hierarchical relationships within the data. The graph below is rendered by plotting the hc object using plot(). At the bottom of the dendrogram are all observations in the sample. Given the number of observations included, it is challenging to clearly identify each observation. As we move from the bottom to the top, vertical lines emerge and come together, representing observations and subclusters that were clustered together. Eventually, all subclusters are linked at the top. A given height in the graph indicates the cumulative number of linkages that are contained in the dendrogram up to that point. Given all the possible clusters, the number of clusters could be determined purely based on the height. Fewer the clusters, greater the height. par(mfrow = c(1,3)) # Draw dendrogram plot(hc, cex = 0.001, col = &quot;grey&quot;, main = &quot;Dendrogram&quot;) # Cut at k = 3 plot(hc, cex = 0.001, col = &quot;grey&quot;, main = &quot;k = 2&quot;) rect.hclust(hc, k = 2, border=&quot;red&quot;) # Cut at k = 10 plot(hc, cex = 0.001, col = &quot;grey&quot;, main = &quot;k = 10&quot;) rect.hclust(hc, k = 10, border=&quot;red&quot;) Figure 6.10: Dendrogram of hierarchical clustering on gridded NYC 311 data The sample generally appears to be cleaner cut at \\(k=2\\) than at higher values, thus we cut the sample into two groups using the cutree() function. groups &lt;- cutree(hc, k = 2) While it is easy to separate the observations into their respective clusters, the process leaves much to be desired when it comes to interpretation. Ideally, the most common characteristics could be surfaced to characterize the cluster. To do so, a custom function (clustSum) is required to calculate the mean share of each service request for each cluster and return the top X most frequent requests. clustSum &lt;- function(data, clusters, depth = 3, horizontal = FALSE){ # Summarize cluster variables by most frequently occurring # # Args: # data: input data # clusters: vector of cluster labels # depth: top X most frequent variables (depth = 3 as default) # horizontal: control format of results. FALSE means one cluster per row. # # Returns: # A data frame of k-number of centroids # #Calculate means, rotate such that features = rows overview &lt;- aggregate(data, list(clusters), FUN = mean) #Transpose data so that each row contains the mean frequency of a complaint type overview &lt;- as.data.frame(cbind(colnames(overview)[2:ncol(overview)], t(overview[,2:ncol(overview)]))) #Clean up table row.names(overview) &lt;- 1:nrow(overview) overview[,1] &lt;- gsub(&quot;count.&quot;,&quot;&quot;,as.character(overview[,1])) #Clean up values as numerics for(i in 2:ncol(overview)){ overview[,i] &lt;- round(as.numeric(as.character(overview[,i])),2) } #Get top X features depth.temp &lt;- data.frame() for(i in 2:ncol(overview)){ temp &lt;- overview[order(-overview[,i]), ] temp &lt;- paste(&quot;(&quot;,temp[,i], &quot;): &quot;, temp[,1], sep = &quot;&quot;) temp &lt;- as.data.frame(matrix(temp[1:depth], nrow = 1, ncol = depth)) colnames(temp) &lt;- paste0(&quot;Rank.&quot;, 1:depth) depth.temp &lt;- rbind(depth.temp, temp) } depth.temp &lt;- cbind(data.frame(table(clusters)), depth.temp) #Rotate? if(horizontal == TRUE){ depth.temp &lt;- t(depth.temp) } return(depth.temp) } The result indicates that one cluster is associated with road-way conditions and the other cluster is associated with residential problems. Note that the value in parentheses indicates what proportion of a given type of service request will appear in the average grid cell in a cluster. clustSum(nyc311.short[,3:ncol(nyc311.short)], groups, depth = 3) ## clusters Freq Rank.1 Rank.2 ## 1 1 7047 (0.07): heat/hot.water (0.06): noise.-.residential ## 2 2 7953 (0.1): street.condition (0.09): illegal.parking ## Rank.3 ## 1 (0.05): blocked.driveway ## 2 (0.08): blocked.driveway As public housing tends to be clustered in New York City, one might expect to see spatial patterns in the data. The clusters are mapped back to the original grid cells and indicate that there is some degree of spatial clustering of service requests. From an operational perspective, clustering could be an analytical strategy to help field operations to employ preventive maintenance. For example, a housing unit may have heating issues and may also be suceptible to santitation issues. Knowing which requests tend to cluster together could give way to more coordinated visits, thereby reducing the amount of scheduling burden placed on customers. #Set color palette palette(colorRampPalette(c(&#39;#a6cee3&#39;,&#39;#6a3d9a&#39;))(2)) #Graph lat-lons with color coding by cluster plot(nyc311.short$lon, nyc311.short$lat, col = factor(groups), pch = 15, cex = 0.3, frame.plot = FALSE, yaxt = &#39;n&#39;, ann = FALSE, xaxt = &#39;n&#39;) legend(x = &quot;topleft&quot;, bty = &quot;n&quot;, legend = levels(factor(groups)), cex = 1, x.intersp = 0, xjust = 0, yjust = 0, text.col=seq_along(levels(factor(groups)))) Tobler W., (1970) “A computer movie simulating urban growth in the Detroit region”. Economic Geography, 46(Supplement): 234-240.↩ https://www.nasa.gov/topics/earth/earthmonth/earthmonth_2013_01.html↩ "],
["model-validation-how-do-we-know-what-we-know.html", "Chapter 7 Model validation: How do we know what we know? 7.1 When knowing the answer is not enough 7.2 Converting a farcical scenario into practical knowledge", " Chapter 7 Model validation: How do we know what we know? 7.1 When knowing the answer is not enough It’s the first day of differential equations class and anxiety is in the air. Students chat amongst themselves, trying to play cool as the anticipation of a semester of tedious, grueling problem sets weighs down on them – the sort that merit all-nighters. As a distant clock tower chimes at the top of the hour, the professor stands up and begins to hand out a stack of packets, with a final exam and answer key. The first page is a blurry cover letter that appears to have been photocopied to the \\(n^{th}\\) degree: Dear Student: You will be graded on this final exam and this exam alone. The answer key has been provided – memorize it. When you’re done, copy your answers into this online survey form. Good luck with the rest of your time at the university. As the first student finishes reading the letter, the door slams as the professor makes her getaway. So, from an epistemological perspective, what exactly does this final exam score reflect? To answer this question, we would need to first define the objective of a test, which is measuring the degree to which students can retain knowledge of and apply mathematical concepts – not if students remember the specific sequence of numbers and characters that are on a specific handout or event the use of the CTRL + C - CTRL + V hot key combination. If the resulting test score were used to assess student performance, we would grossly overstate and overestimate their skill. We shall refer to this as the final exam as practice paradigm – it is not possible to assess learned ability if a student that has already seen the final exam. From the testing perspective, the goal is to estimate \\(Pr(\\text{Correct | Subject Matter Knowledge})\\) – not \\(Pr(\\text{Correct | Test Question})\\), which is biased as \\(\\text{mean(score on practice final exam) ~ mean(score on final test)}\\). So, what’s a more correct way of testing? We all know the answer: teach the material, assign homeworks and provide practice exams before administering the final exam. To keep units consistent, we’ll simply focus on practice and final exams. A common experiment design is to provide \\(k\\) number of practice exams for students to study and keep at least one hold-out exam to test their learned abilities. Assuming that the structure of all exams are approximately the same but with different questions, this train - test design is a stronger method for assessing how well knowledge is generalized out-of-sample. There, of course, are a couple of issues with this approach: A student may actually know the material inside and out, but may stumble during the exam at random. Thus, placing the whole semester’s grade on one exam may not provide a true indication of their ability. If there are a maximum of \\(k\\) available tests, then it may be possible to split the tests into more hold-outs. This, however, reduces the amount of opportunities for a student to learn the material and may thus increase the variability in the extra testing. In the classroom, there is not a magic bullet design. Perhaps science fiction could help. Suppose the professor has six possible exams to give to students. She decides to give five exams as practice and reserve a sixth as the actual exam. If the same exact students existed in six different parallel universes with the same behaviors, intelligence, laws of physics, etc (this is the science fiction part) and the professor had some trans-dimensional method of monitoring student performance, then we could do the following: For universe A, use practice exams 1, 2, 3, 4, 5 for studying. Hold out exam 6 as the final test. For universe B, use practice exams 1, 2, 3, 4, 6 for studying. Hold out exam 5 as the final test. For universe C, use practice exams 1, 2, 3, 6, 5 for studying. Hold out exam 4 as the final test. For universe D, use practice exams 1, 2, 6, 4, 5 for studying. Hold out exam 3 as the final test. For universe E, use practice exams 1, 6, 3, 4, 5 for studying. Hold out exam 2 as the final test. For universe F, use practice exams 6, 2, 3, 4, 5 for studying. Hold out exam 1 as the final test. At the end of the semester, take the average performance of each student’s hold out tests. This assumes that the learned knowledge of each student is independent between universes (e.g. student 1 in Universe A does not know what student 1 in Universe F has seen). This in turn means that the hold out exams are independent of knowledge of the practice exams, therefore, the average of the hold out exams gives an unbiased and lower variance estimate of how a given student has learned. This is a rather farcical scenario on so many levels, but unfortunately the final test as practice paradigm happens everyday, but the multi-verse validation approach is also used everyday– not necessarily in a classroom setting, but in analytical teams in virtually all fields. 7.2 Converting a farcical scenario into practical knowledge Suppose an economic analyst at a hedge fund is given bitcoin price data along with a few hundred financial market data series that coincide with bitcoin trading. She is asked to develop a statistical model to predict bitcoin prices using the market indicators, then forecast prices using a real-time input stream. In data science parlance, this is a supervised learning problem: given a target (dependent variable), a model trained to map a set of input features (independent variables or explanatory variables) onto a target. Supervised learning essentially is a method of teaching an algorithm to see certain patterns, but not in a way that is explicitly programmed by the modeler (e.g. heuristics, rules). Our enterprising, endeavoring analyst determines that a linear regression approach makes most sense (see (a) below) and devises a methodology for developing predictions. First, she acquires data, then cleans and explores it as we have illustrated in past chapters. When the data are ready for modeling, she experiments with combinations of a few variables to see which produces desirable results based on a series of diagnostics. More often than not, analysts will rely on subject matter intuition to construct models, then fixate on one or two measures of model fit (e.g. R-squared, F-test, p-values, etc.) and judge model performance based on those diagnostics alone. Although prediction is not the same as characterization, models are built with the intent to tell a narrative that is rooted in a parsomonious set of variables (e.g. increased social hype increases bitcoin prices). Without realizing it, this equates to the assumption that a phenomenon can be characterized by a few specific features of a data set. A set of candidate models are selected based on some quantitative or qualitative criteria. As a final check, all candidate models are “backcasted”, which is a way compare the actual bitcoin prices \\(y\\) and the predicted prices \\(\\hat{y}\\) using the data that was used to build the model. A function is a function, so what could go wrong? Not to be overly dramatic, many, many things could go wrong. Comparison of two modeling workflows: (a) model training without validation, (b) model training with k-folds cross validation. The above approach, while seemingly logical, follows the final exam as practice materials paradigm. While diagnostics and backcasts are conducted, the analyst is unable to truly determine if one model’s formulation is better than another as the models were all constructed on the final exam set. What are the consequences? Unlike the earlier scenario where students may enjoy grade inflation, failure to incorporate model validation to test assumptions may lead to any number of techical horror stories: The input data used to predict bitcoin prices are correlated with prices in early years of a sample, but not in later years. Failing to validate a model’s assumptions is a failure in checking if modeled relationships are stable. Unstable, unchecked relationships will invariably lead to erroneous predictions, which in turn lead to large losses at the cryptocurrency hedge fund. Needless to say, the economic analyst would likely be afforded opportunities to consider roles elsewhere. With so many variables available, the analyst uses a kitchen sink approach by dumping all available variables into the model. While there are plenty of techniques to efficiently surface the most influential features within high-dimensional data sets (e.g. k is large), ordinary least squares is not one of those techniques. As a result, OLS will attempt to map correlated, but spurious features with bitcoin prices, which– for all intents and purposes – is developing a model on noise. This is known as model overfitting – erroneous relationships sensitive to irrelevant movements in the inputs and investment become highly volatile. This is known as model overfitting. While the analyst will not likely be provided opportunities to consider roles elsewhere, her work will likely be scrutinized a bit closer. The observed paradigm is not mature in that all types of movement in the data have not been observed. For example, conditions for price growth and decline should be observed in the sample – not only growth. This ensures that conditions for growth and decline are learned by the model. After all, what goes up must come down, and if that is not reflected in the data, then a model is not needed to predict what will eventually happen. Inaccurate model assumptions may give way to overfitting in which the patterns that are learned reflect noise rather than true underlying patterns. Strong model biases may also give rise to underfitting in which models are not sensitive to the patterns in the data, leading to models that do not provide much insight. In both cases, the result are high variance out-of-sample predictions. Let’s take a look at the underlying Bitcoin prices. Below are three examples of model conditions: Underfitting. Starting from the left, an overly simplistic explanation might get the gist of bitcoin prices. Indeed, as of September of 2017, the prices have been increasing, but that is insufficient information to be used for anything other than long term investment. This idea of underfit is when an empirical model does not capture observed relationships. The variance of predictions in the training set is likely to be high for missing peaks and troughs. That being said, underfit, high bias models tend to tell a digestible story, but have little predictive value. Overfitting. The middle graph shows a low bias model, which is often a far more complex model with more features and polynomials to improve the model fit. At first glance, it captures much of the variability in the point-spread. While low bias models tend to have low bias in the training step, their predictions in out-of-sample testing tend to have high variance as the model was calibrated to noise and is thus extra sensitive to noise. Goldilocks. The last graph shows a good balance between the two extremes: Just enough complexity to capture the curvature, but not so much that the model reacts to noise. Figure 7.1: Bitcoin prices and model fits. Data obtained from coindesk. Model validation is crucial to test proposed models in order to find the right balance. Among the many designs are the following: Train-Test. A simple design involves a train-test where a training set of 70% of the records is used to built models and a testing set of 30% is used as a final exam. While this is a step in the right direction, the testing set will eventually be biased. As the analyst builds and tests models, she will predict results for the hold-out 30%, find model deficiencies, then iterate. In theory, the 30% should be used to validate once one is confident with model performance, but that requires a degree of discipline that is not easy enforceable. Eventually, the model accuracies of the 30% bias towards to that of the 70%, resembling a less severe version of the final exam as a practice materials problem. Train-Validate-Test. An improvement to the train-test design is to add a validation step: train-validate-test in which 70% of the sample is used for model development, 15% of the sample is used for validation and tuning of the model, and 15% is used as a final hold out. While an improvement, the problem with both train-test and train-validate-test designs are that large proportions of the data sample are lost to testing and the variance on the testing accuracies are large. K-Folds Cross Validation. A better, but computationally more costly approach is k-folds cross validation (see below). This is equivalent to the multi-verse scenario: Each \\(k\\) is simply a set of observations that are assigned to a group known as a fold, which is a convenient way of assigning and keeping track of hold out samples. For a proposed set of models, whether model specification or different types of algorithms, the approach is iterative: to start, for folds \\(1\\) through \\(k-1\\), we develop our models, then predict on the hold out sample \\(k\\), record the scores and diagnostics. Then, we repeat this process until each of the k-folds is used as a hold out sample. All model performance results are averaged across all hold out samples to derive a more accurate estimate of model performance. K-Folds Cross-Validation Workflow Model validation can become a complex topic. This section is meant to plant the seed. In subsequent chapters, we will put model selection into action. "],
["continuous-problems-how-much-should-we-expect.html", "Chapter 8 Continuous Problems: How much should we expect…? 8.1 An Ordinary Case of Regression 8.2 More Than Plain Vanilla Regression 8.3 K-Nearest Neighbors 8.4 DIY 8.5 Exercises", " Chapter 8 Continuous Problems: How much should we expect…? 8.1 An Ordinary Case of Regression Every year, cities and states across the United States publish measures on the performance and effectiveness of operations and policies. Performance management practitioners typically would like to know the direction and magnitude, as illustrated by a linear trend line. Is crime up? How are medical emergency response times? Are we still on budget? Which voting blocks are drifting? For example, the monthly number of highway toll transactions in the State of Maryland is plotted over time from 2012 to early 2016. The amount is growing with a degree of seasonality. But to concisely summarize the prevailing direction of toll transactions, we can use a trend line. That trend line is an elegant solution that shows the shape and direction of a linear relationship, taking into account all values of the vertical and horizontal axes to find a line that weaves through and divides point in a symmetric fashion. ## toll_transactions has been loaded into memory. ## Dimensions: n = 368, k = 7 Figure 8.1: Total Toll Transactions, Maryland 2012 to 2016 This trend line can be simply described in using the following formula: \\[\\text{transactions} = 10.501 + 0.036 \\times \\text{months}\\] and every point plays a role. We can infer that the trend grows at approximately 36,000 transactions per month. Using the observed response \\(y\\) and the independent variable \\(x\\), calculating the intercept and slope is a fairly simple task: \\[\\text{slope = } \\hat{w_1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\] and \\[\\text{intercept = } \\hat{w_0} = \\bar{y}-\\hat{w_1}\\bar{x}\\] In a bivariate case such as this one, it’s easy to see the interplay. In the slope, the covariance of \\(X\\) and \\(Y\\) (\\(\\sum_{i=1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}\\)) is tempered by the variance of \\(x\\) (\\(\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\)). If the covariance is greater than the variance, then the absolute value of the slope will be greater than one. The direction of the slope (positive or negative) is determined by the y7 between \\(x\\) and \\(y\\) alone. Trend lines are one of many uses of a class of supervised learning algorithms called regression – in this case Ordinary Least Squares or Regression to the mean – a method that is specifically formulated for continuous variables. There are quite a few other types of regression, such as quantile regression, non-linear least squares, partial least squares among others – each of which handles continuous values with a different spin, some focused on estimating relationships at various points of an empirical distribution and others capture relationships that simply do not fit into a linear trend line. OLS regression is the quantitative workhorse in most fields. The technique is a statistical method that estimates unknown parameters by minimizing the sum of squared differences between the observed values and predicted values of the target variable. To better understand arguably the most commonly used supervised learning method, we can start by defining a regression formula: \\[y_i = w_0 x_{i,0} + w_{1} x_{i,1} + ... + w_{k} x_{i,k} + \\epsilon_{i}\\] where: \\(y_i\\) is the target variable or “observed response” \\(w_{k}\\) are coefficients associated with each \\(x_k\\). Each coefficient can be obtained by solving \\(\\hat{w} = (X&#39;X)^{-1}X&#39;Y\\). Note that \\(w\\) may be substituted with \\(\\beta\\) in some cases. \\(x_{i,k}\\) are input or independent variables subscript \\(i\\) indicates the index of individual observations in the data set \\(k\\) is an index of position of a variable in a matrix of \\(x\\) \\(\\epsilon_{i}\\) is an error term that is assumed to have a normal distribution of \\(\\mu = 0\\) and constant variance \\(\\sigma^2\\) Note that \\(x_{i,0} = 1\\), thus \\(w_0\\) is often times represented on its own. For parsimony, this formula can be rewritten in matrix notation as follows: \\[y = XW + \\epsilon\\] such that \\(y\\) is a vector of dimensions \\(n \\times 1\\), \\(X\\) is a matrix with dimensions \\(n \\times k\\) regressors, and \\(W\\) is a vector of coefficients of length \\(k\\), containing an intercept and a coefficient corresponding to each of \\(k\\) features. \\(W\\) is oftem times represented with \\(\\beta\\). \\[TSS = argmin (\\sum^n_{i=1}(y_i - \\sum^k_{j=1} x_{ij}w_j)^2\\] Given this formula, the objective is to minimize the Total Sum of Squares (also known as Sum of Squared Errors): \\[TSS = \\sum_{i=0}^{n}{(y_i - \\hat{y}_i)^2} \\] Which can also be written by substituting the prediction formula that yields \\(\\hat{y}\\): \\[TSS = \\sum^n_{i=1}(y_i - \\sum^k_{j=1} x_{ij}w_j)^2\\] where \\(k\\) is the total number of variables and \\(j \\in k\\). More commonly, TSS is better contextualized as the Mean Squared Error \\[MSE = \\frac{1}{n}\\sum_{i}^{n}{(y_i - \\hat{y_i})^2}\\]. The SSE and MSE are measures of uncertainty relative to the observed response. Minimization of least squares can be achieved through a a method known as gradient descent. 8.1.1 Interpretation There are a number of attributes of a linear squares regression model that are examined, namely the specification, coefficients, R-squared, and error. Specification and coefficients The specification is the formulation of a model, comprised of the target feature and the input features. It is often times represented loosely as: \\(Y = f(x_1, x_2,...,x_n)\\) Each of the \\(w\\) values is a coefficient that describes the marginal relationship between each \\(x\\) and the target \\(y\\). Recall the toll road example. \\[\\text{transactions} = 10.501 + 0.036 \\times \\text{months}\\] The \\(w_0\\) (or y-intercept) is equal to 10.501, meaning when months = 0, the expected traffic was 10.5 million (without accounting for seasonality). The \\(w_1\\) or coefficient for month is 36,000, meaning that for each additional month forward, we would expect an average 36,000 traffic increase. As we will see in the the following elaborated example, these coefficients will offer robust insights into the inner quantitative workings of the modeled relationship. As we will see in the practical example at the end of this section, there are countless ways of representing relationships. R-squared R-squared or \\(R^2\\) is a measure of the proportion of variance of the target variable that can be explained by a estimated regression equation. A few key bits of information are required to calculate the \\(R^2\\), namely: \\(\\bar{y}\\): the sample mean of \\(y\\); \\(\\hat{y_i}\\): the predicted value of \\(y\\) for each observation \\(i\\) as produced by the regression equation; and \\(y_i\\): the observed value of \\(y\\) for each observation \\(i\\). Putting these values together is fairly simple: Total Sum of Squares or TSS is the variance of \\(y\\): \\[\\text{TSS} = \\sigma^2(y) = \\sum_{i=1}^{n}(y_i - \\hat{y})^2\\] Sum of Squared Errors is the squared difference between each observed value of \\(y\\) and its predicted value \\(\\hat{y_i}\\): \\[\\text{SSE} = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\] Regression Sum of Squares or RSS is the difference between each predicted value \\(\\hat{y_i}\\) and the sample mean \\(\\bar{y}\\). Together, \\(R^2 = 1 - \\frac{SSE}{TSS}\\). As TSS will always be the largest value, \\(R^2\\) will always be bound between 0 and 1 where a value of \\(R^2 = 0\\) indicates a regression line in which \\(x\\) does not account for variation in the target whereas \\(R^2 = 1\\) indicates a perfect regression model where \\(x\\) accounts for all variation in \\(y\\). Error Error can be measured in a number of ways in the OLS context. The most common is the Root Mean Square Error (RMSE), which is essentially the standard error between predictions \\(\\hat{y_i}\\) and \\(y_i\\). RMSE is defined as \\(\\text{RMSE} = \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n(\\hat{y_i}-y_i)^2}{n}}\\). Note that RMSE is interpreted in terms of levels of \\(y\\), which may not necessarily facilitate easy communication of model accuracy. For example, a \\(\\text{RMSE = 0.05}\\) in one model might appear to be small relative to a \\(RMSE = 164\\). However, when contextualized, the former may be a larger proportion of its respective sample mean than the latter’s, indicating a less accurate fit. There are other methods of representing error. In time series forecasts, Mean Absolute Percentage Error (MAPE) is used to contextualize prediction accuracy as a percentage of \\(y\\). This measure is defined as \\(\\text{MAPE} = \\frac{100}{n}\\sum_{i=1}^n|\\frac{\\hat{y_i}-y_i}{y_i}|\\) and can be easily interpretted and communicated. For example, MAPE = 1.1% from one model can be compared against the MAPE = 13.5% of another model. 8.1.2 Assumptions matter All models have properties and assumptions that guide their use, but also help determine which use cases are appropriate. OLS is bound by four basic assumptions that, in general, jive with how most people like to view math problems: in a linear, additive frame of mind where each factor has a distinct and perceivably consistent influence on the outcome. In other words: Giovanni: How do you think those entrepreneurs made all that money (dependent variable)? Giorgio: Well, I think it may be due to …[independent variables go here]… First, OLS assumes that the target feature’s relationship with input features is linear and additive. The equation \\(y_i = w_0 x_{i,0} + w_{1} x_{i,1} + ... + w_{k} x_{i,k} + \\epsilon_{i}\\) is another way of saying that each of the factors can be weighed and summed up to equate to the outcome. If we think about the above conversation, it is assumed that earnings can be described by a set of independent factors to which a weight can be definitively assigned to describe its role in earnings. This weighted average of factors is then added together. While this seems reasonable, it also is a strong assumption that two or more quantities have a linear relationship. If there is not a strong correlationship between the target \\(y\\) and inputs \\(X\\), apply transformations such as the natural logarithm (log()). In time series models (data captured over equal intervals), data may be non-stationary (e.g. unstable) and may benefit from differencing (e.g. \\(\\Delta(y_t) = y_t -y_{t-1}\\) – try the diff() function). If worse comes to worse, a non-linear or tree-based technique, as will be described later in the book, may be better at capturing the patterns. Figure 8.2: Ways of transforming data to [hopefully] make them appropriate for linear models. Second, all observations are randomly sampled such that the features \\(X\\) for a given observation \\(i = 1\\) are not dependent on observation \\(i = 2\\). This assumption has many implications across observations and across features. To be able to estimate the specification below, Giovanni and Giorgio would need to first define a universe of entrepreneurs and identify a target that is dependent on input features, but the inputs are not dependent on the target. From that universe of entrepreneurs, obtain a list of people, then randomly select a subset for data collection. \\[\\text{Y(Earnings) = f(Education, Age, Socioeconomic Status, Industry, ...)}\\] The survey needs to frame questions in a way that can roll into input features that are not correlated with one another. For example, education should not perfectly correlate with age, but should correlate with earnings. This matters as the linear specification needs to be able to attribute a part of earnings to each input feature, but has trouble doing in the presence of multicollinearity or when two or more variables are highly correlated. As the number of input features \\(k\\) grows, the chance of multicollinearity grows. The implications The regression equation has an error term \\(\\epsilon\\) that captures everything else that is not predicted by a model. This error term carries tremendous information that can be used to make a model more reliable. A number of assumptions are also thus built upon errors: The conditional mean should be zero \\(E(\\epsilon|X) = 0\\), meaning that the error should not have any relationship to the input features \\(X\\). Errors should have constant variance or be homoscedastic. Given the variance of the error \\(Var(\\epsilon|X) = \\sigma^2\\), the spread should be constant (middle). Heterscedastic errors may be an indication that the underlying relationship between the target and input features may not be consistent or there are systematic errors with how the data was collected, both of which have an effect on the reliability of predictions. For reliability of estimates, heteroscedastic errors would lead to inaccurate confidence intervals. Figure 8.3: Homoscedasticity vs. Heterscedasticity Errors should be autocorrelated, or correlated amongst each other. Under the ideal scenario, the covariance should be \\(cov(\\epsilon_i\\epsilon_j|x) = 0\\) where i and j are index values of observations and are not equal to one another. Any data that involves a trend and cyclicality or seasonality will likely suffer from autocorrelation, thus the weather yesterday may be correlated with weather today. Generally, autocorrelation can appear in any context that is spatial (e.g. German cities are likely to have more autocorrelation within country than with outside the country with the US) or more commonly temporal (e.g. yesterday is associated with today). The errors should be normally distributed. This assumption is not necessary when evaluating the validity of a model, but provides a clue as to how well the model operates. 8.1.3 In the code To understand how a function works means to build it from scratch. Below, we illustrate how OLS is constructed using time series data about online searches relating to _human rights&quot; as found on Google Trends, then compare our model coefficients against the pre-built lm() function. Starting off, be sure to install the gtrendsR, then execute a request for “Human rights” for the period of January 2011 through January 2017. Note that the number of hits is an index value to the maximum number of searches for the requested topic during the period of interest. #Load library library(gtrendsR) library(ggplot2) #Download human rights for out = gtrends(c(&quot;Human rights&quot;), gprop = &quot;web&quot;, time = &quot;2011-01-01 2017-01-01&quot;)[[1]] #Monthly line plot ggplot(out, aes(date, hits)) + geom_line() Figure 8.4: Google trends monthly time series for Human Rights searches, Jan. 2011 through Jan. 2017 The resulting data series exhibits some degree of regularity – perhaps a combination of monthly seasonality and a slight downward trend. A specification can be represented as: \\[\\text{Hits Index} = w_0 + w_1(\\text{date index}) + w_2(January) + ... + w_{13}(November) + \\epsilon\\] where the date index is a sequential identifier for each month and each month is a represented as a dummy, leaving one month in reserve to avoid collinearity traps. The goal of the regression model is to estimate a weight for each input feature and the intercept. Thus, a matrix X needs to include a value for each input \\(x_k\\) including a placeholder for the intercept. Using model.matrix(), we can convert a vector of months into a matrix of dummies with the first column representing the y-intercept. #Monthly dummies month = format(out$date, &quot;%m&quot;) dummies = model.matrix(~ month) colnames(dummies) ## [1] &quot;(Intercept)&quot; &quot;month01&quot; &quot;month02&quot; &quot;month03&quot; &quot;month04&quot; ## [6] &quot;month05&quot; &quot;month06&quot; &quot;month07&quot; &quot;month08&quot; &quot;month09&quot; ## [11] &quot;month10&quot; &quot;month11&quot; &quot;month12&quot; Next, we create a simple sequential date index and add it to the dummies matrix to create X. #Date Index date.index &lt;- 1:nrow(out) #Create matrix X &lt;- cbind(dummies[, -13], date.index) head(X) ## (Intercept) month01 month02 month03 month04 month05 month06 month07 ## 1 1 1 0 0 0 0 0 0 ## 2 1 0 1 0 0 0 0 0 ## 3 1 0 0 1 0 0 0 0 ## 4 1 0 0 0 1 0 0 0 ## 5 1 0 0 0 0 1 0 0 ## 6 1 0 0 0 0 0 1 0 ## month08 month09 month10 month11 date.index ## 1 0 0 0 0 1 ## 2 0 0 0 0 2 ## 3 0 0 0 0 3 ## 4 0 0 0 0 4 ## 5 0 0 0 0 5 ## 6 0 0 0 0 6 Lastly, extract the hits feature from the trends data as a standalone vector \\(y\\). y &lt;- out$hits With the vector \\(y\\) and matrix \\(X\\), we can now estimate each coefficient \\(w_i\\) by solving for the following equation: \\(W = (X^TX)^{-1}X^Ty\\). As R is well-adapted to statisticians’ needs, the %*% operator is an easy way to conduct matrix multiplication between two matrices. The solve() function is used to solve for a system of equations. In this case, solve() inverts the result of \\(X^TX\\). a &lt;- t(X) %*% X w &lt;- solve(a) %*% t(X) %*% y Likewise, we can estimate the same regression using the lm() function and extract the coefficients to illustrate that the calculation steps yield identifical results. While the lm() function makes the process far simpler and is designed to facilitate analysis with ease, there is something to be said for being able to engineer the underlying steps as it may come in handy when building new classes of algorithms. #Run lm() lm.obj &lt;- lm(y ~ X[,-1]) #Consolidate and compare model coefficients comparison &lt;- data.frame(`From Scratch` = w, `lm Function` = coef(lm.obj)) #Print print(comparison) Table 8.1: Comparison of model built from scratch versus pre-built function From.Scratch lm.Function (Intercept) 75.4206349 75.4206349 month01 0.6562736 0.6562736 month02 10.0506425 10.0506425 month03 17.3622449 17.3622449 month04 11.3405140 11.3405140 month05 14.6521164 14.6521164 month06 -4.5362812 -4.5362812 month07 -14.7246788 -14.7246788 month08 -12.7464097 -12.7464097 month09 1.2318594 1.2318594 month10 8.8767952 8.8767952 month11 10.3550642 10.3550642 date.index -0.1449358 -0.1449358 For more in-depth discussion on the derivation of linear models, refer to Chapter 3 of Introduction to Statistical Learning. 8.2 More Than Plain Vanilla Regression 8.2.1 The Ridge and the LASSO More often than not, the qualities that bound a data set are not ideal. Imagine a scenario in which \\(n = 1000\\), but the number of features \\(k = 5000\\). In an increasingly data-rich world, this is becoming more common. This leads to a number of methodological problems. Generally, analysts and policy makers want a parsimonious explanation as fewer driving factors are easier to interpret. An analyst may be tempted to use his or her intuition to guide her variable choices using tests such as AIC, BIC, F-Test among others. However, when \\(k\\) is large, a manual strategy may not necessarily be scalable and the strongest predictors may simply be overlooked, whether due to overriding theoretical assumptions (e.g. economic theory, policy theories) or inability to screen and test all variable combinations. From a purely statistical perspective, least squares is dependent on being able to solve for \\((X&#39;X)^{-1}\\) – or inverting the matrix in order to calculate each of the coefficients \\(w\\), which is not possible if the matrix were singular (not invertible). Enter regularized regression methods. As we know with linear regression, the goal is to estimate \\(\\w\\) by minimizing the Total Sum of Squares (TSS): \\[\\hat{w} = (X&#39;X)^{-1}X&#39;Y\\] where each \\(X\\) and \\(Y\\) is standardized (mean 0 with unit variance). If we assume that there is a limited amount of TSS, then we can force linear regression to make trade offs between coefficients by introducing a bias term – a constraint \\(\\lambda I_k\\) can be added to the diagonals of \\(X&#39;X\\): \\[\\hat{w} = (X&#39;X + \\lambda I_k)^{-1}X&#39;Y\\]. \\(\\lambda\\) is a tuning parameter – a value that cannot be solved for and must be uncovered through cross-validation. Depending on how much bias is added into the model determines how much “wiggle room” coefficients have to move. Expanding upon the typical TSS, \\(\\lambda\\) is tacked onto the end of the formula as a scalar of the sum of squared coefficients (an L2-norm): \\[TSS_{penalized} = \\sum^n_{i=1}(y_i - \\sum^k_{j=1} x_{ij}w_j)^2 + \\lambda \\sum^k_{j=1}|w_j|^2\\] The second part of the penalized TSS can be viewed as a constraint. Given the standard TSS \\(\\sum^n_{i=1}(y_i - \\sum^k_{j=1} x_{ij}w_j)^2\\), we place a constraint such that \\[\\sum^k_{j=1}|w_j|^2 &lt; c \\] where \\(c &gt; 0\\). Taken together, the interpretation of \\(\\lambda\\) is that it regulates the size of coefficients \\(w\\): As the size \\(\\lambda\\) is increased, the magnitude of coefficients are reduced. The value of \\(\\lambda\\) is determined through a grid search in which values of \\(\\lambda\\) are tested at equal intervals to identify the value that minimizes some error measure such as MSE. This specific formulation of a regularization is known as a ridge regression and helps to bound coefficients to a “reasonable” range, but does not zero out coefficient values. In order to surface the most “influential” features, we may rely on a close cousin of ridge regression known as Least Absolute Shrinkage and Selection Operator or LASSO, which relies on an L1-norm: \\[\\sum^k_{j=1}|w_j| &lt; c\\]. By swapping a L2-quadratic constraint with a L1, least squares will behave differently. To understand the effects on how least squares model behave under regularization, we can use a geometric interpretation. Constraints force the possible OLS esimates to fall along the edge of a constraint region. In the diagram below, the blue are the L1/L2 norm constraints, the concentric ellipses are various possibilities of the Residual Sum of Squares, and two coefficients are considered. The tangent at which a RSS ellipse and a constraint region meets jointly determines the magnitude of a coefficient – there is a trade off imposed due to the constraints. In the case of Ridge regression, the L2-norm constraint region is circular, meaning that a large coefficient \\(w_1\\) for a feature \\(x_1\\) is possible when minimizing the magnitude of coefficient \\(w_2\\). In LASSO regression, the L1-norm results in a constraint region with “corners”. If the RSS ellipse lands on acorner, a coefficient \\(w_2\\) can be forced to exactly zero while allowing \\(w1\\) take a non-zero value. The significance of this is far reaching: LASSO regression can conduct variable selection. Figure 8.5: Ridge and LASSO constraint regions and their effect on OLS estimates. There are hurdles in using these methods, however. Uncertainty in the form of standard errors is a mainstay of statistical methods. While standard errors can be derived for Ridge regression as the estimator is linear, the same property is not shared by LASSO due to the L1-constraint. One would expect that collinearity should be a concern, in particular the number of input features exceeds the number of observations. As it turns out, Ridge regression is robust to collinearity, but LASSO is not. 8.2.2 Why regularized methods matter In situations where priors are not known and the features are too numerous to individually test, regularized regression methods facilitates analysis and prediction at scale. Perhaps this class of techniques goes against time-honored scientific traditions of starting from a well-defined hypothesis, then systematically proving and disproving arguments for and against the hypothesis. But alternatively, regularized methods may take on the roles of as both an exploratory method and a predictive method. LASSO regression, in particular, can serve as a methodological gut check and surface features that contain predictive value. Cases where regularization is commonly used: If users of a service provide ratings and written comments that are not in a structured form, the words can be treated as n-gram features on which the ratings can be regressed upon. Using a LASSO, the most important phrases will be surface Geneticists often use DNA microarrays to capture thousands of genes and how they are expressed. LASSO can be used to surface which genes are most associated with a given gene expression. In marketing, a common task is to target advertisements and product offers to customers who wil buy the product. With the plethora of customer-level data from demographic characteristics to purchasing patterns, a data analyst may have hundreds if not thousands of measures on a given person and thus may be challenged with high dimensionality. Regularized regression can be useful in surfacing important factors and predict a customer’s propensity to buy a product. For a step-by-step walkthrough of regularized methods, see What do I do when there are too many features? in the DIY section of this chapter. 8.3 K-Nearest Neighbors 8.3.1 Formulation Continuous values can also be handed using non-parametric means. K-nearest neighbors (KNN) is a pattern recognition algorithm that is based on a simple idea: observations that are more similar will likely also be located in the same neighborhood. Given a class label \\(y\\) associated with input features \\(x\\), a given record \\(i\\) in a dataset can be related to all other records using Euclidean distances in terms of \\(x\\): \\[ \\text{distance} = \\sqrt{\\sum(x_{ij} - x_{0j})^{2} }\\] where \\(j\\) is an index of features in \\(x\\) and \\(i\\) is an index of records (observations). For each \\(i\\), a neighborhood of taking the \\(k\\) records with the shortest distance to that point \\(i\\). From that neighborhood, the value of \\(y\\) can be approximated. Given a discrete target variables, \\(y_i\\) is determined using a procedure called majority voting where the most prevalent value in the neighborhood around \\(i\\) is assigned. For example, the ten closests points relative to a given point \\(i\\) are provided: Choosing a value of \\(k = 4\\) would mean that the subsample is made up of three a’s and one b. As a makes up the majority, we can approximate \\(y_i\\) as a, assuming points that are closer together are more related. For continuous variables, the mean of neighboring records is used to approximate \\(y_i\\). How does one implement this exactly? To show this process, we will write some pseudocode. It’s an informal language to articulate and plan the steps of an algorithm or program, principally using words and text as opposed to formulae. There are different styles of pseudocode, but the general rules are simple: indentation is used to denote a dependency (e.g. control structures). For all techniques, we will provide pseudocode, starting with kNN: Pseudocode kNN( k, set, y, x){ Pre-Process (optional): &gt; Transform or standardize all input features Loop through each `item` in `set`{ &gt; Calculate vector of distances in terms of x from `item` to all other items in `set` &gt; Rank distance in ascending order if target `y` is continuous: &gt; Calculate mean of `y` for items ranked 1 through k else if target is discrete: &gt; Calculate share of each discrete level for items ranked 1 through k &gt; Use majority voting to derive expected value } } The procedure described above yields the results for just one value of \\(k\\). However, kNNs, like many other algorithms, are an iterative procedure, requiring tuning of hyperparameters – or values that are starting and guiding assumptions of a model. In the case of kNNs, \\(k\\) is a hyperparameter and we do not precisely know the best value of \\(k\\). Often times, tuning of hyperparameters involves a grid search, a process whereby a range of possible hyperparameters is determined and the algorithm is tested at equal intervals from the minimum to maximum of that tuning range. To illustrate this, a two-dimensional dataset with a target \\(y\\) that takes of values \\(0\\) and \\(1\\) has been plotted below. Graph (1) plots the points, color-coded by their labels. Graph (2), (3), and (4) show the results of a grid search along intervals of a \\(log_{10}\\) scale, where the background is color-coded as the predicted label for the corresponding value of \\(k\\). In addition to \\(k\\), two measures are provided above each graph to help contextualize predictions: the True Positive Rate or \\(TPR\\) and the True Negative Rate or \\(TNR\\). The \\(TPR\\) is defined as #\\(TPR = \\frac{\\text{Number of values that were correctly predicted}}{\\text{Number of actual cases values}}\\)#. The \\(TNR\\) is similarly defined as #\\(TNR = \\frac{\\text{Number negative values that were correctly predicted}}{\\text{Number of actual negative values}}\\)#. Both are measures bound between 0 and 1, where higher values indicate a higher degree of accuracy. A high \\(TPR\\) and low \\(TNR\\) indicates that the algorithm is ineffective in distinguishing between positive and negative cases. The same is true with a low \\(TPR\\) and high \\(TNR\\). This is exactly the case in Graph (4) where all points are classified as \\(Y = 1\\), which is empirically characterized by \\(TNR = 0.02\\) and \\(TPR = 1\\). Figure 8.6: Comparison of prediction accuracies for various values of k. 8.3.2 Which K is the right K? The accuracy of a KNN model is principally dependent on finding the right value of \\(k\\) directly determines what enters the calculation used to predict the target variable. Thus, to optimize for accuracy, try multiple values of \\(k\\) and compare the resulting accuracy values. It is helpful to first see that when \\(k = n\\), kNNs are simply the sample statistic (e.g. mean or mode) for the whole dataset. Below, the True Positive Rate (TPR, blue) and True Negative Rate (TNR, green) have been plotted for values of \\(k\\) from 1 to \\(n\\). The objective is to ensure that there is a balance between TPR and TNR such that predictions are accurate. Where \\(k &gt; 20\\), the TPR is near perfect. For values of \\(k &lt; 10\\), TPR and TNR are more balanced, thereby yielding more reliable and accurate results. Figure 8.7: True Positive Rate (TPR = blue) and True Negative Rate (TNR = green) performance for varying values of k There are other factors that influence the selection of \\(k\\): Scale. kNNs are strongly influenced by the scale and unit of values of \\(x\\) as ranks are dependent on straight Euclidean distances. For example, if a dataset contained measurements of age in years and wealth in dollars, the units will over emphasize income as the range varies from 0 to billions whereas age is on a range of 0 to 100+. To ensure equal weights, it is common to transform variables into standardized scales such as: Range scaled or \\[\\frac{x - \\min(x)}{\\max(x)-\\min(x)} \\] yields scaled units between 0 and 1, where 1 is the maximum value Mean-centered or \\[ \\frac{x - \\mu}{\\sigma}\\] yield units that are in terms of standard deviations Grids. Similar to the scale issue, KNNs are particularly effective in data that are distributed on a grid – measurements along a continuous scale at equal incremenets, but may be a poor choice when the data are mixed data formats such as integers and binary. Symmetry. It’s key to remember that neighbors around each point will not likely be uniformly distributed. While kNN does not have any probabilistic assumptions, the position and distance of neighboring points may have a skewing effect. 8.3.3 Usage KNNs are efficient and effective under certain conditions: KNNs can handle target values that are either discrete or continuous, making the approach relatively flexible. However, best performance is achieved when the input features should are in the same scale (e.g. color values in a grid). They are best used when there are relatively few features as distances to neighbors need to be calculated for each and every record and need to be optimized by searching for the value of \\(k\\) that optimizes for accuracy. In cases where data is randomly or uniformly distributed in fewer dimensions, a trained KNN is an effective solution to filling gaps in data, especially in spatial data. KNNs are not interpretable as it is a nonparametric approach – it does not produce results that have a causal relationship or illustrate. Furthermore, kNNs are not well-equipped to handle missing values. For a step-by-step walkthrough of regularized methods, see What’s a good way to fill-in missing data? in the DIY section. 8.4 DIY 8.4.1 What’s a good way to fill-in missing data? In practice in R, KNNs can be trained using the knn() function in the class library. However, this function is best suited for discrete target variables. To illustrate KNN regressions, we will write a function from scratch and illustrate using remote sensed data. Remote sensing is data obtained through scanning the Earth from aircrafts or satellites. Remote sensed earth observations yield information about weather, oceans, atmospheric composition, human development among other things – all are fundamental for understanding the environment. As of Jan 2017, the National Aeronautics and Atmospheric Administration (NASA) maintains two low-earth orbiting (LEO) satellites named Terra and Aqua, each of which takes images of the Earth using the Moderate Resolution Imaging Spectroradiometer (MODIS) instrument. Among the many practical scientific applications of MODIS imagery is the ability to sense vegetation growth patterns using the Normalized Difference Vegetation Index (NDVI) – a measure ranging from -1 to +1 that indicates that amount of live green on the Earth’s surface. Imagery data is a \\(n \\times m\\) gridded matrix where each cell represents the NDVI value for a given latitude-longitude pair. NASA’s Goddard Space Flight Center (GSFC) publishes monthly MODIS NDVI composites. For ease of use, the data has been reprocessed such that data are represented as three columns: latitude, longitude, and NDVI. In this example, we randomly select a proportion of the data (~30%), then use KNNs to interpolate the remaining 70% to see how close we can get to replicating the original dataset. In application, scientific data that is collected in situ on the Earth’s surface may take on a similar format – represembling randomly selected points that can be used to generalize the measures on a grid, even where measures were not taken. This process of interpolation and gridding of point data is the basis for inferring natural and manmind phenomena beyond where data was sampled, whether relating to the atmosphee, environment, infrastructure, among other domains. To start, we’ll use the digIt() library to import the NASA extract. library(digIt) df &lt;- digIt(&quot;ndvi&quot;) To view the data, we can use the geom_raster() option in the ggplot2 library. Notice the color gradations between arrid and lush areas of vegetation. library(ggplot2) ggplot(df, aes(x=lon, y=lat)) + geom_raster(aes(fill = ndvi)) + ggtitle(&quot;NDVI: October 2016&quot;) + scale_fill_gradientn(limits = c(-1,1), colours = rev(terrain.colors(10))) Figure 8.8: Rendering of NDVI for October 2016 The NDVI data does not provide values on water. As can be seen below, cells that do not contain data are represented as 99999 and are otherwise values between -1 and +1. ## lat lon ndvi ## 1 89.875 -179.875 99999.000 ## 2 89.625 -179.875 99999.000 ## 3 89.375 -179.875 99999.000 ## 74 71.625 -179.875 0.105 ## 75 71.375 -179.875 0.447 ## 76 71.125 -179.875 0.376 For this example, we will focus on an area in the Western US and extract only a 30% sample. #Subset image to Western US near the Rocky Mountains us.west &lt;- df[df$lat &lt; 45 &amp; df$lat &gt; 35 &amp; df$lon &gt; -119 &amp; df$lon &lt; -107,] #Randomly selection a 30% sample set.seed(32) sampled &lt;- us.west[runif(nrow(us.west)) &lt; 0.3 &amp; us.west$ndvi != 99999,] A KNN algorithm is fairly simple to build when the scoring or voting function is a simple mean. All that is required is to write a series of a loops to calculate the nearest neighbors for any value of \\(k\\). The knnMean function should take a training set (input features - x.train and target - y.train), and a test set (input features - x.test). knnMean &lt;- function(x.train, y.train, x.test, k){ # # Calculates the mean of k-nearest neighbors # # Args: # x.train and y.train are the input features and target feature for the training set # x.test is the test set to be scored # k is the number of neighbors # # Return: # Vector of kNN-based means #Set vector of length of test set output &lt;- vector(length = nrow(x.test)) #Loop through each row of the test set for(i in 1:nrow(x.test)){ #extract coords for the ith row cent &lt;- x.test[i,] #Set vector length dist &lt;- vector(length = nrow(x.train)) #Calculate distance by looping through inputs for(j in 1:ncol(x.train)){ dist &lt;- dist + (x.train[, j] - cent[j])^2 } dist &lt;- sqrt(dist) #Calculate rank on ascending distance, sort by rank df &lt;- data.frame(id = 1:nrow(x.train),rank = rank(dist)) df &lt;- df[order(df$rank),] #Calculate mean of obs in positions 1:k, store as i-th value in output output[i] &lt;- mean(y.train[df[1:k,1]], na.rm=T) } return(output) } The hyperparameter \\(k\\) needs to be tuned. We thus also should write a function to find the optimal value of \\(k\\) that minimizes the loss function, which is the Root Mean Squared Error (\\(\\text{RMSE} = \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n(\\hat{y_i}-y_i)^2}{n}}\\).). knnOpt &lt;- function(x.train, y.train, x.test, y.test, max, step){ # # Conducts a grid search for KNN and returns RMSE for values of k # # Args: # x.train and y.train = the input features and target feature for the training set # x.test = the test set to be scored # max = the maximum number of neighbors to be considered # step = number of steps between 1 and max k # # Return: # data frame of RMSE by k #create log placehodler log &lt;- data.frame() for(i in seq(1, max, step)){ #Run KNN for value i yhat &lt;- knnMean(x.train, y.train, x.test, i) #Calculate RMSE rmse &lt;- round(sqrt(mean((yhat - y.test)^2, na.rm=T)), 3) #Add result to log log &lt;- rbind(log, data.frame(k = i, rmse = rmse)) } #sort log log &lt;- log[order(log$rmse),] #return log return(log) } Normally, the input features (e.g. latitude and longitude) should be normalized, but as the data are in the same coordinate system and scale, no additional manipulation is required. From the 30% sampled data, a training set is subsetted containing 70% of sampled records and the remaining is reserved for testing. #Set up data set.seed(123) rand &lt;- runif(nrow(sampled)) #training set xtrain &lt;- as.matrix(sampled[rand &lt; 0.7, c(1,2)]) ytrain &lt;- sampled[rand &lt; 0.7, 3] #test set xtest &lt;- as.matrix(sampled[rand &gt;= 0.7, c(1,2)]) ytest &lt;- sampled[rand &gt;= 0.7, 3] The algorithm can now be placed into testing, searching for the optimal value of \\(k\\) along at increments of \\(1\\) from \\(k = 1\\) to $ k = $. Based on the grid search, the optimal value is \\(k = 4\\). #opt logs &lt;- knnOpt(xtrain, ytrain, xtest, ytest, nrow(xtest), 1) #Plot results ggplot(logs, aes(x = k, y = rmse)) + geom_line() + geom_point() + ggtitle(&quot;RMSE vs. K-Nearest Neighbors&quot;) Figure 8.9: RMSE for various tested values of k With this value, we can now put this finding to the test by plotting the interpolated data as a raster. Using the ggplot library, we will produce six graphs to illustrate the tolerances of the methods: the original and sampled images as well as a sampling of rasters for various values of \\(k\\). #Original full &lt;- ggplot(us.west, aes(x=lon, y=lat)) + geom_raster(aes(fill = ndvi)) + ggtitle(&quot;Original NASA Tile&quot;) + scale_fill_gradientn(limits = c(-1,1), colours = rev(terrain.colors(10))) #30% sample sampled &lt;- ggplot(sampled, aes(x=lon, y=lat)) + geom_raster(aes(fill = ndvi)) + ggtitle(&quot;Sample: 30%&quot;) + scale_fill_gradientn(limits = c(-1,1), colours = rev(terrain.colors(10))) #Set new test set xtest &lt;- as.matrix(us.west[, c(1,2)]) #Test k for four different values for(k in c(1, 4, 10, 100)){ yhat &lt;- knnMean(xtrain,ytrain,xtest, k) pred &lt;- data.frame(xtest, ndvi = yhat) rmse &lt;- round(sqrt(mean((yhat - us.west$ndvi)^2, na.rm=T)), 3) g &lt;- ggplot(pred, aes(x=lon, y=lat)) + geom_raster(aes(fill = ndvi)) + ggtitle(paste0(&quot;kNN (k =&quot;,k,&quot;, RMSE = &quot;, rmse,&quot;)&quot;)) + scale_fill_gradientn(limits = c(-1,1), colours = rev(terrain.colors(10))) assign(paste0(&quot;k&quot;,k), g) } #Graphs plotted library(gridExtra) grid.arrange(full, sampled, k1, k4, k10, k100, ncol=2) Figure 8.10: Comparison of Predicted NDVI vs. Actual. 8.4.2 What do I do when there are too many features? Too many features? Regularization methods can do the trick. To illustrate regularization in action, we’ll use an example that is inspired by Google Flu Trends (read more: Detecting influenza epidemics using search engine query data). The research endeavor led by Google scientists sought to predict influenza in near real-time using search engine query data, but became controversial and widely criticized for its pure reliance on correlations. It is a pioneering and iconic nowcasting project and is an early example of how alternative data could be put into action. The basic idea of Google’s research is that some combination of search queries can be combined to predict the CDC’s influenza-like illness (ILI) estimates. In the below example, we do not profess to develop a predict the flu, but rather take advantage of the data made available through Google Trends. As one may imagine, Google’s search engine receives an unfathomable amount of queries. When combined with ILI data, we can easily yield a data set with \\(k &gt; n\\). To start, We’ll use the digIt library to pull the “flu” data. For simplicity, the example presented below contains \\(n = 85\\) and \\(k = 85\\) including target and date variables. Each \\(k\\) is a monthly-level index of the search volume for a specific search query, standardized to the maximum value observed in the period (100 = max). The queries include search terms such as “kleenex”, “cold remedy”, “cough”, and “cvspharmacy” among others. library(digIt) flu &lt;- digIt(&quot;flu&quot;) sample(colnames(flu), 20, replace = FALSE) ## [1] &quot;hits.cvspharmacy&quot; &quot;hits.shortnessofbreath&quot; ## [3] &quot;hits.cough&quot; &quot;hits.whoopingcough&quot; ## [5] &quot;hits.fluseason&quot; &quot;hits.flusymptoms&quot; ## [7] &quot;hits.medicalclinic&quot; &quot;hits.bluishskin&quot; ## [9] &quot;hits.911&quot; &quot;hits.typeafluvirus&quot; ## [11] &quot;hits.individualmandate&quot; &quot;hits.opioidaddiction&quot; ## [13] &quot;hits.runnynose&quot; &quot;hits.nationalinstituteofhealth&quot; ## [15] &quot;hits.sneeze&quot; &quot;hits.ambulance&quot; ## [17] &quot;hits.physician&quot; &quot;hits.outbreak&quot; ## [19] &quot;hits.medicalschool&quot; &quot;hits.fever&quot; The most correlated queries with ILI are conceptually related – all are focused on symptoms of the flu. #Calculate correlation matrix mat &lt;- cor(flu[,ncol(flu):2]) #Extract correlates with ILI top &lt;- data.frame(query = row.names(mat), rho = mat[,1]) top &lt;- top[order(-top$rho),] #Show top ten head(top, 10) query rho ili.rate 1.0000000 hits.flusymptoms 0.8574463 hits.tylenolcold 0.8424577 hits.influenza 0.8226647 hits.typeafluvirus 0.7920574 hits.flu 0.7727208 hits.nasalcongestion 0.7668754 hits.coldremedy 0.7639114 hits.pneumonia 0.7302167 hits.coughdrops 0.7225903 To apply a regularized regression, we will rely on the glmnet library that facilitates the application of an elastic net regression – which allows for a combination of a L1- and L2- constraint to be applied. Given the TSS \\(\\sum^n_{i=1}(y_i - \\sum^k_{j=1} x_{ij}w_j)^2\\), elastic net applies the following constraint: \\[(1-\\alpha)\\sum^k_{j=1}|w_j|_1 + \\alpha\\sum^k_{j=1}|w_j|^2_2&lt; c \\] which is comprised of an L1-norm (\\(|w_j|_1\\)) scaled by \\((1-\\alpha)\\) and a L2-norm scaled by \\(\\alpha\\). When \\(\\alpha = 1\\), then elastic net is effectively a LASSO regression. When \\(\\alpha = 0\\), then elastic net is a ridge regression. This constraint structure provides the flexibility to apply hybrid L1/L2 regularization by selecting values of \\(0 &lt; \\alpha &lt; 1\\). For more details on elastic net, read Zou and Hastie (2005). library(glmnet) The glmnet library requires all data to be in vector and matrix form – data frames are not allowed. Following proper prediction methodology, we will split the sample into a 75-25 train-test samples. #Set sample partition parameters train.prop &lt;- 0.75 train.max &lt;- round(nrow(flu) * train.prop) test.min &lt;- train.max + 1 #Train y.train &lt;- flu$ili.rate[1:train.max] x.train &lt;- flu[1:train.max,] x.train$date &lt;- x.train$ili.rate &lt;- NULL x.train &lt;- as.matrix(x.train) #Test y.test &lt;- flu$ili.rate[test.min:nrow(flu)] x.test &lt;- flu[test.min:nrow(flu), ] x.test$date &lt;- x.test$ili.rate &lt;- NULL x.test &lt;- as.matrix(x.test) The library provides a couple of functions to estimate a regularized regression, but the following uses cross validation to find the optimal value of \\(\\lambda\\) that minimizes error: cv.glmnet(x, y, alpha, type.measure, family, nfolds) where: x is a \\(n \\times k\\) matrix of input features; y is a \\(n \\times 1\\) target vector; alpha is the parameter to choose between LASSO and ridge (LASSO = 1); type.measure indicates the optimization measure, which can be mean squared error (“mse”) or mean absolute error (“mae”) for regression problems; family indicates the response type. This is assumed to be “gaussian” for quantitative targets. nfolds is the number of cross validation folds. Default is 10-folds CV. In this example, we will choose nfolds = 20 and a LASSO model (alpha = 1), then assign the model object ot mod.lasso. mod.lasso &lt;- cv.glmnet(x.train, y.train, nfolds = 20, alpha = 1, type.measure = &quot;mse&quot;) The mod.lasso model object contains a rich amount of diagnostics and model outputs. For example, the MSE can be analyzed along a search grid of \\(\\lambda\\) and identify the value of \\(\\lambda\\) that minimizes error (\\(-3 &lt; log(\\lambda) &lt; -2\\)). Notice that each grid point contains a standard deviation on the error as estimated through cross-validation. plot(mod.lasso) Figure 8.11: MSE vs. Log(Lambda) for ILI model where alpha = 1 Next, we can examine the coefficients that are non-zero for the value of \\(\\lambda\\) that yields the lowest error (“lambda.min”) as well as a set of arbitrarily selected lambda values. Notice that only a handful of the \\(k =83\\) input features remain, all of which are sympotom-related search queries. A couple notes about the coefficients: The cv.glmnet() function automatically mean-centers and standardizes (mean = 0, unit variance) the target and inputs, but returns coefficients in the original scale. Due to the properties of the LASSO method, coefficients are without standard errors. The \\(\\lambda\\) parameter represented as s in the library is calculated along a grid (equal intervals between a minimum and maximum value). This means if coefficients for a specific value of \\(\\lambda\\) is requested, but is not in search grid, then the glmnet library will either need to interpolate the value (exact = FALSE) or refit the model at a specified lambda value (exact = TRUE). coef(mod.lasso, s = c(&quot;lambda.min&quot;) ) coef(mod.lasso, s = c( 0.01, 0.5), exact = TRUE ) Feature Lambda Min = 0.07 Lambda = 0.05 Lambda = 0.5 (Intercept) 1.98557 2.02873 0.95649 hits.typeafluvirus 0.01086 0.01072 0.00155 hits.pneumonia 0.00659 0.00908 0.00355 hits.influenza 0.00811 0.00854 hits.flusymptoms 0.00738 0.00815 0.01211 hits.tylenolcold 0.01021 0.00798 0.00548 hits.coldremedy 0.00583 0.00592 hits.fluspike 0.00555 0.00527 hits.fever 0.00377 hits.nasalcongestion 0.00218 hits.diarrhrea -0.00021 hits.fluvaccine -0.00023 hits.washhands -0.0012 hits.bluishskin -0.00111 -0.00186 hits.fatigue -0.0031 hits.vaccines -0.00317 hits.flushing -0.00453 -0.00379 hits.medicine -0.02348 -0.02427 Lastly, we can predict ILI in the test sample using the LASSO model where \\(\\lambda\\) is minimized. #Predict y yhat.train &lt;- predict(mod.lasso, x.train, s = &quot;lambda.min&quot;) #Calculate out of sample error rmse &lt;- function(y, x){ return(sqrt(mean((y - x)^2))) } err1 &lt;- round(rmse(yhat, y.test),2) print(err1) ## [1] 1.58 This result appears to be strong, but is unsatisfying without a set of benchmarks to compare against. Below, plain vanilla OLS is estimated based on the top most correlated features in sets of 1, 2, 3, 5, and 10, where 10 is selected as the optimal model contains 10 correlates. Across the board, LASSO out-performs OLS. #Calculate correlation matrix using training data rhos &lt;- as.vector(cor(y.train, x.train)) rhos &lt;- data.frame(id = 1:length(rhos), rhos) rhos &lt;- rhos[order(-rhos$rhos),] #Set up a juxtaposed plot area for six graphs par(mfrow=c(2,3), oma = c(5,4,0,0) + 0.5, mar = c(0,0,1,1) + 0.5) #Plot the LASSO plot(y.test, type = &quot;l&quot;, col = &quot;grey&quot;, main = paste0(&quot;LASSO: RMSE = &quot;, err1), cex.main = 1.2, ylab = &quot;outcome&quot;, xaxt=&#39;n&#39;, yaxt = &#39;n&#39;) lines(yhat, col = &quot;red&quot;) #Loop through and plot top X correlates using OLS for(i in c(1, 2, 3, 5, 10)){ #Set up data df.train &lt;- data.frame(y.train, x.train[,rhos$id[1:i]]) df.test &lt;- data.frame(y.test, x.test[,rhos$id[1:i]]) colnames(df.train) &lt;- colnames(df.test) &lt;- c(&quot;y&quot;, paste0(&quot;x&quot;,1:i)) #Model lm.obj &lt;- lm(y~., data = df.train) yhat2 &lt;- predict(lm.obj, newdata = df.test) #Plot y err2 &lt;- round(rmse(yhat2, y.test),2) plot(y.test, type = &quot;l&quot;, col = &quot;grey&quot;, main = paste0(&quot;Top &quot;, i,&quot; Only: RMSE = &quot;, err2), ylab = &quot;outcome&quot;, cex.main = 1.2, xaxt=&#39;n&#39;, yaxt = &#39;n&#39;) lines(yhat2, col = &quot;red&quot;) } 8.4.3 What level of [demand/staff] should I expect? To put regression into perspective, we will predict Maryland tollroad transactions using an assortment of US county-level data. The dataset is comprised of monthly totals of tollroad transactions by county, joined with US Bureau of Labor Statistics’ monthly employment estimates, US Census Bureau building permit estimates, and the Energy Information Agency’s West Texas Crude Fuel Prices. To get started, first we need to import the data set using digIt. Explore data The dataset is provided in long form, where data from multiple geographic areas are stacked. The dataset contains eight fields: date in MM/DD/YY format. year of record. fips is the Federal Information Processing System (FIPS) code for a given county. transactions is total monthly toll transactions in a county. transponders is the total monthly toll transactions conducted using a radio frequency transponder. emp is the employment in a given month in given county. bldgs is the number of new building permits issued in a given county. wti_eia is the West Texas Intermediate spot crude price. This is the only measure that will have the same value across all geographies in a given time period. To get a feel for the data, we use the str() method. All fields should be in numerical or integer form except for date and fips. str(df[1:3,]) ## &#39;data.frame&#39;: 3 obs. of 8 variables: ## $ date : chr &quot;7/1/12&quot; &quot;7/1/12&quot; &quot;7/1/12&quot; ## $ year : int 2012 2012 2012 ## $ fips : int 24003 24017 24510 ## $ transactions: int 1309632 312519 2868274 ## $ transponders: int 758737 142914 1951551 ## $ emp : int 241401 40184 329251 ## $ bldgs : int 84 67 8 ## $ wti_eia : num 87.9 87.9 87.9 Reformatting is simple. date can be converted into a date object using as.Date() and fips can be converted into a factor. df$date &lt;- as.Date(df$date, &quot;%m/%d/%y&quot;) df$fips &lt;- factor(df$fips) Upon doing so, we can run some cursory exploratory checks ahead of formulating any models. First is to produce correlation matrix. We will beautify the text outputs using a library called sjPlot that formats quantitative outputs in an easier to read fashion. The correlation matrix on the pooled data indicates finds that employment is positively associated with both transactions and transponder transactions. library(sjPlot) tab &lt;- cor(df[,c(4:8)], use = &quot;complete&quot;) sjp.corr(tab) For ease of interpretation, we will use a log-log specification, meaning that continuous variables that enter the regression specification are transformed using a natural log. This changes the interpretation of coefficients and in may improve model fit in certain situations. tab &lt;- cor(log(df[,c(4:8)]), use = &quot;complete&quot;) sjp.corr(tab) In addition, we can run an ANOVA (Analysis Of Variance) to understand if using FIPS county codes help explain the variation in transactions by looking at if the mean of transactions for each county are the same. The null hypothesis that county means are equivalent is rejected as the p-value of the F-test below is asymptotically small (&lt; 0.01). fit &lt;- aov(transactions ~ fips, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fips 5 3.381e+14 6.762e+13 526.8 &lt;2e-16 *** ## Residuals 268 3.440e+13 1.284e+11 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The same test using \\(log(transactions)\\) yields a better fit when comparing, indicating that we may want to consider to use log-transformed target feature. fit &lt;- aov(log(transactions) ~ fips, data = df) summary(fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fips 5 184.84 36.97 768.9 &lt;2e-16 *** ## Residuals 268 12.88 0.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Graphically, it is also important to develop a visual understand of how the data are distributed. Using ggplot2, we plot the toll transactions over time, finding a degree of seasonality and clear differences in the levels at which traffic flows through each county. library(ggplot2) ggplot(df, aes(x = date, y = transactions, group = fips, colour = fips)) + geom_line() + geom_point() Recognizing that each county experiences different levels of traffic and economic activity, it is prudent to break apart the data into histograms for each county to expose skewedness and determine if the fundamental Gaussian assumptions of OLS are met. The histograms below indicate that there most measures can be characterized by a central tendency, but there are some measures in some counties that are significantly right skewed. fips &lt;- unique(df$fips) for(k in 1:length(fips)){ temp &lt;- ggplot(data = df[df$fips==fips[k],], aes(transactions)) + geom_density(fill = &quot;orange&quot;) + theme(plot.title = element_text(size = 9), axis.text.x=element_blank(), axis.text.y=element_blank()) assign(paste0(&quot;trans&quot;,k), temp) temp &lt;- ggplot(data = df[df$fips==fips[k],], aes(emp)) + geom_density(fill = &quot;red&quot;) + theme(plot.title = element_text(size = 9), axis.text.x=element_blank(), axis.text.y=element_blank()) assign(paste0(&quot;emp&quot;,k), temp) temp &lt;- ggplot(data = df[df$fips==fips[k],], aes(bldgs)) + geom_density(fill = &quot;navy&quot;) + theme(plot.title = element_text(size = 9), axis.text.x=element_blank(), axis.text.y=element_blank()) assign(paste0(&quot;bldgs&quot;,k), temp) } library(gridExtra) grid.arrange(trans1, trans2, trans3, trans4, trans5, trans6, emp1, emp2, emp3, emp4, emp5, emp6, bldgs1, bldgs2, bldgs3, bldgs4, bldgs5, bldgs6, ncol = 6) This skewness can be improved by applying mathematical transformations such as \\(log_{10}\\) or natural logarithm. Set train/test samples With a basic understanding of the patterns in the underlying data, we can partition the data for training and testing. Given the small sample of points, we will partition the data into a 60-30 split, which is approximately 2012 through 2014 and 2015 through 2016, respectively. This partition is captured in a new variable flag. df$flag &lt;- 0 df$flag[df$year &gt; 2014] &lt;- 1 Regression Running a linear regression is a fairly simple task when using the lm() method. The basic syntax involves specifying the \\(y\\), \\(x\\) and the dataframe or matrix. lm(&lt;yvar&gt; ~ &lt;xvars&gt; , data = &lt;data&gt;) When evoked, the lm() method produces an class object that contains all the outputs of a regression model, including coefficients, fitness tests, residuals, predictions among other things.As a simple example, we will fit the specification: \\(\\log\\text{(transactions)} = \\beta_0 + \\beta_1 \\text{log(employment)} + \\epsilon\\) using only the training set (flag == 0). We can assign the output to fit. To see all attributes of the fit object, use the str() method. For a high-level summary, use the summary() method. The bivariate regression yielded statistically significant results for employment, indicating that a 100% increase in employment is associated with a 43% increase in highway toll transactions. The amount of variance explained is modest with an \\(R^2 = 0.321\\) with a relatively large \\(RMSE = 0.6904\\). fit &lt;- lm(log(transactions) ~ log(emp), data = df[df$flag == 0,]) summary(fit) ## ## Call: ## lm(formula = log(transactions) ~ log(emp), data = df[df$flag == ## 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3887 -0.5482 0.1730 0.5631 1.0330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.92253 0.57238 15.588 &lt;2e-16 *** ## log(emp) 0.43759 0.04755 9.202 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6904 on 176 degrees of freedom ## Multiple R-squared: 0.3248, Adjusted R-squared: 0.321 ## F-statistic: 84.68 on 1 and 176 DF, p-value: &lt; 2.2e-16 The fit object contains other rich information about attributes of a regression model, such as the coefficients, residuals, among other features. The full detail coefficients, for instance, can be easily viewed by using the following command: summary(fit)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.9225327 0.57238137 15.58844 5.564812e-35 ## log(emp) 0.4375895 0.04755384 9.20198 1.013823e-16 To view the full list of attributes contained in the regression object, use the structure method str(). A common step in assessing model fitness regressions is to check if the normality assumptions are met as this will influence reliability and accuracy of the model. The residuals, for example, should be normally distributed; however, the kernel density graph below shows that the residuals (in blue) are bimodally distributed, which is not normally distributed as compared with the simulated normal distribution (yellow). This indicates provides an indication that the regression needs to be refined in order to account for other parameters. x &lt;- data.frame(resid = fit$residuals) set.seed(50) x$norm &lt;- rnorm(nrow(x), mean(x$resid), sd(x$resid)) ggplot(x, aes(resid)) + geom_density(aes(norm), alpha = 0.4, fill = &quot;yellow&quot;) + geom_density(fill = &quot;navy&quot;, alpha = 0.6) Thinking back to the earlier results of the ANOVA test, it makes sense to incorporate the fips feature and try a few other features. The results are far more promising, with an R-squared above 0.9 and a RMSE that is roughly one-third the size. The employment feature is statistically significant with a p-value under 0.05 with a coefficient that indicates that essentially every one-person (or 100%) increase in employment is associated with a two fold increase in trips – this makes sense as people may be commuting for work. Additional building permits (log(bldgs)) appear to have a modest effect as may be expected as their effect is likely lagged. The FIPS counties, while not fully significant, seem to help explain much of the variability. fit &lt;- lm(log(transactions) ~ log(emp) + log(bldgs) + log(wti_eia) + fips, data = df[df$flag == 0,]) summary(fit) ## ## Call: ## lm(formula = log(transactions) ~ log(emp) + log(bldgs) + log(wti_eia) + ## fips, data = df[df$flag == 0, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.02680 -0.07561 -0.00165 0.08227 1.89656 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.99154 12.47438 -1.522 0.1298 ## log(emp) 2.57132 0.96714 2.659 0.0086 ** ## log(bldgs) 0.08442 0.04349 1.941 0.0539 . ## log(wti_eia) 0.20735 0.14657 1.415 0.1590 ## fips24003 -0.45659 0.30409 -1.502 0.1351 ## fips24005 -0.11387 0.11006 -1.035 0.3023 ## fips24015 5.59326 2.31513 2.416 0.0168 * ## fips24017 2.98459 2.03639 1.466 0.1446 ## fips24031 -1.60048 0.30429 -5.260 4.32e-07 *** ## fips24510 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2103 on 169 degrees of freedom ## Multiple R-squared: 0.9398, Adjusted R-squared: 0.937 ## F-statistic: 330 on 8 and 169 DF, p-value: &lt; 2.2e-16 A cursory look at the residuals finds a much more normally distributed set of residuals, indicating that incorporating the FIPS codes enables better a more proper model. x &lt;- data.frame(resid = fit$residuals) set.seed(50) x$norm &lt;- rnorm(nrow(x), mean(x$resid), sd(x$resid)) ggplot(x, aes(resid)) + geom_density(aes(norm), alpha = 0.4, fill = &quot;yellow&quot;) + geom_density(fill = &quot;navy&quot;, alpha = 0.6) Prediction The critical step is the prediction step, using the trained regression model to score the test set. In this example, both train and test sets are contained in data frame df. Generating predicted values \\(\\hat{y}\\) using the input features is a simple task that relies on the predict() method. The method accepts the regression object and a new dataset. In our example, we input use fit and df. The output is a vector of predictions for each line of df. We assign this vector as a new column as df. df$yhat &lt;- predict(fit, newdata = df) With the prediction available, a Mean Absolute Percentage Error (MAPE) can be used to quantify the model fit in terms that are interpretable. To do so, we first need to write a MAPE function. mape &lt;- function(y_hat, y){ return(mean(abs(y_hat/y-1), na.rm=T)) } Upon doing so, we can compare the performance of the model in the training phase and the test phase. The error rate of the training set was 0.8% and grows to 1.4% in the test set, indicating some degree of overfitting. However, in absolute terms, the errors are relatively small. #train error train_error &lt;- mape(df[df$flag == 0, &quot;yhat&quot;], log(df[df$flag == 0, &quot;transactions&quot;])) print(paste0(&quot;MAPE-train = &quot;, train_error)) ## [1] &quot;MAPE-train = 0.00804538836067337&quot; #test error test_error &lt;- mape(df[df$flag == 1, &quot;yhat&quot;], log(df[df$flag == 1, &quot;transactions&quot;])) print(paste0(&quot;MAPE-test = &quot;, test_error)) ## [1] &quot;MAPE-test = 0.0143640376554185&quot; The predictions can be compared against the actual transactional volumes using an ordinary scatter plot. To contextualize the predictions, we also add a 45-degree line that indicates how accurate predictions are relative to actual. Deviations below the line suggest that predictions are underestimation and above the line indicates overestimation. The effects of overfitting are clear in the models when comparing training results to test results. The training sample (left graph) predictions are spot on with predictions landing on the diagonal, whereas some of the test sample (right graph) stray below the line. library(gridExtra) g1 &lt;- ggplot(data = df[df$flag == 0, ], aes(x = log(transactions), y = log(transactions))) + geom_line() + geom_point() + ggtitle(paste0(&quot;Train set (MAPE = &quot;,round(100*train_error,2),&quot;%)&quot;)) + geom_point(aes(x = log(transactions), y = yhat, colour = fips)) + theme(plot.title = element_text(size = 10), ) g2 &lt;- ggplot(data = df[df$flag == 1, ], aes(x = log(transactions), y = log(transactions))) + geom_line() + geom_point() + ggtitle(paste0(&quot;Test set (MAPE = &quot;,round(100*test_error,2),&quot;%)&quot;)) + geom_point(aes(x = log(transactions), y = yhat, colour = fips)) + theme(plot.title = element_text(size = 10)) grid.arrange(g1, g2, ncol=2) 8.5 Exercises Prediction Using the Maryland Toll Road data, try the following: Run a regression for FIPS = 24015 where \\(Y\\) is \\(log(transactions)\\) and \\(X\\) are log(emp), log(bldgs), and log(wti_eia). Obtain the training and testing MAPEs for FIPS = 24015. How does this compare to the MAPE for FIPS = 24015 from the example pooled model? Calculating Distance Write a function dist() that calculates the distance between all records of a two variable data frame df and a given reference coordinate. The reference coordinate will be an index \\(i\\) of a row in the data frame (e.g. calculate the distance between row \\(i\\) and all other points). Expand that distance function to accept one or more variables. Use the example data to test your function. set.seed(150) data &lt;- data.frame(x1 = rnorm(1000, 10, 5), x2 = rnorm(1000, 20, 35), x3 = rnorm(1000, 14, 1), x4 = rnorm(1000, 100, 200)) For the following values, write a function to retrieve the value of \\(y\\) where \\(k = 1\\) for each record \\(i\\). Modify the function to handle \\(k = 2\\). Nearest Neighbors Write a function dist() that calculates the distance between all records of a two variable data frame df and a given reference coordinate. The reference coordinate will be an index i of a row in the data frame (e.g. calculate the distance between row i and all other points). dist &lt;- function(df, i){ temp &lt;- df[i,] dist &lt;- sqrt(df[,1]^2 + df[,1]^2) return(dist) } Expand that distance function to accept one or more variables. #Modify the function dist &lt;- function(df, i){ temp &lt;- df[i,] col &lt;- ncol(df) dist &lt;- 0 for(k in 1:col){ dist &lt;- dist + (df[,k] - temp[,k])^2 } return(sqrt(dist)) } #Test it set.seed(150) data &lt;- data.frame(x1 = rnorm(1000, 10, 5), x2 = rnorm(1000, 20, 35), x3 = rnorm(1000, 14, 1), x4 = rnorm(1000, 100, 200)) out &lt;- dist(data, 1) head(out) ## [1] 0.0000 131.7022 122.2110 336.8763 263.7995 322.0519 Write a function to retrieve the value of \\(y\\) where \\(k = 1\\) for each record \\(i\\). Modify the function to handle \\(k = 2\\). "],
["classification.html", "Chapter 9 Classification 9.1 Healthcare sans the politics 9.2 What goes into a classifier? 9.3 Six Common Techniques 9.4 DIY", " Chapter 9 Classification 9.1 Healthcare sans the politics In many countries, universal healthcare has become a basic human right. In the United States, this is not currently a guarantee, shrouded in heated political debate and controvery whether its a matter of human rights or a matter in which an individual may choose his or her fate. Regardless of the politics, there is data on healthcare coverage. According to the American Community Survey ACS, an annual survey of approximately 3.5% of the US population as conducted by the US Census Bureau, over 22.4% of residents of the U.S. state of Georgia were without healthcare coverage in 2009. That is a fairly sizable proportion of the population – for every ten people, between two to three did not have coverage. To some degree, this is [un]surprising. If you read the news in 2010, the then President of the United States championed a new law to provide affordable healthcare to the uninsured. Leaving aside the operational logistics of coordinating and establishing the actual healthcare system, imagine that you are hypothetically tasked with getting the word out and drive recruitment in the state of Georgia. There is a hiccup, however. While commercial registries exist with people’s demographic and personal contact information, most statistics on coverage are based on surveys, thus we do not precisely know who does not have insurance. A brute force approach could be to reach out to everyone under the sun though we can easily infer a wasted effort as 776 of every 1000 people are already covered. Of the 224 people, they are likely to come from different walks of life, which means that the message will need to be cater to different target segments. How can we more efficiently target and identify audience profiles? For marketers, this is a classic targeting problem. Data needs to enable the prediction and classification of a population into two classes: covered and not covered. This _binary problem or membership problem is known as a classification problem. By correctly classifying a population as covered and not covered, decision makers and outreach staff can mobilize targeted outreach. From a data science perspective, the real objective is to be able to identify and replicate re-occurring patterns in the training data, then generalize the insights onto a sample or population that is not contained in the sample. In most environments, a data analyst will typically manually select population characteristics to use in cross tabulations to find statistical patterns; however, this tradtional approach can suffer from human bias that may yield misleading results. Some features may be more important than others, and humans usually do not systematically check all features. For example, the table below compares healthcare coverage and citizenship. Each of the cells are quite interpretable: 63.2% of non-citizens are without coverage, but non-citizens are only 7.2% of the population. Coverage Without coverage % Without coverage Citizen 5,642,889 1,341,211 19.2% Non-citizen 199,039 343,088 63.2% All 5,841,928 1,684,299 22.3% % Non-citizen 3.4% 20.4% A cross-tabulation does not provide sufficient predictive power and solely relying on it will place one at the biased end of the bias-variance trade off. Expanding the table to include more features such as age, gender, wages, etc. may not improve inference either – too much information will invariably lead to analysis paralysis. Enter classifiers Classifiers or classification algorithms are a form of supervised learning that can efficiently and effectively identify patterns, surface important variables, and predict membership. Given the label \\(Y(Coverage)\\), we can use supervised learning techniques to find ways in which the following features can be used to make predictions: \\(Y(Coverage) = f(\\text{Sex, Age, Education, Marital Status, Race, Citizenship})\\) An algorithm can take on many forms, one of which known as a decision tree can essentially perform many cross-tabulations on steroids. The point of a cross-tabulation is to find patterns. But what defines a pattern? In some respects, a pattern is a sustained difference – a distinction that appears over and over again. For example, if we recall the citizenship vs. healthcare coverage example, we know that non-citizens are roughly 3.3-times more likely to not have coverage. Decision trees recursively split a population into smaller, more homogeneous cells. The result is a tree-like set of rules (below) that can not only be visualized, but interpreted as discrete cells of Georgians who have and do not have healthcare coverage. Green boxes indicate a majority of people have health insurance and blue boxes indicate a majority of people in the cell do not have insurance. For example, people who are not married, making less than $30,000 per year, are between the ages of 18 and 64 and are not citizens have a 73% chance of not having coverage. This subpopulation or leaf in decision tree parlance is roughly 1% of the population or roughly \\(n = 75262\\) and focusing on that leaf would in theory provide a maximum of a 75% hit rate. ## acs_health has been loaded into memory. ## Dimensions: n = 27382, k = 8 Figure 9.1: Simple decision tree Granted, the decision tree above is a simplistic biased instance. More complex, lower bias decision trees also can be trained (below), but may suffer from overfitting. Ultimately, the information provided by supervised models should be able to give outreach campaigns an economical advantage: a well-trained classification algorithm can weigh many more variables than a human, make predictions that are magnitudes better than random, and inform decisions using hard quantitative evidence. Figure 9.2: A “deep” decision tree. Decision tree algorithms are just one of many classifiers or classification algorithms, and, in fact, decision trees form the basis of many other classifiers. Some use recursive partitioning to segment a population into many, smaller homogeneous subpopulations. Other algorithms estimate geometrically inspired formulae to fit a multi-dimensional plane between two or more classes. Others average the results of a series of models in order to get the best of many worlds. Each class of model is defined with a mathematical scenarios in mind. 9.2 What goes into a classifier? Classifiers predict discrete targets, otherwise known as classes. Using the health insurance example, the classes are with insurance and without insurance. Being part of Generations X, Y, and Z would be three classes. Being a Red Sox or Yankees fan would be two classes. For classifiers to work, classes need to separable – the input features used to describe the target can be used to distinguish one group from another with some degree of accuracy. A low separability scenario, for example, would be one where the distributions of two classes substantially overlap, whereas a high separability case would have little overlap. The output of a classification algorithm is a probability that indicates how likely a given record belongs to a target class given the input features. Figure 9.3: Separability of two classes given a continuous feature. The output probability is the key to evaluating the accuracy of a model. Unlike regression, classifiers rely on entirely different measures of accuracy given the nature of the labeled data. All measures, however, rely on metrics that can be derived from a confusion matrix, or a \\(2 \\times 2\\) table where the rows typically represent actual classes and columns represent predicted classes. Predicted (+) Predicted (-) Actual (+) True Positive (TP) False Negative (FN) Actual (-) False Positive (FP) True Negative (TN) Each of the cells contains the building blocks of accuracy measures: The True Positive (TP) is the count of all cases where the actual positive (\\(Y = 1\\)) case is accurately predicted. The True Negative (TN) is the count of all cases where the actual positive (\\(Y = 0\\)) case is accurately predicted. The False Positive (FP) is count of all cases where the actual label was \\(Y = 0\\), but the model classified a record as \\(\\hat{Y} = 1\\). This is also known as Type I error. The False Negative (FN) is count of all cases where the actual label was \\(Y = 1\\), but the model classified a record as \\(\\hat{Y} = 0\\). This is also known as Type II error. Accuracy. Overall accuracy is measured as the sum of the main diagonal divided by the population (below). \\[TPR = \\frac{TP + TN}{TP + FN + FP + TN}\\] True Negative Rate. By combining TN and FP, we can calculate the True Negative Rate (TPR), which is proportion of \\(Y=0\\) cases that are accurately predicted. TNR is also referred to as the “specificity”. \\[TNR = \\frac{TN}{TN + FP} = \\frac{TN}{Actual (-)} \\] True Positive Rate. By combining TP and FN, we can calculate the True Positive Rate (TPR), which is proportion of \\(Y=1\\) cases that are accurately predicted. TPR is also referred to as the “sensitivity” or “recall”. \\[TPR = \\frac{TP}{TP + FN} = \\frac{TP}{Actual (+)} \\] Positive Predicted Value. By combining TP and FP, we can calculate the Positive Predicted Value (PPV), which is proportion of predicted \\(Y=1\\) cases that actually are of tht class. PPV is also referred to as “precision”. \\[PPV = \\frac{TP}{TP + FP}\\] What does accuracy look like? To illustrate this, the next series of tables provides simulated results of a classifer. Let’s assume that a health insurance classifier was tested on a sample of \\(n = 100\\) with actual labels perfectly split between \\(Y = 1\\) and \\(Y = 0\\). A perfect performing model would resemble the following table, where TP = 50 and FP = 50. With perfect predictions with \\(Accuracy = \\frac{50+50}{100} = 100\\), the \\(TPR = \\frac{50}{50 + 0} = 100\\) and \\(PPV = \\frac{50}{50 + 0} = 100\\) indicate that model is perfectly balanced and precise. Predicted (+) Predicted (-) Actual (+) 50 0 Actual (-) 0 50 A model with little discriminant power or ability to distinguish between classes would look like the following. While the \\(TPR = \\frac{35}{35 + 5} = 87.5\\) is high, overall \\(Accuracy = \\frac{35+0}{100} = 45\\), which is largely driven by low precision \\(PPV = \\frac{35}{35 + 60} = 36.8\\). Predicted (+) Predicted (-) Actual (+) 35 5 Actual (-) 60 0 While these calculations are simple and understandable, determining the predicted label is not as simple. In a simple case, given an outcome \\(Y = 1\\), a voting rule would classify a probability of greater than 50% as \\(Y = 1\\). However, it is fairly common that a trained classifier with strong performance may never produce a probability of more than 50%. In order to generalize accuracy, we can rely on one or a combination of the following measures. Measure Description Interpretation Receiving Operating Characteristic (ROC) Curve ROC curves plotpairs of TPRs and FPRs that correspond to varied discriminant thresholds between 0 and 1. By systematically testing thresholds. For example, TPRs and FPRs are calculated and plotted given probability thresholds \\(p = 0.2\\), \\(p = 0.5\\), and \\(p=0.8\\). Once plotting the curve with TPR as Y and FPR as X, the area under the curve (AUC) represents robustness of the model, ranging from 0.5 (model is as good as a coin toss) to 1.0 (perfectly robust model). In the social sciences, an acceptable AUC is over 0.8.The AUC statistic is sometimes referred to as the “concordance”. \\(F_1\\) Score The score is formulated as \\(F_1 = 2 \\times \\frac{precision \\times recall}{precision + recall}= 2 \\times \\frac{PPV \\times TPR}{PPV + TPR}\\) where \\(\\text{precision or PPV} = \\frac{TP}{TP + FP}\\) and \\(\\text{recall or TPR} = \\frac{TP}{TP + FN}\\) The measure is bound between 0 and 1, where 1 is the top score indicating a better model. 9.3 Six Common Techniques In the context of healthcare coverage, we will use KNNs to illustrate the process of training a classifier. With the practical aspects in mind, we will explore two types of tree-based learning, namely decision trees and random forests. Then wrap up with logistic regression and a comparison of the performance of each of the four classifiers. To start, we will need to import data from the healthcare coverage example. The data was obtained from the 2015 American Community Survey (ACS), which is available from US Census Bureau website. So that this chapter can focus more on classification methods, data has been pre-processed, and any data wrangling that is shown herein is specific to each method. Note that the sample has been balanced such that people who have and do not have health insurance are represented in equal proportions. The file can be imported using the digIt library. Upon loading the data set, five string variables will be converted into factors. #Import library(digIt) health &lt;- digIt(&quot;acs_health&quot;) #Factors factor_vars &lt;- c(&quot;cit&quot;, &quot;mar&quot;, &quot;schl&quot;, &quot;esr&quot;) for(i in factor_vars){ health[,i] &lt;- as.factor(health[,i]) } str(health) ## &#39;data.frame&#39;: 27382 obs. of 8 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ coverage: chr &quot;No Coverage&quot; &quot;No Coverage&quot; &quot;No Coverage&quot; &quot;No Coverage&quot; ... ## $ agep : int 58 52 40 18 56 52 62 26 41 23 ... ## $ wage : int 0 0 0 0 0 0 0 0 0 0 ... ## $ cit : Factor w/ 2 levels &quot;Citizen&quot;,&quot;Non-citizen&quot;: 1 1 2 2 1 1 1 1 1 1 ... ## $ mar : Factor w/ 5 levels &quot;Divorced&quot;,&quot;Married&quot;,..: 1 3 2 2 1 1 2 3 2 3 ... ## $ schl : Factor w/ 4 levels &quot;Graduate Degree&quot;,..: 2 2 2 2 2 3 2 3 2 2 ... ## $ esr : Factor w/ 4 levels &quot;Armed Forced&quot;,..: 2 2 2 3 3 3 3 4 3 2 ... 9.3.1 KNN As covered in Lecture 6, KNNs are a weak learning algorithm that treats input features as coordinate sets. Given a class label \\(y\\) associated with input features \\(x\\), a given record \\(i\\) in a dataset can be related to all other records using Euclidean distances in terms of \\(x\\): \\[ \\text{distance} = \\sqrt{\\sum(x_{ij} - x_{0j})^{2} }\\] where \\(j\\) is an index of features in \\(x\\) and \\(i\\) is an index of records (observations). For each \\(i\\), a neighborhood of taking the \\(k\\) records with the shortest distance to that point \\(i\\). From that neighborhood, the value of \\(y\\) can be approximated. Given a discrete target variables, \\(y_i\\) is determined using a procedure called majority voting where the most prevalent value in the neighborhood around \\(i\\) is assigned. Recall that in the case of KNNs, all variables should be in the same scale such that each input feature has equal weight. A review of the data indicates that the health data is not in the appropriate form to be used. 9.3.1.1 Data preparation: Mixed variable formats Continuous variables can be discretized by binning records into equal intervals, then converting the bins into dummy matrices For simplicity, we’ll bin the age and wage varaibles in the following manner: age: 10 year intervals. wage: $20,000 intervals, topcoded at $200,000. Upon binning, each variable needs to be set as a factor. #Age health$age.bin &lt;- round(health$agep / 10) * 10 health$age.bin &lt;- factor(health$age.bin) #Wage health$wage.bin &lt;- round(health$wage / 20000) * 20000 health$wage.bin[health$wage.bin &gt; 200000] &lt;- 200000 health$wage.bin &lt;- factor(health$wage.bin) For all discrete features including the newly added age and wage variables, we can convert them into dummy matrices (e.g. all except one level in a discrete feature is converted into a binary variable). The former can be easily achieved by using the model.matrix() method, which returns a binary matrix for all levels: model.matrix(~ health$variable - 1) As is proper in preparation of dummy variables, if there are \\(k\\) levels in a given discrete variable, we should only keep \\(k-1\\) dummy variables For example, citizenship is a two level variable, thus we only need to keep one of two dummies. It’s common to leave out the level with the most records, but any level will do. #Make copy of health data frame knn_data &lt;- health[, c(&quot;id&quot;,&quot;coverage&quot;)] #Specify variables that need to be discretized discrete.vars &lt;- c(&quot;cit&quot;, &quot;mar&quot;, &quot;schl&quot;, &quot;wage.bin&quot;, &quot;age.bin&quot;, &quot;esr&quot;) #Loop through and add dummy matrices to knn_data for(i in discrete.vars){ dummy_mat &lt;- model.matrix(~ health[,i] - 1) knn_data &lt;- cbind(knn_data, dummy_mat) } Now the data can be combined. Notice that the new dataset knn_data has 36 features. Note that perform these transformations are necessary given mixed variable types; however, a datasets containing continuous variables only does not require any manipulation other than scaling. #Dimensions dim(knn_data) ## [1] 27382 27 9.3.1.2 Sample partition As is proper, the next step is to partition the data. For simplicity, we’ll create a vector that will split the data into two halves, denoting the training set as TRUE and the test set as FALSE. We then split the data into two objects contain the input features for each train and test sets. #Split into simple train-test design set.seed(100) rand &lt;- runif(nrow(knn_data)) rand &lt;- rand &gt; 0.5 train &lt;- knn_data[rand == T, 2:ncol(knn_data)] test &lt;- knn_data[rand == F, 2:ncol(knn_data)] 9.3.1.3 Modeling As it common and proper, the kNN algorithm needs to be calibrated for the best \\(k\\) using the training set, then applied to a test set. To do this, we will use the kknn library. The training portion uses the train.kknn() function to conduct k-folds cross validation, then the scoring uses the kknn(). While both functions can be fairly easily written from scratch (and we encourage new users to write their own as was demonstarted in the previous chapter), we will plow forth with using the library. To start, we will load the kknn library: #Call &quot;class&quot; library library(kknn) In order to find the optimal value of \\(k\\), we will execute the train.kknn() function, which accepts the following arguments: train.kknn(formula, data, kmax, kernel, distance, kcv) formula is a formula object (e.g. “coverage ~ .”). data is a matrix or data frame of training data. kmax is the maximum number of neighbors to be tested kernel is a string vector indicating the type of distance weighting (e.g. “rectangular” is unweighted, “biweight” places more weight towards closer observations, “gaussian” imposes a normal distribution on distance, “inv” is inverse distance). distance is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary). kcv is the number of partitions to be used for cross validation. The flexibility of train.kknn() allows for test exhaustively and find the best parameters. Below, we conduct 10-folds cross validation up to \\(k = 200\\) for three kernel (rectangular, biweight and inverse) assuming L1-distances. While the command is simple, it runs the kNN algorithm for 2000 times (10 cross-validation models for each k - kernel combination). pred.train &lt;- train.kknn(factor(coverage) ~. , data = train, kcv = 10, distance = 1, kmax = 500, kernel = c(&quot;rectangular&quot;, &quot;biweight&quot;, &quot;inv&quot;)) The resulting model object contains the cross-validation error log in the MISCLASS attribute, which has been plotted below, as well as best.parameters that indicates that \\(k = 335\\) using an inverse distance kernel yields the lowest error. #Find optimal k and kernel plot(pred.train$MISCLASS[,c(&quot;biweight&quot;)], type = &quot;l&quot;, col = &quot;orange&quot;, ylab = &quot;Classification error&quot;, xlab = &quot;k&quot;) lines(pred.train$MISCLASS[,c(&quot;inv&quot;)], col = &quot;red&quot;) lines(pred.train$MISCLASS[,c(&quot;rectangular&quot;)], col = &quot;blue&quot;) Figure 9.4: 10-fold cross validated errors for k = 1 to k = 200 The result suggest that a combination of \\(k = 410\\) using inverse distance yields the best result. With the kNN algorithm tuned, we can now use the kknn() function to score the test set. The function syntax is as follows: kknn(formula, train, test, k, kernel, distance) formula is a formula object (e.g. “coverage ~ .”). train is a matrix or data frame of training data. test is a matrix or data frame of test data. k is the number of neighbors. kernel is the type of weighting of distance (e.g. “rectangular” is unweighted, “biweight” places more weight towards closer observations). distance is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary). #Score train set out &lt;- kknn(factor(coverage) ~. , train = train, test = test, k = 335, kernel = &quot;inv&quot;, distance = 1) #Extract probabilities test.prob &lt;- out$prob[,2] #Convert probabilities to prediction pred.class &lt;- vector(length = length(test.prob)) pred.class[test.prob &lt; 0.5] &lt;- &quot;Coverage&quot; pred.class[test.prob &gt;= 0.5] &lt;- &quot;No Coverage&quot; #Confusion matrix table(test$coverage, pred.class) ## pred.class ## Coverage No Coverage ## Coverage 4397 2480 ## No Coverage 1210 5699 Using the extracted probabilities, we now can calculate the accuracy using the True Positive Rate (TPR) using a probability cutoff of 0.5. Typically, one would expect a \\(2 \\times 2\\) matrix given a binary label where the accuracy rate can be calculated based on the diagonals. In this case, prediction accuracy was 73.2%, indicating that the model performs reasonably well. The test model accuracy can also be calculated by taking the Area Under the Curve (AUC) of the Receiving-Operating Characteristic. The ROC calculates the TPR and FPR at many thresholds, that produces a curve that indicates the general robustness of a model. The AUC is literally the area under that curve, which is a measure between 0.5 and 1 where the former indicates no predictive power and 1.0 indicates a perfect model. In order to visualize the ROC, we will rely on the plotROC library, which is an extension of ggplot2. We will create a new data frame input that is comprised of the labels for the test set ytest and the predicted probabilities test.prob. #Load libraries library(ggplot2) library(plotROC) #Set up test data frame input &lt;- data.frame(ytest = test$coverage, prob = test.prob) We then will first create a ggplot object named base that will contain the labels (d =) and probabilities (m =), then create the ROC plot using geom_roc() and style_roc(). A ROC curve for a well-performing model should sit well-above the the 45 degree diagonal line, which is the reference for an AUC of 0.5 (the minimum expected for a positive predictor). However, as the curve is below the 45 degree line, we may have a seriously deficient model. #Base object roc &lt;- ggplot(input, aes(d = ytest, m = prob)) + geom_roc() + style_roc() #Show result roc Figure 9.5: ROC for k = 410 using inverse distance To calculate the AUC, we can use the calc_auc() method, from which we find that 0.8, which is generally a decent level of accuracy. calc_auc(roc)$AUC ## [1] 0.8001445 Despite the promising result, there are a few one should ask the following question: Is there a better classifier? 9.3.2 Logistic Regression Let’s assume that you’ve been provided with a three feature dataset: a target label \\(z\\) and two input features (\\(x1\\) and \\(x2\\)). Upon graphing the features and color coding using the labels, you see that the points are clustered such that light blue points represent to \\(z = 1\\) and gold points represent \\(z = 0\\). We could, of course, use decision trees and random forests to determine some threshold to classify the two groups; but surely, there is a way to write an elegant statistical formula that would separate one group from the other? As it turns out, we can express the relationship between \\(z\\), \\(x_1\\), and \\(x_2\\) as a linear model similar to OLS: \\[z = w_0 + w_1 x_1 + w_2 x_2 + \\epsilon\\] where \\(z\\) is a binary outcome and, like OLS, \\(w_k\\) are weights that are learned using some optimization process. If treated as a typical linear model with a continuous outcome variable, we run the risk that \\(\\hat{z}\\) would exceed the binary bounds of 0 and 1 and would thus make little sense. Imagine if \\(\\hat{z}\\), the predicted value of \\(z\\) were -103 or +4: what would that mean in the case of a binary variable? This could easily be the shortcoming of a linear model approach. Statistical methodologists have, however, cleverly solved the bounding problem by inserting the predicted output into a logistic function: \\[F(z) = \\frac{1}{1+ e^{-z}}\\] For a feature \\(x\\) that ranges for -10 to +10, the logit transformation converges to +1 where \\(x &gt; 0\\) and to 0 where \\(x &lt; 0\\). This S-shaped curve is known as a sigmoid and is a well-used distribution for bounding variables to a 0/1 range. By substituting the linear model output \\(z\\) into the logistic function, we bound the output between 0 and 1 and interpret the result as a conditional probability: \\[p = Pr(Y=1|X) = F(z) = \\frac{1}{1+ e^{-(w_0 + w_1 x_1 + w_2 x_2 )}} \\] To interpret the coefficients, we need to start by defining what odds are: \\[odds = \\frac{p}{1-p}= \\frac{F(z)}{1-F(z)}=e^z\\] where \\(F(z)\\) is a probability of some event \\(z = 1\\)and \\(1-F(z)\\) is the probability of \\(z = 0\\). The odds can be re-formulated as: \\[pr(success) = \\frac{e^{(w_0 + w_1 x_1 + w_2 x_2 )}}{1+e^{(w_0 + w_1 x_1 + w_2 x_2 )}}\\] \\[pr(failure) = \\frac{1}{1+e^{(w_0 + w_1 x_1 + w_2 x_2 )}}\\] Typically, we deal with odds in terms of log odds as the exponentiation may be challenging to work with: \\[log(odds)=log(\\frac{p}{1-p})= w_0 + w_1 x_1 + w_2 x_2 \\] where log is a natural logarithm transformation. This relationship is particularly important as it allows for conversion of probabilities into odds and vice versa. The underlying weights of the logistic regression can be interpretted using Odds Ratios or OR. Odds ratios can be expressed as marginal unit comparison. Since \\(odds = e^{z} = e^{w_0 + w_1 x_1 + w_2 x_2}\\), then we can express an odds ratio as a marginal 1 unit increase in \\(x_1\\) comparing \\(odds(x+1)\\) over \\(odd(x+0)\\): \\[OR = \\frac{e^{w_0 + w_1 (x_1+1) + w_2 x_2}}{e^{w_0 + w_1 (x_1+0) + w_2 x_2}} = e^{w_1}\\] After a little exponential arithmetic, the OR is simply equal to \\(e^{w_1}\\), which can be interpreted as a multiplicative effect or a percentage effect if transformed as \\(100 \\times (1-e^{w_1})\\%\\). In practice, this means simply exponentiating the regression weights to interpret the point relationship. For example, if the following regression were estimated for healthcare non-coverage where \\(wage\\) is a continuous variable and \\(non-citzen\\) is a discrete binary: \\[z(\\text{non-coverage}) = 0.1878 - 0.000001845 \\times wage + 1.69 \\times \\text{non-citizen} \\] Then, the odds of coverage are as follows for each variable: \\(OR_{wage} = e^{0.000001845} = 0.9999816\\) translates to -0.00000184% lower chance of not being covered. \\(OR_{non-citizen} = e^{1.690} = 5.419481\\) translates to 441% higher chance of not being covered. Optimization As in the case of all machine learning methods, the formulae need to be optimized. In order to estimate each weight \\(w_k\\), we will rely on maximum likelihood estimation (MLE) as a framework, starting with a probability function for one record that is inspired by a Bernoulli random variable: \\[ p(z = z_i | x) = [F(x)]^{z_i}[1-F(x)]^{1-z_i}\\] If \\(z_i=1\\), then the function is equal to the \\([F(x)]^{z_i}\\). Otherwise, if \\(z_i = 0\\), then the function is equal to \\([1-F(x)]^{1-z_i}\\). For all records, we can define a likelihood function as the product of the above: \\[L = \\Pi_{i=1}^N [F(x)]^{z_i}[1-F(x)]^{1-z_i}\\] Mathematically, it is easier to handle this formula by taking the natural logarithm, which is also known as the log-likelihood: \\[ log(L) = z_ilog(F(x)) + (1-z_i)log(1-F(x))\\] The goal here is to maximize \\(log(L)\\), driven by a search for \\(w_k\\) by taking partial derivatives of \\(L\\) with respect to each \\(w_k\\) and setting them to zero: \\[\\frac{\\partial L}{\\partial w_k} = 0\\] This process can be driven using optimization algorithms such as gradient descent, the Newton-Raphson algorithm, among other commonly used techniques. Practicals After all the derivation is done, keep the following points in mind when applying logistic regression: Tuning a logistic regression is a matter of selecting combinations of features (variables): it all depends on finding the right combination of features that maximize classification accuracy. Logistic regression have strong probabilistic assumptions that a linear combination of features is sufficient to describe a phenomenon. The technique is well-suited for socializing an empirical problem, but often is outperformed in accuracy by more flexible techniques that are described later in this chapter. This tradeoff between narrative and accuracy is a good example of the bias-variance tradeoff. Like ordinary least squares, the method does not perform well when the number of features is greater than the number of observations. Regularization methods (e.g. LASSO and Ridge) described in the previous chapter can be generalized for classification problems. 9.3.2.1 In Practice: Logistic Regression For the remaining techniques in this chapter, we will use the following data set. The health data are split into a 50-50 train-test sample. Whereas the variables in the kNN example were converted into discrete variables, this sample will use mixed data classes with two continuous variables (wage - wage and age = age) and four discrete variables (coverage = health coverage, mar = marriage, cit = citizenship, esr = employment status, schl = education attainment). # Load ACS health care data library(digIt) health &lt;- digIt(&quot;acs_health&quot;) # Convert characters into discrete factors factor_vars &lt;- c(&quot;coverage&quot;, &quot;mar&quot;, &quot;cit&quot;, &quot;esr&quot;, &quot;schl&quot;) for(var in factor_vars){ health[,var] &lt;- as.factor(health[,var]) } # Randomly assign set.seed(100) rand &lt;- runif(nrow(health)) &gt; 0.5 # Create train test sets train &lt;- health[rand == T, ] test &lt;- health[rand == F, ] Training a logistic regression can be easily done using the glm() function, which is a flexible algorithm class known as Generalized Linear Models. Using this one method, multiple types of linear models can be estimated including ordinary least squares for continuous outcomes, logistic regression for binary outcomes and Poisson regression for count outcomes. At a minimum, three parameters are required: glm(formula, data, family) where: formula is a formula object. This can take on a number of forms such as a symbolic description (e.g. \\(y = w_0 + w_1 x_1+ w_2 x_2 + \\epsilon\\) is represented as y ~ x1 + x2). data is a data frame containing the target and inputs. family indicates the probability distribution used in the model. Distributions typically used for GLMs are binomial (binary outcomes), poisson (count outcomes), gaussian (continuous outcomes - same as OLS), among others. The family refers to the probability distribution family that underlies the specific estimation method. In the case of logistic regression, the probability family is binomial. To start, we will specify three models: Economic: \\(coverage = f(log(age) + wage + employment)\\) Social: \\(coverage = f(citizenship + marital + schooling)\\) Combined: \\(coverage = f(log(age) + wage + employment + citizenship + marital + schooling)\\) then assign each to a formula object and estimate each formula. # Formula objects econ &lt;- as.formula(&quot;coverage ~ log(agep) + wage + esr&quot;) soc &lt;- as.formula(&quot;coverage ~ cit + mar + schl&quot;) all &lt;- as.formula(&quot;coverage ~ log(agep) + wage + schl + esr + cit + mar&quot;) # Estimated GLM models glm_econ &lt;- glm(econ, data = train, family = binomial) glm_soc &lt;- glm(soc, data = train, family = binomial) glm_all &lt;- glm(all, data = train, family = binomial) In the social sciences and in public policy, the focus of regression modeling is typically placed on identifying an effect or an associated relationship that describes the process being studied. Often times, coefficient tables are examined, in particular the direction of the relationships (e.g. positive or negative weights), their statistical significance (e.g. p-value or t-statistics), and the relative fit of the model (e.g. the lowest Akaike Information Criterion or AIC provides relative model fit comparison). For example, an analyst may point out that education has an effect on coverage by interpreting the coefficient point estimates. In the combined model, education attainment coefficients are are estimated relative to people who hold a graduate degree, thus indicating that people who : did not finish high school have a 6.58-times higher chance of not having health coverage ($ e^{} = 6.58$) hold a high school degree have a 4.91-times higher chance of not having health coverage ($ e^{} = 4.91$) hold a college degree are relatively better off than the previous two groups with a 1.79-times higher chance of not having health coverage ($ e^{} = 1.79$) All coefficients are statistically significant. While it is valid to evaluate models on this basis, it is necessary to remember that this is not the same as evaluating a model for predictive use cases as predictive accuracy is not assessed on the basis of coefficients. Dependent variable: coverage (1) (2) (3) log(agep) -1.415*** -0.785*** (0.045) (0.062) wage -0.0005*** -0.001*** (0.0001) (0.0001) esrArmed Forced -5.261*** -4.937*** (0.723) (0.726) esrEmployed Civilian -1.419*** -1.260*** (0.073) (0.077) esrNot in Labor Force -1.250*** -1.307*** (0.075) (0.080) esrUnemployed citCitizen -2.132*** -2.016*** (0.083) (0.084) citNon-citizen marDivorced 1.837*** 1.459*** (0.113) (0.118) marMarried 0.844*** 0.421*** (0.105) (0.110) marNever Married 2.074*** 1.222*** (0.106) (0.121) marSeparated 2.116*** 1.666*** (0.152) (0.159) marWidowed schlGraduate Degree -0.648*** -0.580*** (0.114) (0.115) schlHS Degree 1.064*** 1.035*** (0.062) (0.064) schlLess than HS 1.346*** 1.356*** (0.071) (0.074) schlUndergraduate Degree Constant 6.427*** -0.348** 4.166*** (0.179) (0.138) (0.298) Observations 13,596 13,596 13,596 Log Likelihood -8,497.670 -7,927.152 -7,529.608 Akaike Inf. Crit. 17,007.340 15,872.300 15,087.220 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Like the kNN example, the absolute accuracy of a model needs to be obtained through model validation techniques like cross validation. The boot library can be used to generate cross-validated accuracy estimates through the cv.glm() function: cv.glm(data, glmfit, cost, K) where: data is a data frame or matrix. fit is a glm model object. cost specifies the cost function for cross validation. K is the number of cross validation partitions. Note that the cost function needs to take two vectors. The first is the observed responses and the second is the predicted responses. For example, the cost function could be the overall accuracy rate: \\[ \\frac{FP+FN}{TP+FP+TN+FN}\\] or the true positive rate (TPR): \\[\\frac{TP}{TP+FN}\\] Both are written as functions below: # Misclassification Rate costAccuracy &lt;- function(y, y.hat){ a &lt;- sum((y == 1 ) &amp; (y.hat &gt;= 0.5)) b &lt;- sum((y == 0 ) &amp; (y.hat &lt; 0.5)) c &lt;- ((a + b) / length(y)) return(c) } # True Positive Rate costTPR &lt;- function(y, y.hat){ a &lt;- sum((y == 1 ) &amp; (y.hat &gt;= 0.5)) b &lt;- sum((y == 1 ) &amp; (y.hat &lt; 0.5)) return((a) / (a + b)) } So that we can compare the cross validation accuracy with kNN, we will specify the cost using the misclassification rate for each of the three candidate models and set \\(k = 10\\). Whereas kNN was able to achieve a 74% accuracy rate, the best GLM model was able to reach 72%, suggesting that some of the underlying variability in coverage rate is not captured in linear relationships. Also note that the input features for the kNN model were in a dummy matrix, thus the comparison is not perfect. ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading specification accuracy Economic 0.6482789 Social 0.6946161 All 0.7217564 In order to obtain the predicted values of \\(coverage\\), we use predict(): predict(object, newdata, response) where: object is a GLM model object. newdata is a data frame. This can be the training data set or the test set with the same format and features as the training set. response indicates the type of value to be returned, whether it is the untransformed “link” or the probability “response”. We will now apply predict() to score the responses for each train and test samples. pred.glm.train &lt;- predict(glm_all, train, type = &quot;response&quot;) pred.glm.test &lt;- predict(glm_all, test, type = &quot;response&quot;) A quick review of the predicted probabilities indicates confirms that we have the right response values (probabilities), bound by 0 and 1. summary(pred.glm.train) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000006 0.3015000 0.5117000 0.4988000 0.6881000 0.9892000 Lastly, to calculate the prediction accuracy, we will once again rely on the combination of ggplot2 and `plotROC libraries for the AUC. Interestingly, the test set AUC is greater than that of the train set. This occurs occassionally and is often times due to the luck of the draw. #plotROC library(plotROC) library(ggplot2) #Set up ROC inputs input.glm &lt;- rbind(data.frame(model = &quot;train&quot;, d = train$coverage, m = pred.glm.train), data.frame(model = &quot;test&quot;, d = test$coverage, m = pred.glm.test)) #Graph all three ROCs roc.glm &lt;- ggplot(input.glm, aes(d = d, model = model, m = m, colour = model)) + geom_roc(show.legend = TRUE) + style_roc() + ggtitle(&quot;ROC: GLM&quot;) #AUC calc_auc(roc.glm)[,2:3] ## group AUC ## 1 1 0.7898052 ## 2 2 0.7958594 9.3.3 Decision trees In everyday policy setting and operations, decision trees are a common tool used for communicating complex processes, whether for how an actor moves through intricate and convoluted bureaucracy or how a sub-population can be described based on a set of criteria. While the garden variety decision tree can be laid out qualitatively, supervised learning allows decision trees to be created in an empirical fashion that not only have the power to aesthetically communicate patterns, but also predict how a non-linear system behaves. The structure of a decision tree can be likened to branches of a tree: moving from the base of the tree upwards, the tree trunk splits into two or more large branches, which then in turn split into even smaller branches, eventually reaching even small twigs with leaves. Given a labeled set of data that contains input features, the branches of a decision tree is grown by subsetting a population into smaller, more homogeneous units. In other words, moving from the root of the tree to the terminating branches, each subsequent set of branches should contain records that are more similar, more homogeneous or purer. 1. Let Sample = S, Target = Y, Input Features = X 2. Screen records for cases that meet termination criteria. If each base case that is met, partition sample to isolate homogeneous cases. 3. For each X: Calculate the attribute test comparing all X&#39;s and Y 4. Compare and identify Xi that yields the greatest separability 5. Split S using input feature that maximizes separability 6. Iterate process on steps 3 through 5 until termination criteria is met As was demonstrated at the beginning of this chapter, decision trees use a form of recursive partitioning to learn patterns, doing so using central concepts of information theory. There are a number of decision tree algorithms that were invented largely in the 1980s and 1990s, including the ID3 algorithm, C4.5 algorithm, and Classification And Regression Trees for Machine Learning (CART). All these algorithms follow the same framework that includes the following elements: (1) nodes and edges, (2) attribute tests, and (3) termination criteria. 9.3.3.1 (1) Nodes + Edges Recalling the healthcare insurance decision tree, the tree can be characterized by nodes and edges. Nodes (circles) contain records. Edges (lines) show dependency between nodes and is the product of a split decision. Nodes are split based on an attribute test – a technique to identify the optimal criterion to subset records into more homogeneous groups of the target variable. The node at the top of the tree is known as the rootand represents the full population. Each time a node is split, the result is two nodes – each of which is referred to as a child node. A node without any child nodes is known as a leaf. The goal is to grow a tree from the root node into as many smaller, more homogeneous child nodes with respect to the target variable. 9.3.3.2 (2) Attribute tests To understand attribute tests means to have a thorough understanding of separability. Let’s suppose we have a list of residents of a town. The list contains both users and non-users of a given healthcare service. For each person, the inventory captures whether a given person is employed, has income over $20k, and lives on the west side or east side of town. Each of the features are plotted in the pie chart below. 50% of town residents use the health service, but which of the features is best at separating users from non-users? Figure 9.6: Summary characteristics of town residents. To answer that question, we can rely on a visual cross-tabulation where the size of the circles is scaled proportional to the number of records. The objective is to identify the matrix where the circles are the largest along any diagonal – this would indicate that given usership, a feature is able to serve as a criterion that separates users from non-users. Of the three graphs below, graph #2 is able to separate a relatively large proportion of users from non-users. For a relatively low-dimensional dataset (fewer attributes), a visual analysis is accomplishable. However, on scale, undertaking this process manually may be onerous and prone to error. Figure 9.7: A visual comparison of low separability (1 and 3) and high separability (2). Enter attribute tests. Decision trees are grown by splitting a data set into many smaller samples. Attribute tests are the mode of finding the split criterion, following an empirical process to systematically test all input features to find the feature with the greatest separability. The process starts from the root node where the algorithm examines each input feature to find the one that maximizes separability at that node: Let Sample = S, Target = Y, Input Features = X For each X: Calculate the attribute test statistic comparing X and Y Store statistic Compare and identify Xi that yields the greatest separability Split S using input feature that maximizes separability Iterate process on child node Upon finding the optimal feature for a given node, the decision tree algorithm splits the node into two child nodes based on the optimal feature, then moves onto the next node (often times a child node) and runs the same process to find the next split. There are a number of attribute tests, of which we will cover two: Information Gain and Gini Impurity. Information gain is a form of Entropy, which is a measure of purity of information. Based on these distinct states of activity, entropy is defined as: \\[\\text{Entropy} = \\sum{-p_{i} log_2(p_{i})}\\] where \\(i\\) is an index of states, \\(p\\) is the proportion of observations that are in state \\(i\\), and \\(log_2(p_i)\\) is the Base 2 logarithm of the proportion for state \\(i\\). Information Gain (IG) is variant of entropy, which is the entropy of the root node less the average entropies of the child nodes. \\[\\text{IG} = \\text{Entropy}_\\text{root} - \\text{Avg Child Entropy}\\] How does this work in practice? Starting from the root node, we need to calculate the root entropy, where the classes are based on the classes of the target usership. \\(\\qquad \\text{Entropy}_\\text{usership} = (-p_{user} log_2(p_{\\text{user}})) - (-p_{\\text{non-user}} log_2(p_{\\text{non-user}}))\\) \\(\\qquad \\qquad \\qquad \\qquad \\qquad = (-\\frac{6}{12} log_2(\\frac{6}{12})) + (-\\frac{6}{12} log_2(\\frac{6}{12}))\\) \\(\\qquad \\qquad \\qquad \\qquad \\qquad = 1.0\\) Then, the attribute test is applied to the root node by calculating the weighted entropy for each proposed child node. Using the income feature, the calculation is as follows: Split the root node into two child nodes using the income class. This yields the following subsamples as shown in the table below: &lt; $20k &gt; $20k No 0 6 Yes 5 1 Total 5 7 For each child node (the columns in the table), calculate entropy: \\(\\qquad \\text{Entropy}_\\text{income &lt; 20k } = (-p_{user} log_2(p_{\\text{user}})) - (-p_{\\text{non-user}} log_2(p_{\\text{non-user}}))\\) \\(\\qquad \\qquad \\qquad \\qquad \\qquad = -\\frac{5}{5} log_2(\\frac{5}{5}) = 0\\) \\(\\qquad \\text{Entropy}_\\text{income &gt; 20k } = (-p_{user} log_2(p_{\\text{user}})) - (-p_{\\text{non-user}} log_2(p_{\\text{non-user}}))\\) \\(\\qquad \\qquad \\qquad \\qquad \\qquad = -\\frac{6}{7} log_2(\\frac{6}{7}) + -\\frac{1}{7} log_2(\\frac{1}{7}) = 0.5916728\\) Calculate the weighted average entropy of children: \\(\\qquad \\text{Entropy}_\\text{income split} = \\frac{5}{12}(0) + \\frac{7}{12}(0.5916728) = 0.3451425\\) Then calculate the information gain: \\(\\qquad \\text{IG}_\\text{income} = \\text{Entropy}_\\text{root} - \\text{Entropy}_\\text{income split}\\) \\(\\qquad \\qquad \\qquad \\qquad \\qquad = 1 - 0.3451425 = 0.6548575\\) We then can perform the same calculation on all other features (e.g. employment, part of town) and compare results. The goal is to maximize the IG statistic at each decision point. In this case, we see that income is the best attribute to use for splitting. This split is easily interpretable: “The majority of users of health services can be predicted to earn less than $20,000.” Measure IG Employment 0.00 Income 0.6548575 Area of Town 0.027119 Gini Impurity is closely related to the entropy with a slight modification: \\[\\text{Gini Impurity} = \\sum{p_{i}(1-p_{i})} = 1 - \\sum{p_{i}^2}\\] Using Gini Impurity as an attribute test is also similar to Information Gain: \\[\\text{Gini Gain} = \\text{Gini}_\\text{root} - \\text{Weighted Gini}_\\text{child}\\] 9.3.3.3 (3) Stopping Criteria + Tree Pruning Both Gini Gain and Information Gain attribute tests can be recursively applied until there are no longer input features available to split the data. This is also known as a “fully grown tree” or an “unpruned tree”. While the terminal leafs may yield a high degree of accuracy in training, trees may grow to epic and complex proportions that have leaf sizes are often times too small to provide accurate and generalizable results. While fully grown trees are considered to have low bias, their out-of-sample performance may be high in variance. There [theoretically] exists some optimal balancing point where trees are complex enough to capture statistical patterns, but are not too complex to yield misleading results. Fortunately, the methodologists who invented decision tree learning have designed two approaches to balance accuracy and generalizability: stopping criteria and pruning. Recall that a leaf is defined as a node with no child nodes. Otherwise stated, a leaf is a terminal node in which no additional attribute testing is conducted – it’s placed out of commission. Stopping criteria are employed to determine if a node should be labeled a leaf during the growing process, thereby stopping tree growth at a given node. These criteria are specified before growing the tree and take on a number of different forms including: A node has fewer records than a pre-specific threshold; The purity or information gain falls below a pre-specified level or is equal to zero; The tree is grown to n-number of levels (e.g. Number of levels of child nodes relative to the root exceeds a certain threshold). While stopping criteria are useful, the results in some studies indicate their performance may be sub-optimal. The alternative approach involves growing a tree to its fullest, then comparing the prediction performance given tree complexity (e.g. number of nodes in the tree) using cross-validation. In the example graph below, model accuracy degrades beyond a certain number of nodes. Thus, optimal number of nodes is defined as when cross-validation samples (e.g. train/test, k-folds) reaches a minimum across samples. Upon finding the optimal number of nodes, the tree is pruned to only that number of nodes. 9.3.3.4 Issues Like any technique, decision trees have strengths and weaknesses: Strengths Weakness - Rules (e.g. all the criteria that form the path from root to leaf) can be directly interpreted. - Data sets with large number of features will have overly complex trees that, if left unpruned, may be too voluminous to interpret. - Method is well-suited to capture interactions and non-linearities in data. - Trees tend to overfitted at the terminal leafs when samples are too small. - Technique can accept both continuous and continuous variables without prior transformation. - Feature selection is conducted automatically 9.3.3.5 In Practice: Decision Trees To put decision trees into practice, we will use the same train and test data frames introduced in the GLM section. There are a number of R implementations of decision trees, the most popular of which is the rpart library: library(rpart) The main function within the library comes with flexible capabilities to grow decision trees: rpart(formula, method, data, cp, minbucket, minsplit) where: formula is a formula object. This can take on a number of forms such as a symbolic description (e.g. \\(y = f(x_1, x_2, ...)\\) is represented as “y ~ x1 + x2”“). method indicates the type of tree, which are commonly either a classification tree “class” or regression tree “anova”. Split criteria can also be custom written. data is the data set in data frame format. cp is a numeric indicates the complexity of the tree. \\(cp = 1\\) is a tree without branches, whereas \\(cp = 0\\) is the fully grown, unpruned tree. If \\(cp\\) is not specified, rpart() defaults to a value of 0.01. minbucket is a stopping criteria that specifies the minimum number of observations in any terminal leaf. minsplit is a stopping criteria that specifies the number of observation in a node to qualify for an attribute test. As a first pass, we’ll run rpart() with the default assumptions. Note that in rpart() automatically conducts k-folds cross-validation for each level of tree growth. If one were to use summary() or str() to check the structure of the output object named fit, the inner workings would likely be found to be quite exhaustive and rather complex. Fortunately, the printcp() method can be used to obtain a summary of the overall model accuracy for tree at different stages of growth. Key features of the printcp() output include: A listing of the variables actually used in construction (note that cit) In the table, CP indicates the tree complexity, nsplit is the number of splits, rel error is the prediction error in the training data, xerror is the cross-validation error, and xstd is the standard error. To choose the best tree, a rule of thumb is to first find the tree with the lowest cross-validation xerror, then find the tree that has the lowest number of splits that is still within one standard deviation xstd of the best tree44. The idea behinds this rule of thumb takes advantage of uncertainty: the true value lies somewhere within a confidence interval, thus any value within a tight confidence interval of the best value is approximately the same. In this first model, the best tree has nsplit = 7 and xerror = 0.542760247714538. By applying the rule, the upper bound of acceptable error is xerror = 0.54276 + 0.00764 = 0.550399764766261. As it turns out, the tree with nsplit = 6 is within one standard deviation and is thus the best model. #Fit decision tree under default assumptions fit &lt;- rpart(coverage ~ agep + wage + cit + mar + schl + esr, method = &quot;class&quot;, data = train) #Tools to review outpu printcp(fit) The model’s learned rules contained in fit can be plotted with plot(), but it takes a bit of work to get the plot into a presentable format. The substitute is using the rpart.plot library, which auto-formats the tree and color codes nodes based on the concentration of the target variable. #Plot library(rpart.plot) rpart.plot(fit, shadow.col=&quot;gray&quot;, nn=TRUE) Figure 9.8: Decision tree using default parameters. While this answer is valid, it should be noted that the CP lower threshold is 0.01, which is the default value. For robustness, we should run the model once more, this time specifying \\(cp = 0\\) to obtain the full, unpruned tree (see below). Applying the error minimization rule once more, the minimum xerror = 0.495429, which corresponds to nsplit = 40. The maximum \\(xerror\\) within one standard deviation is xerror = 0.495429 + 0.007416 = 0.502845, which corresponds to nsplit = 21 with xerror = 0.502802 and cp = 0.000737 #cp = 0 fit.0 &lt;- rpart(coverage ~ agep + wage + cit + mar + schl + esr , method = &quot;class&quot;, data = train, cp = 0) printcp(fit.0) ## ## Classification tree: ## rpart(formula = coverage ~ agep + wage + cit + mar + schl + esr, ## data = train, method = &quot;class&quot;, cp = 0) ## ## Variables actually used in tree construction: ## [1] agep cit esr mar schl wage ## ## Root node error: 6782/13596 = 0.49882 ## ## n= 13596 ## ## CP nsplit rel error xerror xstd ## 1 2.3061e-01 0 1.00000 1.00664 0.0085963 ## 2 1.4258e-01 1 0.76939 0.81598 0.0084465 ## 3 1.6883e-02 2 0.62681 0.63934 0.0080128 ## 4 1.2976e-02 6 0.54364 0.54527 0.0076506 ## 5 9.5842e-03 7 0.53067 0.53657 0.0076119 ## 6 5.9717e-03 8 0.52109 0.52389 0.0075538 ## 7 4.8658e-03 10 0.50914 0.51533 0.0075135 ## 8 2.9490e-03 11 0.50428 0.50944 0.0074852 ## 9 1.9168e-03 12 0.50133 0.50723 0.0074744 ## 10 1.8185e-03 13 0.49941 0.50560 0.0074665 ## 11 1.1059e-03 16 0.49395 0.50413 0.0074593 ## 12 9.3384e-04 18 0.49174 0.50472 0.0074622 ## 13 7.3725e-04 21 0.48894 0.50280 0.0074527 ## 14 6.8810e-04 25 0.48599 0.50088 0.0074433 ## 15 5.8980e-04 28 0.48393 0.50015 0.0074396 ## 16 4.4235e-04 31 0.48172 0.49808 0.0074293 ## 17 3.4405e-04 34 0.48039 0.49646 0.0074212 ## 18 3.3176e-04 40 0.47832 0.49543 0.0074160 ## 19 2.9490e-04 44 0.47700 0.49720 0.0074249 ## 20 2.4575e-04 54 0.47405 0.49912 0.0074345 ## 21 2.2117e-04 58 0.47302 0.49882 0.0074330 ## 22 1.4745e-04 68 0.47066 0.50265 0.0074520 ## 23 1.2287e-04 94 0.46682 0.50973 0.0074866 ## 24 1.1796e-04 100 0.46609 0.51239 0.0074994 ## 25 9.8299e-05 105 0.46550 0.51357 0.0075050 ## 26 8.8469e-05 108 0.46520 0.51489 0.0075114 ## 27 7.3725e-05 118 0.46432 0.51858 0.0075289 ## 28 5.8980e-05 122 0.46402 0.51932 0.0075324 ## 29 4.9150e-05 132 0.46343 0.52197 0.0075449 ## 30 3.6862e-05 141 0.46299 0.52197 0.0075449 ## 31 2.9490e-05 149 0.46270 0.52256 0.0075476 ## 32 2.1064e-05 154 0.46255 0.52374 0.0075531 ## 33 0.0000e+00 161 0.46240 0.52433 0.0075559 At this point, we’ll re-run the decision tree once more with the updated \\(cp\\) value, assign the decision tree object to fit.opt, and plot the resulting decision tree. Notice how the rendered tree is significantly more complex relative to the default and interpretation may be more challenging with a plethora of criteria. fit.opt &lt;- rpart(coverage ~ agep + wage + cit + mar + schl + esr, method = &quot;class&quot;, data = train, cp = opt.select) rpart.plot(fit.opt, shadow.col=&quot;gray&quot;, nn=TRUE) Figure 9.9: Decision tree for optimized complexity. In lieu of a thorough review of the learned rules, we may rely on a measure of variable importance, that is defined as follows: \\[\\text{Variable Importance}_k = \\sum{\\text{Goodness of Fit}_\\text{split, k} + (\\text{Goodness of Fit}_\\text{split,k}\\times \\text{Adj. Agreement}_\\text{split})}\\] Where Variable Importance for variable \\(k\\) is the sum of Goodness of Fit (e.g. Gini Gain or Information Gain) at a given split involving variable k. In otherwords, a variable’s importance is the sum of all the contributions variable \\(k\\) makes towards predicting the target. Below, we can see that the measure can be extracted from the fit.opt object. As it turns out, age is the most important factor. #Extract variable importance list from fit object fit.opt$variable.importance ## agep schl mar cit esr wage ## 928.68854 559.14016 375.75373 271.38939 192.99726 68.11775 Using the plotROC package once again, we calculate the AUC score for each model to assess predictive performance on both the training and test set. One particularly striking difference is the switch in position of the \\(optimal\\) and \\(cp = 0\\) curves: \\(cp = 0\\) is higher in the training set, but are at the approximate safe height in test. This indicates that \\(cp = 0\\) notably overfits, likely to the extra low bias of unpruned leafs. #plotROC library(plotROC) library(gridExtra) #Predict values for train set pred.opt.train &lt;- predict(fit.opt, train, type=&#39;prob&#39;)[,2] pred.0.train &lt;- predict(fit.0, train, type=&#39;prob&#39;)[,2] pred.default.train &lt;- predict(fit, train, type=&#39;prob&#39;)[,2] #Predict values for test set pred.opt.test &lt;- predict(fit.opt, test, type=&#39;prob&#39;)[,2] pred.0.test &lt;- predict(fit.0, test, type=&#39;prob&#39;)[,2] pred.default.test &lt;- predict(fit, test, type=&#39;prob&#39;)[,2] #Set up ROC inputs input.test &lt;- rbind(data.frame(model = &quot;optimal&quot;, d = test$coverage, m = pred.opt.test), data.frame(model = &quot;CP = 0&quot;, d = test$coverage, m = pred.0.test), data.frame(model = &quot;default&quot;, d = test$coverage, m = pred.default.test)) input.train &lt;- rbind(data.frame(model = &quot;optimal&quot;, d = train$coverage, m = pred.opt.train), data.frame(model = &quot;CP = 0&quot;, d = train$coverage, m = pred.0.train), data.frame(model = &quot;default&quot;, d = train$coverage, m = pred.default.train)) #Graph all three ROCs roc.test &lt;- ggplot(input.test, aes(d = d, model = model, m = m, colour = model)) + geom_roc(show.legend = TRUE) + style_roc() + ggtitle(&quot;Test&quot;) roc.train &lt;- ggplot(input.train, aes(d = d, model = model, m = m, colour = model)) + geom_roc(show.legend = TRUE) + style_roc() +ggtitle(&quot;Train&quot;) #Plot grid.arrange(roc.train, roc.test, ncol = 2) Figure 9.10: ROC curves for train and test sets. Lastly, we can extract the AUC statistics using calc_auc(). As multiple AUCs were calculated, we will need to extract the labels for the AUCs from the input file in order to produce a a ‘prettified’ table using xtable. The resulting table below presents the results of the three models that were trained. For all models, we should expect that the training AUC will be greater than the test AUC. This is generally true, but occassionally the test AUC may be greater and is largely a matter of how the data was sampled. Starting from the top of the table: Full grown. The unpruned tree is the most complex model, which means the model has a higher chance of overfitting. This is characterized by an artificially inflated training AUC and a large drop in test AUC. As seen, the AUC drops from 0.88 to 0.826 in the test sample. The unreliable results of an unpruned tree are likely due to the algorithm’s sensitivity to irregular noise at leafs. Optimal. The optimal tree achieves a consistent \\(AUC = 0.83\\) with minimal loss of accuracy as an appropriate level of complexity was precisely tuned. Default. An underfit model will have consistently low performance in both training and testing. As we can see, these patterns are played out in the table below containing AUCs for each the default decision tree, the optimal model complexity and the fully grown tree. As the result of tuning towards an optimal model, we can see that the decision tree yields a marked improvement over the kNN model’s \\(AUC = 0.44\\). For a social science problem, this is considered to be a decent result. #Assemble a well-formatted table tab &lt;- data.frame(model = unique(input.test$model), train = round(calc_auc(roc.train)$AUC,3), test = round(calc_auc(roc.test)$AUC,3)) model train test optimal 0.824 0.824 CP = 0 0.847 0.835 default 0.781 0.780 9.3.4 Random Forests In much of modern data references, we see more uncertainty being characterized. When a hurricane approaches the US Eastern Seaboard, forecasters often map the “cone of uncertainty” that provides the possible range of motion of a storm based on the results of many forecasted simulations. In presidential elections, often times the most polling results are ones that ensemble or average the results of many other similarly conducted polls. The reliance on predictions from a group of models with the same aim may very well improve prediction quality. In statistical learning, average the results of multiple models is known as ensemble learning or ensembling for short. Single models may imposes biases on data and may be well-suited in specific situations. Ensemble methods combine the results of many models to obtain more stable results. For example, the curve in graph #1 can be approximated using a decision tree algorithm. The result of a single tree only loosely fits the curve in a jagged fashion (#2). That one tree may impose biases on the data, perhaps through how the tree is pruned or the assumption that the jagged approximation is appropriate, which may then translate into greater variance in predictions. One could imagine that the structure of that one tree may have happened by chance, and under different situations, the fit could be better. Bootstrapping can help. Recall from elementary statistics that bootstrapping is defined as any statistical process that involves sampling records with replacement. By bootstrapping a sample, we treat a sample like a population, we can expose and characterize the qualities of an estimator under various scenarios already available in the data, which in turn produces an empirical probability distribution for predictions using the estimator. We can bootstrap the decision tree by (1) sampling the data with replacement up to the full size of the sample, then (2) run the decision tree. The result of repeating the process 50 times is (graph #3) produces a result that appears to be more organic and more accurate. This process of bootstrapping and aggregating the results is referred to as bagging. Figure 9.11: Comparison of results of applying a single model to fit a curve versus an ensemble of models. Applying bagging to decision trees may not necessarily be enough to develop a well-balance prediction. In the social sciences and public policy, it is generally assumed that a model’s specification is a choice left to the analyst; However, it may also be a source of methodological bias. Random forests can help. The technique, as crystallized in Breiman (2001), is an extension of decision trees using a modified form of bootstrapping and ensemble methods to mitigate overfitting and bias issues. Not only are individual records bootstrapped, but input features are bootstrapped such that if \\(K\\) variables are in the training set, then \\(k\\) variables are randomly selected to be considered in a model such that \\(k &lt; K\\). Each bootstrap sample is exhaustively grown using decision tree learning and is left as an unpruned tree. The resulting predictions of hundreds of trees are ensembled. The logic is described below. Pseudo-code Let S = training sample, K = number of input features 1. Randomly sample S cases with replacement from the original data. 2. Given K features, select k features at random where k &lt; K. 3. With a sample of s and k features, grow the tree to its fullest complexity. 4. Predict the outcome for all records. 5. Out-Of-Bag (OOB). Set aside the predictions for records not in the s cases. Repeat steps 1 through 5 for a large number of times saving the result after each tree. Vote and average the results of the tree to obtain predictions. Calculate OOB error using the stored OOB predictions. The Out-Of-Bag (OOB) sample is a natural artifact of bootstrapping: approximately one-third of observations are naturally left un-selected, which can be used as the basis of calculating each tree’s error and the overall model error. Think of it as a convenient built in test sample. How about interpretation? Unlike decision trees, it is not a simple task to deduce rules or criteria that describe the target variable. Instead, random forests use variable importance, which, like for a decision tree, measures the contribution of a feature to the homogeneity of a classifier. Unlike decision trees, variable importance for a Random Forest is calculated as the mean decrease in the Gini coefficient of a split relative to the Gini coefficient of the root node. Gini coefficients measures homogeneity on a scale of 0 to 1, where 0 is perfect homogeneity and 1 is perfect heterogeneity. The Gini changes are summed for each variable and normalized. Figure 9.12: Random Forests construct hundreds of trees sampling from both observations and features, then combine the trees into one prediction through voting. 9.3.4.1 Tuning Whereas methods like regression have a closed form solution, Random Forest require tuning as optimal models need to be searched for under different conditions. The principal tuning parameters include: Number of features and number of trees. Number of input features. As \\(k\\) number of parameters need to be selected in each sampling round, the value of \\(k\\) needs to minimize the error on the OOB predictions. Number of trees influences the stability the Variable Importance metric that is commonly used to infer variable influence in decision tree learning. More trees help to stabilize the Variable Importance estimate. To determine the number of trees, keep adding trees to a sample until the OOB error for a randomly select set of trees is approximately equal to that of the ensemble. 9.3.4.2 Random Forests in Practice Like decision trees, much of Random Forests rely on easy to use methods made available through the randomForest library. There are a couple of ways to run the algorithm, including: randomForest(formula, data, method, mtry, ntree) where: - formula is an object containing the specification to be estimated. Note that - data is a data frame. - mtry is the number of variables to be randomly sampled per iteration. Default is \\(\\sqrt{k}\\) for classification trees. - ntree is the number of trees. Default is 500. Using the same formula as the rpart() function, we can train a naive Random Forest and check the OOB error. Approximately 75.6% of observations in the OOB sample were correctly classified using 2 randomly selected variables in each of the 500 trees. #Load randomForest library library(randomForest) #Run Random Forest spec &lt;- as.formula(&quot;coverage ~ agep + wage + cit + mar + schl + esr&quot;) fit.rf &lt;- randomForest(spec, data = train, mtry = 2, ntree = 500) #Check OOB error fit.rf ## ## Call: ## randomForest(formula = spec, data = train, mtry = 2, ntree = 500) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 24.43% ## Confusion matrix: ## Coverage No Coverage class.error ## Coverage 4958 1856 0.2723804 ## No Coverage 1466 5316 0.2161604 Using the importance() method, we can see the Mean Decrease Gini, which calculates the mean of Gini coefficients. agep has the largest value of 801.3155193, indicating that age is the best predictor of coverage; However, the values themselves do not have any meaning outside of a comparison with other Gini measures. importance(fit.rf) ## MeanDecreaseGini ## agep 801.31552 ## wage 77.79932 ## cit 321.67357 ## mar 446.41256 ## schl 444.07750 ## esr 234.16484 By default, the randomForests library sets the number of trees to equal 500. By plotting the fit object, we can see how OOB error and the confidence interval converges asymptotically as more trees are added to the ensemble. Otherwise stated, more trees will help up to a certain point and the default is likely more than enough. plot(fit.rf) As we know that \\(n = 500\\) trees is more than enough, we will now need to tune the tree for the number of variables. To tune the algorithm, we will use the tuneRF() method. The method searches for the optimal number of variables per split by incrementally adding variables. While it’s a useful function, it is relatively verbose. In addition to the target and input features, a number of other parameters need to be specified: tuneRF(x, y, ntreeTry, mtryStart, stepFactor, improve, trace, plot) where: - x is a data frame or matrix of input features. - ntreeTry is the number of trees used in each iteration of tuning. - mtryStart is the number of variables to start. - stepFactor is the number of additional variables tested per iteration. - improve is the minimum relative improvement in OOB error for the search to go on. - trace is a boolean that indicates where to print the search progress. - plot is a boolean that indicates whether to plot the search results. Below, we conduct a search from mtryStart = 1 with a stepFactor = 2. The search result indicates that 2 variables per split are optimal. #Search for most optimal number of input features fit.tune &lt;- tuneRF(x = train[,3:ncol(train)], y = train[,2], ntreeTry = 500, mtryStart = 1, stepFactor = 2, improve = 0.001, plot = TRUE) ## mtry = 1 OOB error = 26.53% ## Searching left ... ## Searching right ... ## mtry = 2 OOB error = 24.37% ## 0.08123094 0.001 ## mtry = 4 OOB error = 25% ## -0.02564876 0.001 Figure 9.13: Random Forest tuning result (m = number of features, OOB Error = out of sample error). #Extract best parameter tune.param &lt;- fit.tune[fit.tune[, 2] == min(fit.tune[, 2]), 1] Using the optimal result, we can plug back into the randomForest() method and re-run. However, as the default model already has the same parameters as the optimal model, we can proceed to calculating the model accuracy. Comparing the training and test models for the Random Forest algorith, we see a large drop in the AUC between train and test, indicating quite a bit of overfitting. #plotROC library(plotROC) #Predict values for train set pred.rf.train &lt;- predict(fit.rf, train, type=&#39;prob&#39;)[,2] #Predict values for test set pred.rf.test &lt;- predict(fit.rf, test, type=&#39;prob&#39;)[,2] #Set up ROC inputs input.rf &lt;- rbind(data.frame(model = &quot;train&quot;, d = train$coverage, m = pred.rf.train), data.frame(model = &quot;test&quot;, d = test$coverage, m = pred.rf.test)) #Graph all three ROCs roc.rf &lt;- ggplot(input.rf, aes(d = d, model = model, m = m, colour = model)) + geom_roc(show.legend = TRUE) + style_roc() +ggtitle(&quot;Train&quot;) #AUC calc_auc(roc.rf) ## PANEL group AUC ## 1 1 1 0.8374157 ## 2 1 2 0.8316592 9.3.5 Support Vector Machines Logistic regression is a probabilistic approach. The linear formulation allows for ease of interpretation and is thus a technique of choice in many fields for general applications. But the predictive accuracy may be a whole magnitude lower relative to other methods. Support Vector Machines, on the other hand, take a purely geometric approach to classification. The technique often yields relatively higher accuracy, at the expense of interpretation. For technical tasks that involve organic relationships such as computer vision or genetic research, SVMs are particularly adept at pattern recognition and classification. It should be noted that it is due to the highly mathematical nature of SVMs in addition to the computational requirements that the technique is typically used for tasks where social interpretation is not required. Building upon the same three feature dataset once more, let’s assume this time when data are plotted, there is a clear gap between groups such that a straight line can partition one group from the other (see panel (1) below). A line as simple as \\(wx + b = y\\) may do the trick in two dimensional space, but can also be described as a plane in n-dimensional space \\(w^T + b = y\\). That line may then serve as a boundary between the two groups where \\(w^T + b &gt; y\\) may describe the group above the boundary and \\(w^T + b &lt; y\\) describes the group below. Given the space, however, you realize that multiple lines could do the job: there are almost infinite lines (see panel (2) below) that could serve as the boundary between the groups. But which is the best? There should, in theory, be one line that optimally describes the separation between the groups. Figure 9.14: (1) A two class data set in two dimensional space with a clear gap between classes. (2) Numerous possible decision boundaries in a two class data set. 9.3.5.1 Classification If we are to assume a straight line is appropriate, we can find a line that maximizes the distance between the groups. To intuit distance requires defining points of reference. Let’s then assume that there exists two parallel planes: each sits at the edge of each respective group and the space, labeling the top plane as \\(y = +1\\) and bottom plane as \\(y = -1\\). As seen in Figure 3, the dashed grey lines and the solid purple lines are hyperplanes, but are simply lines in two dimensional space. H1 (\\(y = +1\\)) and H2 (\\(y = -1\\)) are hyperplanes that are defined by a set of “support vectors” – points that serve as control or reference points for the location of the hyperplane (see Figure 4). The elegance of this method is that not all points in a dataset are used to define H1 and H2: only select points on or near the hyperplanes are required to define the plane. These planes are defined using simple linear equations shown in dot-product form: \\[w^T x - b = +1\\] and \\[w^T x - b = -1\\] for H2, where \\(w\\) is a weight that needs to be calibrated. H1 and H2 primarily serve as the boundaries of what is known as the margin, or the space that maximally separates the two classes that are linearly separable The optimal hyperplane or decision boundary is defined as \\[w^T x - b = 0\\] and sits at a distance of \\(d+\\) from H1 and \\(d-\\) from H2. When H1, H2, and the decision boundary are determined through training, scoring essentially maps where a new record falls in the decision space. A point to the left of H1 is scored as \\(+1\\) and to the right of H2 is \\(-1\\). Note that thus far, a point that falls in between H1 and H2 is not considered. Figure 9.15: (3) Two hyperplanes (H1 and H2) flank the decision boundary. (2) Hyperplanes, including the decision boundary, are defined by support vectors (green points). 9.3.5.2 Learning Function To tune a SVM, we want to find the maximum distance between H1 and H2. This can be done by finding the distance of the line that is perpendicular to H1 and H2 since they are parallel. The following equations are the points at which the perpendicular line intersects at two points: \\[w^T_1 + b =1\\] and \\[w^T_2 + b =-1\\] By subtract the two equations, we obtain \\(w^T(x_1 - x_2) = 2\\), which then can be manipulated by dividing the normalized \\(w\\) vector \\(||w||\\) of the weights. This yields a distance formula for the margin: \\[\\text{margin} = x_1 - x_2 = \\frac{2}{||w||}\\] To maximize the margin in its current form may be challenging and is typically reformulated as a minimization problem that can be solved using quadratic programming: \\[min \\frac{1}{2}||w||^2\\] subject to \\(y_i(w^Tx_i+b) \\geq 1\\) for all records in the sample. Like gradient descent and Newton Raphson, these are problems that have standard implementations that are pre-packaged in R in the e1071 library. For the sake of exposure, the learning function for \\(w\\) is maximized using the following formulation: \\[w(\\alpha) = \\sum_i{\\alpha_i} - \\frac{1}{2}\\sum_i{\\alpha_1\\alpha_0 y_1 y_0 x_1^Tx_0}\\] subject to \\(\\alpha_i \\geq 0\\) (non-negatives), \\(\\sum_i{\\alpha_i y_i} = 0\\) (sum of alpha and y are equal to zero). Otherwise stated: the equation is the sum of all points \\(i\\) minus the product of alphas, labels, values. \\(\\alpha\\) are parameters that are being tuned in order to maximize \\(w\\). An interesting observation of this formula is that since the hyperplanes H1 and H2 sit on the edge of their respective groups, the hyperplane will only intersect with only a few records or “vectors”. Mathematically, many of the \\(\\alpha\\) values will be zero. Intuitively, that means that the optimization equation will retain only a fraction of the total vectors to support the calculation on the plane. This is the origin of the name of the method: only vectors that support the planar calculation are retained. Upon maximizing \\(w\\), a vector of \\(w\\) containing the weights associated with each feature can be extracted \\[w = \\sum_i^N{\\alpha_i y_i x_i + b}\\] which in turn can be used to solve a planar equation to find the corresponding value of \\(b\\) to define the plane. While there are weights in this method, they are not directly interpretable in the way as logistic regression, but the magnitude of the underlying weights correspond to the importance of each feature. 9.3.5.3 In Actuality The first example provided is what is know as a hard margin, where classes are linearly separable. In actuality, most classification problems do not have a clear margin between classes, meaning that there may be points that are misclassified or lie in the margin. A soft margin formulation is more commonly used to handle cases where there is some fuzziness in the separation: the margin must be determined allowing for misclassification of points. We can characterize the position of challenging-to-classify points using slack variables, or a variable \\(\\xi\\) that represents distance from the margin to a point. Figure 6 illustrates a number of commonly observed scenarios: The green points are the support vectors that sit on H1 and H2, which are \\(\\xi = 0\\). The distance from each H2 and H1 to the decision boundary is \\(\\frac{1}{||w||}\\). The large gold points sit between H0 and H2 such that \\(0 \\leq \\xi \\leq \\frac{1}{||w||}\\). While they still are correctly classified (correct side of the decision boundary), the points sit within the margin. These points are referred to as margin violations. The large blue point is a misclassified record as it is to the left of H1, but should be to the right of H2. In terms of slack distance, \\(\\xi &gt; \\frac{2}{||w||}\\) as its distance from the correct hyperplane is greater than the width of the margin. Figure 9.16: Soft margin SVMs allow some margin violations in order to fit the hyperplanes. What does this mean for optimizing the margin? The slack variables need to be accounted for in the optimization of \\(||w||\\): \\[min \\frac{1}{2}||w||^2 + C\\sum_i^N{\\xi_i}\\] subject to \\(y_i(w^Tx_i+b) \\geq 1 - \\xi_i\\) for all records in the sample. The first half of the formula is the same as the hard margin formula. The second half adds a constraint where the new variable \\(C\\) is known as a regularization variable or the Cost. If \\(C\\) is small, the slack variables are ignored and thus allows for larger margins. If \\(C\\) is large, then the slack variables reduce the size of the margin. It is worth noting that \\(C\\) is one of two tuning parameters that data scientists will need to calibrate when running SVMs. 9.3.5.4 Extending the hypothesis space So far, the examples have focused on linear problems with hard and soft margins in two dimensional space. What if classes are clearly separated in a parabolic (1) or circular pattern (2)? A parabolic separation between classes can be described in terms of polynomials (e.g. \\(y = x^2\\)). A circular pattern may actually be separable if points are projected into higher dimensional space. Moving from two-dimensions (2) to three-dimensions (3), the contour lines demonstrate that there may be some threshold of the third feature at which a hyperplane can separate the two classes. The projection of records into higher dimensional space to improve separability is known as the kernel trick. Figure 9.17: Scenarios for which a hyperplane separates two class targets. In a paper by Boser et al. (1992) modified the maximization function: \\[w(\\alpha) = \\sum_i{\\alpha_i} - \\frac{1}{2}\\sum_i{\\alpha_1\\alpha_0 y_1 y_0 x_1^Tx_0}\\] such that the dot products \\(x_1^Tx_0\\) are replaced with non-linearkernel functions. Of particular significance are two common kernels: the Gaussian Radial Basis Function (RBF) and Polynomial kernels. RBP is defined as: \\[RBF = exp(-\\gamma ||x_1-x_0||^2)\\] where \\(\\gamma = \\frac{1}{2\\sigma^2}\\) and \\(\\sigma\\) &gt;0. The value of \\(\\gamma\\) determines the tightness of the kernel, where larger values of \\(\\gamma\\) yield a compact, tight kernel whereas smaller values of \\(\\gamma\\) are associated with wider-spread kernels. In R, the value of \\(\\gamma\\) is one of the tuning parameters that a data scientist would need to specify as RBFs are the default kernel. Note that one needs to use a grid search to find the appropriate value of \\(\\gamma\\) as it cannot be mathematically optimized, but rather analyzed. The polynomial kernel is defined as: \\[Polynomial = (1+x_1^Tx_0)^d\\] where the value of \\(d &gt; 0\\), indicates the polynomial degree, and assumes that all polynomials from 1 to \\(d\\) are included. 9.3.5.5 Practical Details After all the derivation is done, keep the following points in mind when applying SVMs: Tuning is centered on two variables: \\(C\\) to manage the extent to which the margin is hard or soft, and \\(\\gamma\\) for when a RBF is applied. Note that the quantities of each are tuned using cross-validation in the form of a grid search (e.g. test multiple values at equal intervals). Non-linear SVMs are computationally expensive. Very high dimensional data sets will likely take a long time to compute. SVMs are particularly well-suited for a pattern recognition, computer vision among other computationally challenging problems. While they may yield more accurate results than many other classifiers, the ability for data scientists to give social policy decision makers control over the story is limited. ROC and AUC may at times be challenging to calculate for SVM results. An alternative is to utilize the F1 statistic defined as: \\[F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\\] 9.3.5.6 Applying SVMs SVMs are neatly packaged into an interface library called e1071. The library contains a suite of machine learning tools in addition to SVMs. library(e1071) Syntax is fairly simple and requires a minimum, six parameters are required: svm(formula, data, cost, gamma, kernel) where: formula specifies a specification to be estimated. data is a data frame. C is the regularization parameter which needs to be grid searched. Default = 1. g is a parameter used for RBF. kernel is string value that indicates the kernel to be used, which may be one of the four types: “linear”, “polynomial”, “radial”, and “sigmoid”. Default = radial basis. To start, we will fit an SVM using a “*radial“” kernel assuming \\(cost = 1\\) and \\(gamma = \\frac{1}{\\text{dim}}\\), where \\(dim\\) is the number of effective variables in our data (e.g. continuous variables, expanded dummies, and intercept). This effectively is 18 in the health data. spec &lt;- as.formula(&quot;coverage ~ agep + wage + cit + mar + schl + esr&quot;) svm_rbf_fit &lt;- svm(spec, data=train, kernel = &quot;radial&quot;, cost = 1, gamma = 0.05555) Typically, it is a good idea to test various values of cost and gamma, though noting that this process for SVMs is computationally expensive (takes a long time), especially for RBF kernels. The e1071 library provides a method tune.svm() to find the best cost and gamma (see below). In this example, we will manually tune to develop a sense of how calibration works in practice. tune &lt;- tune.svm(spec , data = train, kernel = &quot;linear&quot;, cost=10^(-1:2), gamma=c(.5,1,2)) To determine search for the best parameters, we will conduct a grid search: a combination of four values of cost and four values of gamma will be tested for a total of 16 models. We choose equally spaced values on on a quadratic scale (e.g. \\(0.01\\), \\(1\\), \\(10\\)) to emphasize differences in model fit. To evaluate accuracy, we will rely on the F1-scores #F1 score meanF1 &lt;- function(actual, predicted){ # Mean F1 score function # # Args: # actual = a vector of actual labels # predicted = predicted labels # # Returns: # F1 score classes &lt;- unique(actual) results &lt;- data.frame() for(k in classes){ results &lt;- rbind(results, data.frame(class.name = k, weight = sum(actual == k)/length(actual), precision = sum(predicted == k &amp; actual == k)/sum(predicted == k), recall = sum(predicted == k &amp; actual == k)/sum(actual == k))) } results$score &lt;- results$weight * 2 * (results$precision * results$recall) / (results$precision + results$recall) return(sum(results$score)) } # Prep grid search parameters cost_vec &lt;- 10^(-1:2) gamma_vec &lt;- 2^(seq(-5, 2, 2)) combo &lt;- expand.grid(cost = cost_vec, gamma = gamma_vec) # Create 10-folds of random partitions # Create index for rows in train set fold &lt;- rep(seq(1,10), ceiling(nrow(train)/10))[1:nrow(train)] # Randomly reorder fold set.seed(10) fold &lt;- fold[order(runif(nrow(train)))] #Run 10-folds cross validation while tuning gamma and cost parameters cv_results &lt;- data.frame() for(p in unique(fold)){ for(i in 1:nrow(combo)){ #Fit SVM on 1 to k-1 fit &lt;- svm(spec, data = train[fold != p, ], kernel = &quot;radial&quot;, cost = combo[i, 1], gamma = combo[i, 2]) #Predict on kth fold pred &lt;- predict(fit, train[fold == p, ]) cv_results &lt;- rbind(cv_results, data.frame(fold = p, cost = combo[i, 1], gamma = combo[i, 2], mean.f1 = meanF1(train$coverage[fold == p], pred))) } } #View table combo &lt;- aggregate(list(mean.f1 = combo$mean.f1), by = list(cost = combo$cost, gamma = combo$gamma), FUN = mean) print(combo) Cost Gamma Mean.F1 0.1 0.0312 0.7263 1.0 0.0312 0.7412 10.0 0.0312 0.7377 100.0 0.0312 0.7411 0.1 0.1250 0.7398 1.0 0.1250 0.7413 10.0 0.1250 0.7494 100.0 0.1250 0.7502 0.1 0.5000 0.7414 1.0 0.5000 0.7511 10.0 0.5000 0.7499 100.0 0.5000 0.7502 0.1 2.0000 0.7372 1.0 2.0000 0.7500 10.0 2.0000 0.7502 100.0 2.0000 0.7473 Based on the grid search, we find that the best model has a \\(C = 1\\) and \\(gamma = 0.5\\). We then train a model with those parameters, then predict the classes of the test set to find that a \\(F1 = 0.7511\\). #Predict labels pred_test &lt;- svm(spec, data = train, kernel = &quot;radial&quot;, cost = 1, gamma = 0.5) pred_rbf &lt;- predict(pred_test, test) #examine result table(pred_rbf) ## pred_rbf ## Coverage No Coverage ## 6564 7222 ##RBF meanF1(test$coverage, pred_rbf) ## [1] 0.7551924 9.4 DIY Hastie et. al (2001)↩ "]
]
