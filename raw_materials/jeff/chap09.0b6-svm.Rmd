--- 
title: "Data Science + Public Policy"
author: "Jeff Chen"
date: '`r Sys.Date()`'
output: pdf_document
description: Chapter 11
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
---

### Support Vector Machines 

Logistic regression is a probabilistic approach. The linear formulation allows for ease of interpretation and is thus a technique of choice in many fields for general applications. But the predictive accuracy may be a whole magnitude lower relative to other methods. Support Vector Machines, on the other hand, take a purely geometric approach to classification. The technique often yields relatively higher accuracy, at the expense of interpretation. For technical tasks that involve organic relationships such as computer vision or genetic research, SVMs are particularly adept at pattern recognition and classification. It should be noted that it is due to the highly mathematical nature of SVMs in addition to the computational requirements that the technique is typically used for tasks where social interpretation is not required.

Building upon the same three feature dataset once more, let's assume this time when data are plotted, there is a clear gap between groups such that a straight line can partition one group from the other (see panel (1) below). A line as simple as $wx + b = y$ may do the trick in two dimensional space, but can also be described as a plane in n-dimensional space $w^T + b = y$. That line may then serve as a boundary between the two groups where $w^T + b > y$ may describe the group above the boundary and $w^T + b < y$ describes the group below. 

Given the space, however, you realize that multiple lines could do the job: there are almost infinite lines (see panel (2) below) that could serve as the boundary between the groups. But which is the best? There should, in theory, be one line that optimally describes the separation between the groups. 

```{r, echo = FALSE, message = FALSE, warning= FALSE}
#Margin Example
margin_size <- 0.3
set.seed(123)
df <- data.frame(x = runif(200),
                 y = runif(200),
                 supports = NA)

#Set up margin supports
  supports <- data.frame( x = c(0.6, 0.7, 0.7), y = NA, supports = NA)
  supports$supports[1:2] <- -1.08 + 2*supports$x[1:2]
  supports$supports[3] <- -.52 + 2*supports$x[3]
  
df <- rbind(df,
            supports)
  
  
#Best boundary
  df$z <- -0.8 + df$x*2 
  df$perp <- 0.6578033 + df$x*-0.5
  df$perp[df$x >= 0.6951213] <- NA
  df$perp[df$x <= 0.4711213] <- NA
  
#Cut out
  df <- df[which((df$y > df$z + margin_size | df$y < df$z - margin_size | !is.na(df$supports))), ]
  df$group <- "Side A"
  df$group[df$y < df$z - margin_size] <- "Side B"
  df$cols <- "blue"
  df$cols[df$group == "Side B"] <- "green"
  
  
#Alternative boundaries
  df$z1 <- -1.1 + df$x*2.1
  df$z2 <- -0.5 + df$x*1.9
  df$z3 <- -0.95 + df$x*2  
  df$z4 <- -0.65 + df$x*2  
  df$z5 <- -0.95 + df$x*2.3
  df$z6 <- -0.65 + df$x*1.7
  
  df$margin2 <- -1.08 + df$x*2
  df$margin1 <- -.52 + df$x*2
  
  df <- df[order(df$perp),]
  
#Plot
library(ggplot2)

base <- ggplot(df, aes(group=factor(group))) + 
    geom_point(aes(x = x, y = y,  colour = factor(group)))  +
    ylim(0,1) + xlim(0,1) + 
    ylab("x1") + xlab("x2") +
    ggtitle("(1)") + scale_colour_manual(values=c("lightblue", "gold")) +
  coord_fixed(ratio = 1) +
    theme(plot.title = element_text(size = 10), 
          axis.line=element_blank(),
          axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))
  
options1 <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z1), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z2), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z3), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z4), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z5), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z6), alpha = 0.5, colour = "grey") + 
  ylim(0,1) + xlim(0,1) +
  ggtitle("(2)") +  scale_colour_manual(values=c("lightblue", "gold")) +
  coord_fixed(ratio = 1) + 
  ylab("x1") + xlab("x2") +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))


optimal <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "darkgrey") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "darkgrey") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "darkgrey") + 
  geom_line(aes(x = x, y = perp), size = 0.5, colour = "black") + 
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(3)") +  scale_colour_manual(values=c("lightblue", "gold")) +
  coord_fixed(ratio = 1) +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) + 
  annotate("text", x = .75, y = .9, label = "H1", colour = "blue", size = 3) + 
  annotate("text", x = .93, y = .85, label = "H2", colour = "blue", size = 3) + 
  annotate("text", x = .52, y = .45, label = "d+", colour = "black", size = 3) + 
  annotate("text", x = .68, y = .37, label = "d-", colour = "black", size = 3) + 
  annotate("text", x = .2, y = .1, label = "wx-b = +1", colour = "black", size = 3)  + 
  annotate("text", x = .39, y = .2, label = "wx-b = 0", colour = "black", size = 3) + 
  annotate("text", x = .75, y = .1, label = "wx-b = -1", colour = "black", size = 3) 

  
supports <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "darkgrey") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "darkgrey") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "darkgrey") +
  geom_point(aes(x = x, y = supports, colour = "red", size=0.7)) +
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(4)") +  scale_colour_manual(values=c("lightblue", "gold", "darkgreen")) +
  coord_fixed(ratio = 1) +theme_bw() +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) 
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "(1) A two class data set in two dimensional space with a clear gap between classes. (2) Numerous possible decision boundaries in a two class data set."}
library(gridExtra)
grid.arrange(base, options1, ncol = 2)

```

#### Classification
If we are to assume a straight line is appropriate, we can find a line that maximizes the distance between the groups. To intuit distance requires defining points of reference. Let's then assume that there exists two parallel planes: each sits at the edge of each respective group and the space, labeling the top plane as $y = +1$ and bottom plane as $y = -1$. As seen in Figure 3, the dashed grey lines and the solid purple lines are *hyperplanes*, but are simply lines in two dimensional space. H1 ($y = +1$) and H2 ($y = -1$) are hyperplanes that are defined by a set of "support vectors" -- points that serve as control or reference points for the location of the hyperplane (see Figure 4).  The elegance of this method is that not all points in a dataset are used to define H1 and H2: only select points on or near the hyperplanes are required to define the plane. These planes are defined using simple linear equations shown in dot-product form:  $$w^T x - b = +1$$  and $$w^T x - b = -1$$ for H2, where $w$ is a weight that needs to be calibrated.  H1 and H2 primarily serve as the boundaries of what is known as the *margin*, or the space that maximally separates the two classes that are linearly separable The optimal hyperplane or *decision boundary* is defined as $$w^T x - b = 0$$ and sits at a distance of $d+$ from H1 and $d-$ from H2.

When H1, H2, and the decision boundary are determined through training, scoring essentially maps where a new record falls in the decision space. A point to the left of H1 is scored as $+1$ and to the right of H2 is $-1$. Note that thus far, a point that falls in between H1 and H2 is not considered.


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "(3) Two hyperplanes (H1 and H2) flank the decision boundary. (2) Hyperplanes, including the decision boundary, are defined by support vectors (green points)."}
grid.arrange(optimal, supports, ncol = 2)
```

#### Learning Function
To tune a SVM, we want to find the maximum distance between H1 and H2. This can be done by finding the distance of the line that is perpendicular to H1 and H2 since they are parallel.  The following equations are the points at which the perpendicular line intersects at two points:  $$w^T_1 + b =1$$ and $$w^T_2 + b =-1$$ 

By subtract the two equations, we obtain $w^T(x_1 - x_2) = 2$, which then can be manipulated by dividing the normalized $w$ vector $||w||$ of the weights. This yields a distance formula for the *margin*:

$$\text{margin} = x_1 - x_2 = \frac{2}{||w||}$$ 

To maximize the margin in its current form may be challenging and is typically reformulated as a minimization problem that can be solved using quadratic programming: $$min \frac{1}{2}||w||^2$$ subject to $y_i(w^Tx_i+b) \geq 1$  for all records in the sample. Like gradient descent and Newton Raphson, these are problems that have standard implementations that are pre-packaged in R in the `e1071` library.

For the sake of exposure, the learning function for $w$ is maximized using the following formulation:

$$w(\alpha) = \sum_i{\alpha_i} - \frac{1}{2}\sum_i{\alpha_1\alpha_0 y_1 y_0 x_1^Tx_0}$$

subject to $\alpha_i \geq 0$ (non-negatives), $\sum_i{\alpha_i y_i} = 0$ (sum of alpha and y are equal to zero). Otherwise stated: the equation is the sum of all points $i$ minus the product of alphas, labels, values. $\alpha$ are parameters that are being tuned in order to maximize $w$. An interesting observation of this formula is that since the hyperplanes H1 and H2 sit on the edge of their respective groups, the hyperplane will only intersect with only a few records or "vectors". Mathematically, many of the $\alpha$ values will be zero. Intuitively, that means that the optimization equation will retain only a fraction of the total vectors to support the calculation on the plane. This is the origin of the name of the method: only vectors that support the planar calculation are retained.

Upon maximizing $w$, a vector of $w$ containing the weights associated with each feature can be extracted 
$$w = \sum_i^N{\alpha_i y_i x_i + b}$$ 
which in turn can be used to solve a planar equation to find the corresponding value of $b$ to define the plane. While there are weights in this method, they are not directly interpretable in the way as logistic regression, but the magnitude of the underlying weights correspond to the importance of each feature.

#### In Actuality
The first example provided is what is know as a *hard margin*, where classes are linearly separable. In actuality, most classification problems do not have a clear margin between classes, meaning that there may be points that are misclassified or lie in the margin. A *soft margin* formulation is more commonly used to handle cases where there is some fuzziness in the separation: the margin must be determined allowing for misclassification of points.  We can characterize the position of challenging-to-classify points using *slack variables*, or a variable $\xi$ that represents distance from the margin to a point. 

Figure 6 illustrates a number of commonly observed scenarios: 

- The green points are the support vectors that sit on H1 and H2, which are $\xi = 0$. 
- The distance from each H2 and H1 to the decision boundary is $\frac{1}{||w||}$.  
- The large gold points sit between H0 and H2 such that $0 \leq \xi \leq \frac{1}{||w||}$. While they still are correctly classified (correct side of the decision boundary), the points sit within the margin. These points are referred to as *margin violations*.
- The large blue point is a misclassified record as it is to the left of H1, but should be to the right of H2. In terms of slack distance, $\xi > \frac{2}{||w||}$ as its distance from the correct hyperplane is greater than the width of the margin.


```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Soft margin SVMs allow some margin violations in order to fit the hyperplanes."}
#Margin Example
margin_size <- 0.3
set.seed(123)
df <- data.frame(x = runif(1000),
                 y = runif(1000),
                 supports = "",
                 size = 0.3)

#Set up margin supports
supports <- data.frame( x = c(0.55, 0.71, 0.5), 
                        y = c(0.14, 0.5, 0.75), 
                        supports = c("margin violation","margin violation","Misclass"),
                        size = 0.6)
df <- rbind(df,
            supports)


supports <- data.frame( x = c( 0.7, 0.7), 
                        y = NA, 
                        supports = "support",
                        size = 0.6)
supports$y[1] <- -1.08 + 2*supports$x[1]
supports$y[2] <- -.52 + 2*supports$x[2]

df <- rbind(df,
            supports)



#Best boundary
df$z <- -0.8 + df$x*2

#Perp to misclass
df$perp <- 1 + df$x*-0.5
df$perp[df$x >= 0.83] <- NA
df$perp[df$x <= 0.5] <- NA

df$vio1 <- 0.85 + df$x*-0.5
df$vio1[df$x >= 0.78] <- NA
df$vio1[df$x <= 0.7] <- NA

df$vio2 <- 0.42 + df$x*-0.5
df$vio2[df$x >= 0.6] <- NA
df$vio2[df$x <= 0.55] <- NA

df <- df[c(1:200, (nrow(df)-5):nrow(df)),]

#Cut out
df <- df[which(df$y > df$z + margin_size | df$y < df$z - margin_size | df$supports != ""), ]
df$group <- "Side A"
df$group[df$y < df$z - margin_size] <- "Side B"
df$group[df$supports == "margin violation"] <- "Margin Vio"
df$group[df$supports == "Misclass"] <- "Misclass"
df$group[df$supports == "support"] <- "Support"

#margin
df$margin2 <- -1.08 + df$x*2
df$margin1 <- -.52 + df$x*2

#sort
df <- df[order(df$perp),]

#Plot
library(ggplot2)

value <- 0
labxi <- list(bquote(xi==.(value)))



optimal2 <- ggplot(df, aes(x = x, y = y, group = factor(group),label = supports)) + 
  ylim(0,1) + xlim(0,1) +  scale_colour_manual(values=c("gold", "gold","lightblue", "gold","darkgreen")) + ggtitle("(6) Soft Margin Issues") +
  geom_line(aes(x = x, y = z), size = 1, colour = "darkgrey") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "grey") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "grey") + 
  geom_line(aes(x = x, y = perp), size = 0.4,  colour = "navy") + 
  geom_line(aes(x = x, y = vio1), size = 0.4,  colour = "navy") + 
  geom_line(aes(x = x, y = vio2), size = 0.4,  colour = "navy") + 
  geom_point(aes(x = x, y = y, colour = factor(group), size = size)) +
  geom_text(hjust = 0, nudge_x = 0.05, size = 3)+
  ylim(0,1) + xlim(-.20,1.2)  + coord_fixed(ratio = 1) +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) + 
  annotate("text", x = .21, y = .05, label = "H1", colour = "blue") + 
  annotate("text", x = .7, y = .05, label = "H2", colour = "blue") 

optimal2
 


```

What does this mean for optimizing the margin? The slack variables need to be accounted for in the optimization of $||w||$:

$$min \frac{1}{2}||w||^2 + C\sum_i^N{\xi_i}$$

subject to $y_i(w^Tx_i+b) \geq 1 - \xi_i$  for all records in the sample. The first half of the formula is the same as the hard margin formula. The second half adds a constraint where the new variable $C$ is known as a regularization variable or the *Cost*. If $C$ is small, the slack variables are ignored and thus allows for larger margins. If $C$ is large, then the slack variables reduce the size of the margin. It is worth noting that $C$ is one of two tuning parameters that data scientists will need to calibrate when running SVMs.


####Extending the hypothesis space
So far, the examples have focused on linear problems with hard and soft margins in two dimensional space. What if classes are clearly separated in a parabolic (1) or circular pattern (2)? A parabolic separation between classes can be described in terms of polynomials (e.g. $y = x^2$). A circular pattern may actually be separable if points are projected into higher dimensional space. Moving from two-dimensions (2) to three-dimensions (3),  the contour lines demonstrate that there may be some threshold of the third feature at which a hyperplane can separate the two classes. The projection of records into higher dimensional space to improve separability is known as the _kernel trick_.
 

```{r, message = FALSE, warning = FALSE, fig.height = 2.5, echo=FALSE, fig.cap = "Scenarios for which a hyperplane separates two class targets."}
#Margin Example
margin_size <- 0.3
set.seed(123)

#Parabola
  df <- data.frame(x = runif(300) - runif(300) ,
                   y = runif(300) - runif(300) ,
                   supports = "",
                   size = 0.3)
  
  df$z <- -0.2 + 2*df$x^2  
 
  df$group <- "Group 1"
  df$group[df$y > df$z] <- "Group 2"


#Parabola graph
para <- ggplot(df, aes(x = x, y = y, group = factor(group),label = supports)) + 
  geom_point(aes(x = x, y = y, colour = group)) + 
  ylim(0,1) + xlim(0,1) +  scale_colour_manual(values=c("lightblue","gold")) +
  geom_line(aes(x = x, y = z), colour = "darkgrey") +
  geom_text(hjust = 0, nudge_x = 0.05, size = 3)+
  ylim(-1,1) + xlim(-1,1)  + coord_fixed(ratio = 1) +
  ggtitle("(1) Parabola") +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) 

#Circle 

  df <- data.frame(x = runif(10000) - runif(10000) ,
                   y = runif(10000) - runif(10000) ,
                   supports = "",
                   size = 0.3)
  
  df$zp <- sqrt(0.05 - df$x^2) 
  df$zm <- -sqrt(0.05 - df$x^2) 
  
  df$zp2 <- sqrt(0.1 - df$x^2) 
  df$zm2 <- -sqrt(0.1 - df$x^2) 
  
  df$heights <- 0.1
  df$heights[df$y > df$zm2 & df$y < df$zp2] <- 0.3
  df$heights[df$y > df$zm & df$y < df$zp] <- 0.8
  df$heights <- df$heights + runif(nrow(df))
  
  
  
  df$group <- "Group 1"
  df$group[df$y > df$zm & df$y < df$zp ] <- "Group 2"
  
  circ <- ggplot(df, aes(x = x, y = y, group = factor(group))) + 
    geom_point(aes(x = x, y = y, colour = factor(group))) + 
    ylim(0,1) + xlim(0,1) +  scale_colour_manual(values=c("lightblue","gold")) +
    geom_line(aes(x = x, y = zm), colour = "darkgrey") +
    geom_line(aes(x = x, y = zp), colour = "darkgrey") +
    ylim(-1,1) + xlim(-1,1)  + coord_fixed(ratio = 1) +
    ggtitle("(2) Circle") +
    theme(plot.title = element_text(size = 10), 
          axis.line=element_blank(),
          axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank(),
          plot.margin=unit(c(-0.5,1,1,1), "cm")) 
  
  library(dplyr)
  ds <- tbl_df(df)
  circ2 <- ggplot(df, aes(x = x, y = y, z = heights)) +
    geom_point(aes(x = x, y = y, colour = factor(group))) + 
    ylim(0,1) + xlim(0,1) +  
    scale_colour_manual(values=c("lightblue","gold")) +
    ylim(-1,1) + xlim(-1,1)  + coord_fixed(ratio = 1) +
    stat_density2d(colour = "darkgrey") +
    ggtitle("(3) Circle - 3-dimensional") +
    theme(plot.title = element_text(size = 10), 
          axis.line=element_blank(),
          axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank(),
          plot.margin=unit(c(-0.5,1,1,1), "cm")) 
  
  grid.arrange(para, circ, circ2,ncol = 3)
```

In a paper by [Boser et al. (1992)]( http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818) modified the maximization function: 


$$w(\alpha) = \sum_i{\alpha_i} - \frac{1}{2}\sum_i{\alpha_1\alpha_0 y_1 y_0 x_1^Tx_0}$$

such that the dot products $x_1^Tx_0$ are replaced with non-linearkernel functions. Of particular significance are two common kernels: the Gaussian Radial Basis Function (RBF) and Polynomial kernels.

RBP is defined as:

$$RBF = exp(-\gamma ||x_1-x_0||^2)$$
where $\gamma = \frac{1}{2\sigma^2}$ and $\sigma$ >0. The value of $\gamma$ determines the tightness of the kernel, where larger values of $\gamma$ yield a compact, tight kernel whereas smaller values of $\gamma$ are associated with wider-spread kernels. In `R`, the value of $\gamma$ is one of the tuning parameters that a data scientist would need to specify as RBFs are the default kernel. Note that one needs to use a grid search to find the appropriate value of $\gamma$ as it cannot be mathematically optimized, but rather analyzed.

The polynomial kernel is defined as:

$$Polynomial = (1+x_1^Tx_0)^d$$
where the value of $d > 0$, indicates the polynomial degree, and assumes that all polynomials from 1 to $d$ are included.

####Practical Details
After all the derivation is done, keep the following points in mind when applying SVMs:

- Tuning is centered on two variables: $C$ to manage the extent to which the margin is hard or soft, and $\gamma$ for when a RBF is applied. Note that the quantities of each are tuned using cross-validation in the form of a grid search (e.g. test multiple values at equal intervals).
- Non-linear SVMs are computationally expensive. Very high dimensional data sets will likely take a long time to compute.
- SVMs are particularly well-suited for a pattern recognition, computer vision among other computationally challenging problems. While they may yield more accurate results than many other classifiers, the ability for data scientists to give social policy decision makers control over the story is limited.
- ROC and AUC may at times be challenging to calculate for SVM results. An alternative is to utilize the F1 statistic defined as: 

$$F_1 =  2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} +  \text{recall}}$$



####Applying SVMs 
SVMs are neatly packaged into an interface library called `e1071`. The library contains a suite of machine learning tools in addition to SVMs.
```{r, message = FALSE, warning = FALSE}
library(e1071)
```

Syntax is fairly simple and requires a minimum, six parameters are required:

`svm(formula, data, cost, gamma, kernel)`

where:

- `formula` specifies a specification to be estimated.
- `data` is a data frame.
- `C` is the regularization parameter which needs to be grid searched. Default = 1.
- `g` is a parameter used for RBF. 
- `kernel` is string value that indicates the kernel to be used, which may be one of the four types:  "linear", "polynomial", "radial", and "sigmoid". Default = radial basis.


To start, we will fit an SVM using a "*radial"" kernel assuming $cost = 1$ and $gamma = \frac{1}{\text{dim}}$, where $dim$ is the number of effective variables in our data (e.g. continuous variables, expanded dummies, and intercept). This effectively is 18 in the health data.


```{r, message = FALSE, warning = FALSE}
  spec <- as.formula("coverage ~  agep + wage + cit + mar + schl + esr")
  svm_rbf_fit <- svm(spec, data=train, kernel = "radial", cost = 1, gamma = 0.05555)
```


Typically, it is a good idea to test various values of `cost` and `gamma`, though noting that this process for SVMs is computationally expensive (takes a long time), especially for RBF kernels. The `e1071` library provides a method `tune.svm()` to find the best `cost` and `gamma` (see below). In this example, we will manually tune to develop a sense of how calibration works in practice.

```{r, message = FALSE, warning = FALSE, eval = FALSE}
  tune <- tune.svm(spec ,
                    data = train,
                    kernel = "linear", 
                    cost=10^(-1:2), gamma=c(.5,1,2))

``` 


To determine search for the best parameters, we will conduct a grid search: a combination of four values of `cost` and four values of `gamma` will be tested for a total of 16 models. We choose equally spaced values on on a quadratic scale (e.g. $0.01$, $1$, $10$) to emphasize differences in model fit. To evaluate accuracy, we will rely on the F1-scores


```{r, message = FALSE, warning = FALSE}

#F1 score
 meanF1 <- function(actual, predicted){
    # Mean F1 score function
    # 
    # Args: 
    #   actual = a vector of actual labels
    #   predicted = predicted labels
    #
    # Returns: 
    #   F1 score
    
    classes <- unique(actual)
    results <- data.frame()
    for(k in classes){
      results <- rbind(results, 
                       data.frame(class.name = k,
                                  weight = sum(actual == k)/length(actual),
                                  precision = sum(predicted == k & actual == k)/sum(predicted == k), 
                                  recall = sum(predicted == k & actual == k)/sum(actual == k)))
    }
    results$score <- results$weight * 2 * (results$precision * results$recall) / (results$precision + results$recall) 
    return(sum(results$score))
 }
```

```{r, message=FALSE, warning=FALSE, eval = FALSE}
# Prep grid search parameters
  cost_vec <- 10^(-1:2)
  gamma_vec <- 2^(seq(-5, 2, 2))
  combo <- expand.grid(cost = cost_vec, 
                       gamma = gamma_vec)
  
# Create 10-folds of random partitions
  # Create index for rows in train set
  fold <- rep(seq(1,10), ceiling(nrow(train)/10))[1:nrow(train)]
  
  # Randomly reorder fold
  set.seed(10)
  fold <- fold[order(runif(nrow(train)))]
  
#Run 10-folds cross validation while tuning gamma and cost parameters
  cv_results <- data.frame()
  
  for(p in unique(fold)){
    for(i in 1:nrow(combo)){
      #Fit SVM on 1 to k-1
      fit <- svm(spec, data = train[fold != p, ], kernel = "radial", 
                 cost = combo[i, 1], gamma = combo[i, 2])
      
      #Predict on kth fold
      pred <- predict(fit, train[fold == p, ])
      cv_results <- rbind(cv_results, 
                          data.frame(fold = p,
                                     cost = combo[i, 1],
                                     gamma = combo[i, 2],
                                     mean.f1 = meanF1(train$coverage[fold == p], pred)))
    }
  
  }

#View table
  combo <- aggregate(list(mean.f1 = combo$mean.f1), 
                     by = list(cost = combo$cost, gamma = combo$gamma), 
                     FUN = mean)
  print(combo)
```


```{r, message=FALSE, warning=FALSE, echo = FALSE, eval = FALSE}
#This is the same as the previous chunk but parallelized to speed up for publication purposes
#The outputs are saved in the book assets under the classification directory

# Prep grid search parameters
  cost_vec <- 10^(-1:2)
  gamma_vec <- 2^(seq(-5, 2, 2))
  combo <- expand.grid(cost = cost_vec, 
                       gamma = gamma_vec, 
                       fold = 1:10)
  
# Create 10-folds of random partitions
  # Create index for rows in train set
  fold <- rep(seq(1,10), ceiling(nrow(train)/10))[1:nrow(train)]
  
  # Randomly reorder fold
  set.seed(10)
  fold <- fold[order(runif(nrow(train)))]
  
#Run 10-folds cross validation while tuning gamma and cost parameters
 library(foreach)
 library(doParallel)
  cl <- makeCluster(3)
  registerDoParallel(cl)
  cv_results <- foreach(i = 1:nrow(combo), .combine = rbind) %dopar% {
      require(e1071)
      p <- combo[i, 3]
      c <- combo[i, 1]
      g <- combo[i,2]
    
     #Fit SVM on 1 to k-1
      fit <- svm(spec, data = train[fold != p, ], kernel = "radial", 
                 cost = c, gamma = g)
      
      #Predict on kth fold
      pred <- predict(fit, train[fold == p, ])
      
      res <- data.frame(fold = p,
                         cost = c,
                         gamma = g,
                         mean.f1 = meanF1(train$coverage[fold == p], pred))
      return(res)
    }
  stopCluster(cl)
  write.csv(cv_results, file = "assets/classification/data/svm_cv.csv", row.names = FALSE)
```
```{r echo =FALSE, warning = FALSE, message=FALSE, fig.cap = "Mean F1 scores for 10-folds cross validation"}
combo <- read.csv("assets/classification/data/svm_cv.csv")
res <- aggregate(list(Mean.F1 = combo$mean.f1), 
                 by = list(Cost = combo$cost, Gamma = combo$gamma), FUN = mean)
knitr::kable(res, booktab = TRUE, digits = 4)
```
Based on the grid search, we find that the best model has a $C = 1$ and $gamma = 0.5$. We then train a model with those parameters, then predict the classes of the test set to find that a $F1 = 0.7511$.

```{r, message = FALSE, warning = FALSE}
#Predict labels
  pred_test <- svm(spec, data = train, kernel = "radial", cost = 1, gamma = 0.5)
  pred_rbf <- predict(pred_test, test)
  
#examine result
  table(pred_rbf)

##RBF
  meanF1(test$coverage, pred_rbf)
```

