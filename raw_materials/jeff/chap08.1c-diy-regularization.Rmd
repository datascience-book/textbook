--- 
title: "Data Science + Public Policy"
author: "Jeff Chen"
date: '`r Sys.Date()`'
output: pdf_document
description: Chapter 10
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
---


### What do I do when there are too many features? 

Too many features? Regularization methods can do the trick.

To illustrate regularization in action, we'll use an example that is inspired by Google Flu Trends (read more: [Detecting influenza epidemics using search engine query data](http://static.googleusercontent.com/media/research.google.com/en/us/archive/papers/detecting-influenza-epidemics.pdf)). The research endeavor led by Google scientists sought to predict influenza in near real-time using search engine query data, but became controversial and widely criticized for its pure reliance on correlations. It is a pioneering and iconic nowcasting project and is an early example of how alternative data could be put into action. 

The basic idea of Google's research is that some combination of search queries can be combined to predict the CDC's influenza-like illness (ILI) estimates. In the below example, we do not profess to develop a predict the flu, but rather take advantage of the data made available through [Google Trends](https://trends.google.com/trends/). As one may imagine, Google's search engine receives an unfathomable amount of queries. When combined with ILI data, we can easily yield a data set with $k > n$. 



To start, We'll use the digIt library to pull the "flu" data. For simplicity, the example presented below contains $n = 85$ and $k = 85$ including target and date variables. Each $k$ is a monthly-level index of the search volume for a specific search query, standardized to the maximum value observed in the period (100 = max). The queries include search terms such as "kleenex", "cold remedy", "cough", and "cvspharmacy" among others. 

```{r, message = FALSE, warning = FALSE}
library(digIt)
flu <- digIt("flu")
sample(colnames(flu), 20, replace = FALSE)
```

The most correlated queries with ILI are conceptually related -- all are focused on symptoms of the flu.

```{r, eval = FALSE}
#Calculate correlation matrix
mat <- cor(flu[,ncol(flu):2])

#Extract correlates with ILI
top <- data.frame(query = row.names(mat), 
                  rho = mat[,1])
top <- top[order(-top$rho),]

#Show top ten
head(top, 10)
```
```{r, echo = FALSE}
#Calculate correlation matrix
mat <- cor(flu[,ncol(flu):2])

#Extract correlates with ILI
top <- data.frame(query = row.names(mat), 
                  rho = mat[,1])
top <- top[order(-top$rho),]
row.names(top) <- NULL

#Show top five
knitr::kable(head(top, 10), booktab = TRUE)
```

To apply a regularized regression, we will rely on the `glmnet` library that facilitates the application of an elastic net regression -- which allows for a combination of a L1- and L2- constraint to be applied. Given the TSS $\sum^n_{i=1}(y_i - \sum^k_{j=1} x_{ij}w_j)^2$, elastic net applies the following constraint:

$$(1-\alpha)\sum^k_{j=1}|w_j|_1 + \alpha\sum^k_{j=1}|w_j|^2_2< c $$
which is comprised of an L1-norm ($|w_j|_1$) scaled by $(1-\alpha)$ and a L2-norm scaled by $\alpha$. When $\alpha = 1$, then elastic net is effectively a LASSO regression. When $\alpha = 0$, then elastic net is a ridge regression. This constraint structure provides the flexibility to apply hybrid L1/L2 regularization by selecting values of $0 < \alpha < 1$. For more details on elastic net, read [Zou and Hastie (2005)](http://www.stat.washington.edu/courses/stat527/s13/readings/zouhastie05.pdf).

```{r, message=FALSE, warning = FALSE}
library(glmnet)
```

The `glmnet` library requires all data to be in vector and matrix form -- data frames are not allowed. Following proper prediction methodology, we will split the sample into 80-20 train-test partitions.
```{r, message=FALSE, warning = FALSE}
#Set sample partition parameters
  train.prop <- 0.75
  train.max <- round(nrow(flu) * train.prop)
  test.min <- train.max + 1

#Train
  y.train <- flu$ili.rate[1:train.max]
  x.train <- flu[1:train.max,]
  x.train$date <- x.train$ili.rate <- NULL
  x.train <- as.matrix(x.train)

#Test
  y.test <- flu$ili.rate[test.min:nrow(flu)]
  x.test <- flu[test.min:nrow(flu), ]
  x.test$date <- x.test$ili.rate <- NULL
  x.test <- as.matrix(x.test)
```

The library provides a couple of functions to estimate a regularized regression, but the following uses cross validation to find the optimal value of $\lambda$ that minimizes error:

`cv.glmnet(x, y, alpha, type.measure, family, nfolds)`

where: 

- `x` is a $n \times k$ matrix of input features;
- `y` is a $n \times 1$ target vector;
- `alpha` is the parameter to choose between LASSO and ridge (LASSO = 1);
- `type.measure` indicates the optimization measure, which can be _mean squared error_ ("mse") or _mean absolute error_ ("mae") for regression problems;
- `family` indicates the response type. This is assumed to be "gaussian" for quantitative targets.
- `nfolds` is the number of cross validation folds. Default is 10-folds CV.

In this example, we will choose `nfolds = 20` and a LASSO model (`alpha = 1`), then assign the model object ot `mod.lasso`.

```{r, message=FALSE, warning = FALSE}

mod.lasso <- cv.glmnet(x.train, y.train, nfolds = 20,
               alpha = 1, type.measure = "mse")

```

The `mod.lasso` model object contains a rich amount of diagnostics and model outputs. For example, the MSE can be analyzed along a search grid of $\lambda$ and identify the value of $\lambda$ that minimizes error ($-3 < log(\lambda) < -2$). Notice that each grid point contains a standard deviation on the error as estimated through cross-validation. 
```{r, message=FALSE, warning = FALSE, fig.cap = "MSE vs. Log(Lambda) for ILI model where alpha = 1"}
plot(mod.lasso)
```

Next, we can examine the coefficients that are non-zero for the value of $\lambda$ that yields the lowest error ("lambda.min") as well as a set of arbitrarily selected lambda values. Notice that only a handful of the $k =83$ input features remain, all of which are sympotom-related search queries. A couple notes about the coefficients:

- The `cv.glmnet()` function automatically  mean-centers and standardizes (mean = 0, unit variance) the target and inputs, but returns coefficients in the original scale.
- Due to the properties of the LASSO method, coefficients are without standard errors.
- The $\lambda$ parameter represented as _s_ in the library is calculated along a grid (equal intervals between a minimum and maximum value). This means if coefficients for a specific value of $\lambda$ is requested, but is not in search grid, then the glmnet library will either need to interpolate the value (`exact = FALSE`) or refit the model at a specified lambda value (`exact = TRUE`).

```{r,eval = FALSE}
coef(mod.lasso,  s = c("lambda.min") )
coef(mod.lasso,  s = c( 0.01, 0.5), exact = TRUE )
```
```{r, echo = FALSE}
#Coefficiets
a <- coef(mod.lasso,  s = "lambda.min")
b <- coef(mod.lasso,  s = c( 0.05, 0.5), exact = TRUE)

tab <- data.frame(Variable = row.names(a), coef1 = a[,1], coef2 = b[,1], coef3 = b[,2])
row.names(tab) <- NULL
tab <- tab[tab$coef2 != 0,]
tab <- tab[order(-tab$coef2),]
for(i in 2:4){
  tab[,i] <- round(tab[,i],5)
  tab[tab[,i]==0,i] <- ""
}

knitr::kable(tab, booktab = TRUE, row.names = FALSE, col.names =  c("Feature", paste0("Lambda Min = ", round(mod.lasso$lambda.min, 3)), "Lambda = 0.05", "Lambda = 0.5"))
```

Lastly, we can predict ILI in the test sample using the LASSO model where $\lambda$ is minimized.

```{r, message=FALSE, warning = FALSE}
#Predict y
  yhat.train <- predict(mod.lasso, x.train, s = "lambda.min")
  yhat.test <- predict(mod.lasso, x.test, s = "lambda.min")
  
#Calculate out of sample error
  rmse <- function(y, x){
    return(sqrt(mean((y - x)^2)))
  }
  err1 <- round(rmse(yhat.test, y.test),2)
  print(err1)
```

This result appears to be promising, but is unsatisfying without a set of benchmarks to compare against. We construct a series of plain vanilla OLS tuned based on specifications of the top most correlated features in sets of 1, 2, 3, 5, and 10. When the two sets of models are compared, a single LASSO appears to out-perform OLS.

```{r, message=FALSE, warning = FALSE}

#Calculate correlation matrix using training data
  rhos <- as.vector(cor(y.train, x.train))
  rhos <- data.frame(id = 1:length(rhos), rhos)
  rhos <- rhos[order(-rhos$rhos),]

#Set up a juxtaposed plot area for six graphs
  par(mfrow=c(2,3), 
      oma = c(5,4,0,0) + 0.5,
      mar = c(0,0,1,1) + 0.5)
  
#Plot the LASSO
  plot(y.test, type = "l", col = "grey", main = paste0("LASSO: RMSE = ", err1), 
       cex.main = 1.2, ylab = "outcome", xaxt='n', yaxt = 'n')
  lines(yhat.test, col = "red")

#Loop through and plot top X correlates using OLS
for(i in c(1, 2, 3, 5, 10)){
  #Set up data
  df.train <- data.frame(y.train, x.train[,rhos$id[1:i]])
  df.test <- data.frame(y.test, x.test[,rhos$id[1:i]])
  colnames(df.train) <- colnames(df.test) <- c("y", paste0("x",1:i))
  
  #Model
  lm.obj <- lm(y~., data = df.train)
  yhat2 <- predict(lm.obj, newdata = df.test)
  
  #Plot y
  err2 <- round(rmse(yhat2, y.test),2)
  plot(y.test, type = "l", col = "grey", main = paste0("Top ", i," Only: RMSE = ", err2), 
       ylab = "outcome", cex.main = 1.2, xaxt='n', yaxt = 'n')
  lines(yhat2, col = "red")
}

```