--- 
title: ''
author: "Jeff Chen"
date: '`r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
documentclass: book
link-citations: yes
description: Chapter 09
site: bookdown::bookdown_site
biblio-style: apalike
---

### Decision trees  
In everyday policy setting and operations, decision trees are a common tool used for communicating complex processes, whether for how an actor moves through intricate and convoluted bureaucracy or how a sub-population can be described based on a set of criteria. While the garden variety decision tree can be laid out qualitatively, supervised learning allows decision trees to be created in an empirical fashion that not only have the power to aesthetically communicate patterns, but also predict how a non-linear system behaves.

The structure of a decision tree can be likened to branches of a tree: moving from the base of the tree upwards, the tree trunk splits into two or more large branches, which then in turn split into even smaller branches, eventually reaching even small twigs with leaves. Given a labeled set of data that contains input features, the branches of a decision tree is grown by subsetting a population into smaller, more homogeneous units. In other words, moving from the root of the tree to the terminating branches, each subsequent set of branches should contain records that are more similar, more homogeneous or purer. 

```
  1. Let Sample = S, Target = Y, Input Features = X
  2. Screen records for cases that meet termination criteria.
        If each base case that is met, partition sample to isolate homogeneous cases.
  3. For each X:
        Calculate the attribute test comparing all X's and Y
  4. Compare and identify Xi that yields the greatest separability
  5. Split S using input feature that maximizes separability
  6. Iterate process on steps 3 through 5 until termination criteria is met
```

As was demonstrated at the beginning of this chapter, decision trees use a form of recursive partitioning to learn patterns, doing so using central concepts of _information theory_. There are a number of decision tree algorithms that were invented largely in the 1980s and 1990s, including the ID3 algorithm, C4.5 algorithm, and Classification And Regression Trees for Machine Learning (CART). All these algorithms follow the same framework that includes the following elements: (1) nodes and edges, (2) attribute tests, and (3) termination criteria.

#### (1) Nodes + Edges
Recalling the healthcare insurance decision tree, the tree can be characterized by nodes and edges. 

- Nodes (circles) contain records. 
- Edges (lines) show dependency between nodes and is the product of a split decision. Nodes are split based on an attribute test -- a technique to identify the optimal criterion to subset records into more homogeneous groups of the target variable. 
- The node at the top of the tree is known as the *root*and represents the full population.
- Each time a node is split, the result is two nodes -- each of which is referred to as a child node. 
A node without any child nodes is known as a leaf.  

The goal is to grow a tree from the root node into as many smaller, more homogeneous child nodes with respect to the target variable.

#### (2) Attribute tests
 
To understand attribute tests means to have a thorough understanding of separability. Let's suppose we have a list of residents of a town. The list contains both users and non-users of a given healthcare service. For each person, the inventory captures whether a given person is employed, has income over $20k, and lives on the west side or east side of town. Each of the features are plotted in the pie chart below. 50% of town residents use the health service, but which of the features is best at separating users from non-users?

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap = "Summary characteristics of town residents."}
  #Libraries
  library(ggplot2)
  library(gridExtra)
  
  #Get fabricated data
  customers <- read.csv("assets/classification/data/entropy_example.csv")
  customers$id <- 1:nrow(customers)
  
  
  #Target
  tab <- as.data.frame(table(customers$user))
  colnames(tab) <- c("customer", "count")
  
  pie_c <- ggplot(data = tab, aes(x = "", y = count, fill = customer)) +
              geom_bar(width = 100, stat = "identity") +
              coord_polar(theta="y", start = 0) +
              theme( plot.title = element_text(size = 9), 
                    axis.text.x=element_blank(), axis.text.y=element_blank(), 
                    axis.title.x=element_blank(), axis.title.y=element_blank()) 
  
  #Area
  tab <- as.data.frame(table(customers$area))
  colnames(tab) <- c("area", "count")
  
  pie_i <- ggplot(data = tab, aes(x = "", y = count, fill = area)) +
              geom_bar(width = 100, stat = "identity") +
              coord_polar(theta="y", start = 0) +
              theme( plot.title = element_text(size = 9), 
                    axis.text.x=element_blank(), axis.text.y=element_blank(), 
                    axis.title.x=element_blank(), axis.title.y=element_blank()) 
  
  #Income
  tab <- as.data.frame(table(customers$income))
  colnames(tab) <- c("income", "count")
  
  pie_r <- ggplot(data = tab, aes(x = "", y = count, fill = income)) +
              geom_bar(width = 100, stat = "identity") +
              coord_polar(theta="y", start = 0) +
              theme( plot.title = element_text(size = 9), 
                    axis.text.x=element_blank(), axis.text.y=element_blank(), 
                    axis.title.x=element_blank(), axis.title.y=element_blank()) 
  
    
  #Employment
  tab <- as.data.frame(table(customers$employ))
  colnames(tab) <- c("employ", "count")
  
  pie_e <- ggplot(data = tab, aes(x = "", y = count, fill = employ)) +
              geom_bar(width = 100, stat = "identity") +
              coord_polar(theta="y", start = 0) +
              theme( plot.title = element_text(size = 9), 
                    axis.text.x=element_blank(), axis.text.y=element_blank(), 
                    axis.title.x=element_blank(), axis.title.y=element_blank()) 
  
  
  grid.arrange(pie_c, pie_e, pie_r, pie_i, ncol = 2)
```

To answer that question, we can rely on a visual cross-tabulation where the size of the circles is scaled proportional to the number of records. The objective is to identify the matrix where the circles are the largest along any diagonal -- this would indicate that given usership, a feature is able to serve as a criterion that separates users from non-users. Of the three graphs below, graph #2 is able to separate a relatively large proportion of users from non-users. For a relatively low-dimensional dataset (fewer attributes), a visual analysis is accomplishable. However, on scale, undertaking this process manually may be onerous and prone to error.

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap = "A visual comparison of low separability (1 and 3) and high separability (2)."}
  
  #employ
  tab <- as.data.frame(table(customers$user, customers$employ))
  colnames(tab) <- c("customer", "employ", "count")
  
  pie_1 <- ggplot(tab, aes(customer, employ)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Customer?") + ylab("Employed?") +  ggtitle("(1)") + 
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  
  #Income
  tab <- as.data.frame(table(customers$user, customers$income))
  colnames(tab) <- c("customer", "income", "count")
  
  pie_2 <- ggplot(tab, aes(customer, income)) + 
              geom_point(aes(size = count), colour = "navy")  +  ggtitle("(2)") +  
              theme_bw() + xlab("Customer?") + ylab("Income") +   
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1) 
  
  #Revenue
  tab <- as.data.frame(table(customers$user, customers$area))
  colnames(tab) <- c("customer", "area", "count")
  
  pie_3 <- ggplot(tab, aes(customer, area)) + 
              geom_point(aes(size = count), colour = "navy")  +  ggtitle("(3)") +  
              theme_bw() + xlab("Customer?") + ylab("Side of town?") +   
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  grid.arrange(pie_1, pie_2, pie_3,  ncol = 3)
```

Enter attribute tests.

Decision trees are grown by splitting a data set into many smaller samples. Attribute tests are the mode of finding the split criterion, following an empirical process to systematically test all input features to find the feature with the greatest separability.  The process starts from the root node where the algorithm examines each input feature to find the one that maximizes separability at that node: 

```
  Let Sample = S, Target = Y, Input Features = X
      For each X:
          Calculate the attribute test statistic comparing X and Y
          Store statistic
      Compare and identify Xi that yields the greatest separability
      Split S using input feature that maximizes separability
      Iterate process on child node
```

Upon finding the optimal feature for a given node, the decision tree algorithm splits the node into two child nodes based on the optimal feature, then moves onto the next node (often times a child node) and runs the same process to find the next split. There are a number of attribute tests, of which we will cover two: *Information Gain* and  *Gini Impurity*.

__Information gain__ is a form of *Entropy*, which is a measure of purity of information. Based on these distinct states of activity, entropy is defined as:

$$\text{Entropy} = \sum{-p_{i} log_2(p_{i})}$$

where $i$ is an index of states, $p$ is the proportion of observations that are in state $i$, and $log_2(p_i)$ is the Base 2 logarithm of the proportion for state $i$. Information Gain (IG) is variant of entropy, which is the entropy of the root node *less* the average entropies of the child nodes.

$$\text{IG} = \text{Entropy}_\text{root} - \text{Avg Child Entropy}$$
How does this work in practice? Starting from the root node, we need to calculate the root entropy, where the classes are based on the classes of the target `usership`.

$\qquad \text{Entropy}_\text{usership} = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad  = (-\frac{6}{12} log_2(\frac{6}{12})) + (-\frac{6}{12} log_2(\frac{6}{12}))$

$\qquad \qquad \qquad \qquad \qquad  = 1.0$

Then, the attribute test is applied to the root node by calculating the weighted entropy for each proposed child node. Using the `income` feature, the calculation is as follows:

- Split the root node into two child nodes using the `income` class. This yields the following subsamples as shown in the table below:

| | < $20k | > $20k|
|--------+---------+----------|
|No | 0 | 6 |
|Yes | 5 | 1 |
|Total | 5 | 7 |

- For each child node (the columns in the table), calculate entropy:

$\qquad \text{Entropy}_\text{income < 20k } = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad  = -\frac{5}{5} log_2(\frac{5}{5}) = 0$


$\qquad \text{Entropy}_\text{income > 20k } = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad = -\frac{6}{7} log_2(\frac{6}{7}) + -\frac{1}{7} log_2(\frac{1}{7}) = 0.5916728$

- Calculate the weighted average entropy of children:

$\qquad \text{Entropy}_\text{income split} = \frac{5}{12}(0) +  \frac{7}{12}(0.5916728) = 0.3451425$

- Then calculate the information gain:

$\qquad \text{IG}_\text{income} = \text{Entropy}_\text{root} -  \text{Entropy}_\text{income split}$

$\qquad \qquad \qquad \qquad \qquad = 1 - 0.3451425 = 0.6548575$

- We then can perform the same calculation on all other features (e.g. employment, part of town) and compare results. The goal is to *maximize* the IG statistic at each decision point. In this case, we see that income is the best attribute to use for splitting. This split is easily interpretable: "The majority of users of health services can be predicted to earn less than $20,000."

| Measure | IG |
|---------+------|
|Employment| 0.00 | 
|Income | 0.6548575 |
|Area of Town|0.027119 |


__Gini Impurity__ is closely related to the entropy with a slight modification:

$$\text{Gini Impurity} = \sum{p_{i}(1-p_{i})} = 1 - \sum{p_{i}^2}$$
Using Gini Impurity as an attribute test is also similar to Information Gain: 

$$\text{Gini Gain} = \text{Gini}_\text{root} - \text{Weighted Gini}_\text{child}$$

#### (3) Stopping Criteria + Tree Pruning
Both Gini Gain and Information Gain attribute tests can be recursively applied until there are no longer input features available to split the data. This is also known as a "fully grown tree" or an "unpruned tree". While the terminal leafs may yield a high degree of accuracy in training, trees may grow to epic and complex proportions that have leaf sizes are often times too small to provide accurate and generalizable results. While fully grown trees are considered to have low bias, their out-of-sample performance may be high in variance. There [theoretically] exists some optimal balancing point where trees are complex enough to capture statistical patterns, but are not too complex to yield misleading results.

Fortunately, the methodologists who invented decision tree learning have designed two approaches to balance accuracy and generalizability: stopping criteria and pruning.

Recall that a leaf is defined as a node with no child nodes. Otherwise stated, a leaf is a terminal node in which no additional attribute testing is conducted -- it's placed out of commission. Stopping criteria are employed to determine if a node should be labeled a leaf during the growing process, thereby stopping tree growth at a given node. These criteria are specified before growing the tree and take on a number of different forms including: 

- A node has fewer records than a pre-specific threshold;
- The purity or information gain falls below a pre-specified level or is equal to zero;
- The tree is grown to n-number of levels (e.g. Number of levels of child nodes relative to the root exceeds a certain threshold).

While stopping criteria are useful, the results in some studies indicate their performance may be sub-optimal. The alternative approach involves growing a tree to its fullest, then comparing the prediction performance given tree complexity (e.g. number of nodes in the tree) using cross-validation.  In the example graph below, model accuracy degrades beyond a certain number of nodes. Thus, optimal number of nodes is defined as when cross-validation samples (e.g. train/test, k-folds) reaches a minimum across samples. Upon finding the optimal number of nodes, the tree is _pruned_ to only that number of nodes. 

```{r, echo=FALSE, warning = FALSE, message = FALSE, fig.height = 2}
set.seed(1020)
n = 15
tree.error <- data.frame( trees = 1:n,
                          v1 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v2 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v3 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v4 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v5 = 50*(1+cos((1:n)/3)) + runif(n)*10)

ggplot(tree.error) + geom_line(aes(x = trees, y = v1), colour = "orange") + geom_line(aes(x = trees, y = v2), colour = "navy") +
  xlab("Number of nodes") + ylab("Error") + geom_vline(xintercept = 10, colour = "red") + geom_line(aes(x = trees, y = v3), colour = "green") + geom_line(aes(x = trees, y = v4), colour = "purple") + geom_line(aes(x = trees, y = v5), colour = "lightblue")  + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())
 
```


#### Issues

Like any technique, decision trees have strengths and weaknesses:

| Strengths  | Weakness|
|--------------------------------------+--------------------------------|
| - Rules (e.g. all the criteria that form the path from root to leaf) can be directly interpreted. | - Data sets with large number of features will have overly complex trees that, if left unpruned, may be too voluminous to interpret. |
| - Method is well-suited to capture interactions and non-linearities in data. | - Trees tend to overfitted at the terminal leafs when samples are too small. |
| - Technique can accept both continuous and continuous variables without prior transformation. | |
| - Feature selection is conducted automatically | |


#### In Practice: Decision Trees
To put decision trees into practice, we will use the same `train` and `test` data frames introduced in the GLM section. There are a number of R implementations of decision trees, the most popular of which is the `rpart` library:
```{r, message = FALSE, warning=FALSE}
  library(rpart)
```

The main function within the library comes with flexible capabilities to grow decision trees: 

`rpart(formula, method, data, cp, minbucket, minsplit)`

where:

- `formula` is a formula object. This can take on a number of forms such as a symbolic description (e.g. $y = f(x_1, x_2, ...)$ is represented as "`y ~ x1 + x2`""). 
- `method` indicates the type of tree, which are commonly either a classification tree "class" or regression tree "anova". Split criteria can also be custom written.
- `data` is the data set in data frame format.
- `cp` is a numeric indicates the complexity of the tree. $cp = 1$ is a tree without branches, whereas $cp = 0$ is the fully grown, unpruned tree. If $cp$ is not specified, `rpart()` defaults to a value of 0.01.
- `minbucket` is a stopping criteria that specifies the minimum number of observations in any terminal leaf.
- `minsplit`  is a stopping criteria that specifies the number of observation in a node to qualify for an attribute test.

As a first pass, we'll run `rpart()` with the default assumptions. Note that in `rpart()` automatically conducts k-folds cross-validation for each level of tree growth. If one were to use `summary()` or `str()` to check the structure of the output object named `fit`, the inner workings would likely be found to be quite exhaustive and rather complex. Fortunately, the `printcp()` method can be used to obtain a summary of the overall model accuracy for tree at different stages of growth. Key features of the `printcp()` output include:

- A listing of the variables actually used in construction (note that `cit`)
- In the table, `CP` indicates the tree complexity, `nsplit` is the number of splits, `rel error` is the prediction error in the training data, `xerror` is the cross-validation error, and `xstd` is the standard error.

```{r, message = FALSE, echo = FALSE, warning=FALSE}
#cp = 0
  fit <- rpart(coverage ~ agep + wage + cit + mar + schl + esr , 
               method = "class", data = train)

#Lowest xerror
  best.error <- as.vector(min(fit$cptable[,4]))
  best.splits <- as.vector(fit$cptable[,2][which(fit$cptable[,4]==min(fit$cptable[,4]))])
  best.sd <- as.vector(fit$cptable[,5][which(fit$cptable[,4]==min(fit$cptable[,4]))])

  opt.error <- best.error + best.sd
  opt.select <- as.vector(fit$cptable[,1][which(fit$cptable[,4] <= opt.error)])[1]
  opt.xerror <- as.vector(fit$cptable[,4][which(fit$cptable[,4] <= opt.error)])[1]
  opt.select.split <- as.vector(fit$cptable[,2][which(fit$cptable[,4] <= opt.error)])[1]
  
```

To choose the best tree, a _rule of thumb_ is to first find the tree with the lowest cross-validation `xerror`, then find the tree that has the lowest number of splits that is still within one standard deviation `xstd` of the best tree^[Hastie et. al (2001)]. The idea behinds this rule of thumb takes advantage of uncertainty: the true value lies somewhere within a confidence interval, thus any value within a tight confidence interval of the best value is approximately the same. In this first model, the best tree has `r paste0("nsplit = ", best.splits)` and `r paste0("xerror = ", best.error)`. By applying the rule, the upper bound of acceptable error is `r paste0("xerror = ", round(best.error,6)," + ", round(best.sd, 6), " = ", opt.error)`. As it turns out, the tree with `r paste0("nsplit = ", opt.select.split)` is within one standard deviation and is thus the best model. 


```{r, eval = FALSE}

#Fit decision tree under default assumptions
  fit <- rpart(coverage ~ agep + wage + cit + mar + schl + esr, 
               method = "class", data = train)
  
#Tools to review outpu
  printcp(fit)
```

The model's learned rules contained in `fit` can be plotted with `plot()`, but it takes a bit of work to get the plot into a presentable format. The substitute is using the `rpart.plot` library, which auto-formats the tree and color codes nodes based on the concentration of the target variable.

```{r, fig.height = 3, fig.cap = "Decision tree using default parameters."}
#Plot
  library(rpart.plot)
  rpart.plot(fit, shadow.col="gray", nn=TRUE)
```

```{r, message = FALSE, echo = FALSE, warning=FALSE}
#cp = 0
  fit.0 <- rpart(coverage ~ agep + wage + cit + mar + schl + esr , 
               method = "class", data = train, cp = 0)

#Lowest xerror
  best.error <- as.vector(min(fit.0$cptable[,4]))
  best.splits <- as.vector(fit.0$cptable[,2][which(fit.0$cptable[,4]==min(fit.0$cptable[,4]))])
  best.sd <- as.vector(fit.0$cptable[,5][which(fit.0$cptable[,4]==min(fit.0$cptable[,4]))])

  opt.error <- best.error + best.sd
  opt.select <- as.vector(fit.0$cptable[,1][which(fit.0$cptable[,4] <= opt.error)])[1]
  opt.xerror <- as.vector(fit.0$cptable[,4][which(fit.0$cptable[,4] <= opt.error)])[1]
  opt.select.split <- as.vector(fit.0$cptable[,2][which(fit.0$cptable[,4] <= opt.error)])[1]
  
```

While this answer is valid, it should be noted that the CP lower threshold is 0.01, which is the default value. For robustness, we should run the model once more, this time specifying $cp = 0$ to obtain the full, unpruned tree (see below). Applying the error minimization rule once more, the minimum `r paste0("xerror = ", round(best.error, 6))`, which corresponds to `r paste0("nsplit = ", best.splits)`. The maximum $xerror$ within one standard deviation is `r paste0("xerror = ", round(best.error, 6), " + ", round(best.sd, 6), " = ", round(opt.error,6) )`, which corresponds to `r paste0("nsplit = ", opt.select.split)` with `r paste0("xerror = ", round(opt.xerror, 6))` and `r paste0("cp = ", round(opt.select,6))`

```{r, eval = FALSE}
#cp = 0
  fit.0 <- rpart(coverage ~ agep + wage + cit + mar + schl + esr , 
               method = "class", data = train, cp = 0)
  printcp(fit.0)
```

```{r, echo = FALSE}
  printcp(fit.0)
```

At this point, we'll re-run the decision tree once more with the updated $cp$ value, assign the decision tree object to `fit.opt`, and plot the resulting decision tree. Notice how the rendered tree is significantly more complex relative to the default and interpretation may be more challenging with a plethora of criteria. 

```{r, fig.height = 4, fig.cap = "Decision tree for optimized complexity."}
  fit.opt <- rpart(coverage ~ agep + wage + cit + mar + schl + esr, 
               method = "class", data = train, cp = opt.select)
  rpart.plot(fit.opt, shadow.col="gray", nn=TRUE)
```

In lieu of a thorough review of the learned rules, we may rely on a measure of variable importance, that is defined as follows:

$$\text{Variable Importance}_k = \sum{\text{Goodness of Fit}_\text{split, k} + (\text{Goodness of Fit}_\text{split,k}\times \text{Adj. Agreement}_\text{split})}$$
Where *Variable Importance* for variable $k$ is the sum of *Goodness of Fit* (e.g. Gini Gain or Information Gain) at a given split involving variable k. In otherwords, a variable's importance is the sum of all the contributions variable $k$ makes towards predicting the target. Below, we can see that the measure can be extracted from the `fit.opt` object. As it turns out, `age` is the most important factor.

```{r}
#Extract variable importance list from fit object
  fit.opt$variable.importance
```


Using the `plotROC` package once again, we calculate the AUC score for each model to assess predictive performance on both the training and test set. One particularly striking difference is the switch in position of the $optimal$ and $cp = 0$ curves: $cp = 0$ is higher in the training set, but are at the approximate safe height in test. This indicates that $cp = 0$ notably overfits, likely to the extra low bias of unpruned leafs.

```{r, fig.height = 3, message = FALSE, warning = FALSE, fig.cap = "ROC curves for train and test sets."}
#plotROC
  library(plotROC)
  library(gridExtra)

#Predict values for train set
  pred.opt.train <- predict(fit.opt, train, type='prob')[,2]
  pred.0.train <- predict(fit.0, train, type='prob')[,2]
  pred.default.train <- predict(fit, train, type='prob')[,2]

#Predict values for test set
  pred.opt.test <- predict(fit.opt, test, type='prob')[,2]
  pred.0.test <- predict(fit.0, test, type='prob')[,2]
  pred.default.test <- predict(fit, test, type='prob')[,2]
  
#Set up ROC inputs
  input.test <- rbind(data.frame(model = "optimal", d = test$coverage, m = pred.opt.test), 
                  data.frame(model = "CP = 0", d = test$coverage,  m = pred.0.test),
                  data.frame(model = "default", d = test$coverage,  m = pred.default.test))
  input.train <- rbind(data.frame(model = "optimal", d = train$coverage,  m = pred.opt.train), 
                  data.frame(model = "CP = 0", d = train$coverage,  m = pred.0.train),
                  data.frame(model = "default", d =  train$coverage,  m = pred.default.train))
  
  
#Graph all three ROCs
  roc.test <- ggplot(input.test, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  + ggtitle("Test")
  roc.train <- ggplot(input.train, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("Train")
  
#Plot
  grid.arrange(roc.train, roc.test, ncol = 2)
  
```

Lastly, we can extract the AUC statistics using `calc_auc()`. As multiple AUCs were calculated, we will need to extract the labels for the AUCs from the `input` file in order to produce a a 'prettified' table using `xtable`. The resulting table below presents the results of the three models that were trained. For all models, we should expect that the training AUC will be greater than the test AUC. This is generally true, but occassionally the test AUC may be greater and is largely a matter of how the data was sampled.

Starting from the top of the table:

- *Full grown*. The unpruned tree is the most complex model, which means the model has a higher chance of overfitting. This is characterized by an artificially inflated training AUC and a large drop in test AUC. As seen, the AUC drops from 0.88 to 0.826 in the test sample. The unreliable results of an unpruned tree are likely due to the algorithm's sensitivity to irregular noise at leafs. 
- *Optimal*. The optimal tree achieves a consistent $AUC = 0.83$ with minimal loss of accuracy as an appropriate level of complexity was precisely tuned.
- *Default*. An underfit model will have consistently low performance in both training and testing.  As we can see, these patterns are played out in the table below containing AUCs for each the default decision tree, the optimal model complexity and the fully grown tree.

As the result of tuning towards an optimal model, we can see that the decision tree yields a marked improvement over the kNN model's $AUC = 0.44$. For a social science problem, this is considered to be a decent result.


```{r, results = 'asis', message = FALSE, warning = FALSE}
#Assemble a well-formatted table
  tab <- data.frame(model = unique(input.test$model), 
                    train = round(calc_auc(roc.train)$AUC,3), 
                    test = round(calc_auc(roc.test)$AUC,3))

```
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.cap = "Comparison of decision tree models"}
  knitr::kable(tab, booktab = FALSE)
```


