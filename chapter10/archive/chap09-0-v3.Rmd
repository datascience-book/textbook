---
output:
  pdf_document: 
      number_sections: true
  html_document: default
  geometry: margin=1in
---

# Chapter 9: Classifiers

## Playing with fire

On an August afternoon in 2007, a fire broke out on the 17th floor of the then-vacant Deutsche Bank Building, a skyscraper situated across from the former World Trade Center in New York City. The building, seriously damaged after the 9/11 attacks, had been undergoing hazard abatement and controlled demolition, leading to changes to the building floor plans and safety apparatus. When the New York City Fire Department (FDNY) responded to the scene, it was clear the fire was a serious one, quickly escalating to a seven-alarm fire incident requiring 87 units and 475 firefighters. ^[https://cityroom.blogs.nytimes.com/2007/08/18/2-firefighters-are-dead-in-deutsche-bank-fire/]  As standpipes had been disabled and floor plans altered, FDNY units found it difficult to navigate the skyscraper and put water on the fire, resorting to unconventional methods of supplying water to crews. Eventually, the fire was put out seven hours after it started, not before two firefighters lost their lives, succumbing to cardiac arrest from heavy smoke inhalation. ^[http://www.nydailynews.com/news/firefighters-dead-7-alarm-deutsche-bank-blaze-article-1.238838]  In response to the tragedy, a mayoral investigation found that the deaths could have been prevented had city agencies established information sharing protocols and leveraged a risk-based strategy to mitigate and avoid hazards. ^[http://www1.nyc.gov/assets/doi/downloads/pdf/pr_db_61909_final.pdf]  While an ideal end state would be to end all structural fires, the recommendations focused on reducing death and injury by ensuring that FDNY had the most up-to-date ground intelligence.

Risk mitigation strategy was indeed due for improvements. Since the 1950â€™s, FDNY building inspections were managed using a manual index card system where inspection schedules were based on tiers of perceived risk, where the riskiest buildings needed to be inspected once a year and the least risky buildings were inspected once every seven years. While it was a longstanding process, it left a shortfall in inspection coverage. Of the one million buildings in New York City, only one-third are inspection-qualified. Of those 300-thousand buildings, FDNY had historically been only able to inspect at most 10% of the buildings in a given year due to other operational priorities. This meant that even on a seven-year schedule, not all buildings would be reached and the fixed timeline meant that both perfectly safe and guaranteed fire traps had equal chance of being inspected. This could be easily changed. By incorporating the latest information about where fires did and did not occur and associating it with building characteristics, a new data-driven strategy could direct how buildings are prioritized for inspection.

In 2013, the New York City Fire Department (FDNY) set out to address the risk management problem by melding data and technology with their field operations. On the surface, the idea of using data and technology to reduce the risk of fire is quite alluring. However, under the hood, there were notable obstacles. On the operational side, buy-in was required. Anyone who has observed firefighters on scene will notice that it is a well-choreographed operation -- every person knows their part and abides by the established protocols as directed by leadership. For data to drive value, it needed to be integrated and accepted into the culture of a 10,000+ person fire fighting organization. On the technical side, decades worth of index cards needed to be digitized and a scheduling platform needed to be developed. Perhaps most importantly, the system had to work. Scheduling just any inspection is simple. But scheduling inspections to buildings with observable fire risks is far more challenging as such as system would need to be able to distinguish between fire-prone and fire-proof buildings. Without effective targeting, the entire effort would be for naught. 

The Commissioner and First Deputy Commissioner at the time both believed that technology had a role to play at FDNY. Aligned with Mayor Michael Bloomberg's vision of smart, data-driven government, they saw an opportunity to set an example for the nation's fire services.  They relied on the the Assistant Commissioner for Management Initiatives to lead a change management process with fire chiefs and fire officers, information technology (IT) managers, among others to change the flow of operations so that data served as a pillar on which FDNY could rely. Alliances were forged with leading fire personnel such as the Deputy Chief of Fire Operations and Battalion Chiefs to formalize the role of data in the culture of the fire house, amending standard operating procedures (SOPs) to use a digital inspection system. On the IT front, a lead software engineer and project manager meticulously gathered specifications that were then used to construct a scheduling platform. Recognizing that the proof was in the pudding, a Director of Analytics was hired to lead the overhaul of a prediction algorithm to rank buildings based on their risk and convincing stakeholders that a statistical representation of fire ignition was indeed trustworthy. The result was the Risk-Based Inspection System (RBIS), a firefighter-facing data platform that scheduled inspections at buildings with the greatest risk of fire. Three times a week for three hours per session, fire officers logged onto RBIS to obtain a list of buildings for scheduled inspection. Buildings were selected using FireCast, a statistical algorithm developed in-house to predict fires at the building level. Through FireCast, buildings no longer used assumed a static risk classification as in the index card system, but rather a dynamic risk score took into account the latest information.  

Prediction often relies on accuracy measures to determine how well algorithms perform in the field; FireCast was no different. The algorithm was able to identify buildings with fires over 80% of the time -- a degree of accuracy that superceded prior attempts at the problem. Upon implementing the new system, impacts were observed in leading operational indicators. In the first month, the number of safety violations issued grew by +19% relative to the trend under the index card system, but fell to +10% in the second month. This indicated that the riskiest buildings did indeed have more observable risks than less risky buildings, but the amount of observable risk fell as building inspection teams progressed down the risk list. 

From a statistical perspective, the prediction should have yielded far more violations, but efficacy of the prediction program was limited by (1) a fire unit's time budget to conduct inspections; (2) a policy requiring that time had to be set aside for weekly inspections, which at times led to inspecting buildings that were not observably risky after all truly risky buildings were exhausted; (3) the rule of law giving residents the right to refuse inspection. To measure efficacy, FDNY developed an indicator known as the Pre-Arrival Coverage Rate (PACR), which measures the proportion of buildings that experienced a fire that were inspected within some period (90 days) before the fire occurred -- essentially measuring if fire companies had the opportunity to evaluate risks of priority buildings. Under FireCast, FDNY had achieved a PACR of 16.5%, which was an eightfold improvement over the old strategy that yielded 1.5%. ^[http://www.nfpa.org/news-and-research/publications/nfpa-journal/2014/november-december-2014/features/in-pursuit-of-smart] ^[https://www.nist.gov/publications/research-roadmap-smart-fire-fighting] 

Since Firecast was launched in 2014, other fire prediction efforts have emerged around the United States such as the Firebird open source system for Atlanta in 2016^[http://firebird.gatech.edu/] and a spatio-temporal fire prediction approach for Pittsburgh in 2018^[http://www.kdd.org/kdd2018/accepted-papers/view/a-dynamic-pipeline-for-spatio-temporal-fire-risk-prediction].

## What's a classifier?

The RBIS/FireCast is an example of a _classification_ problem -- a task in which a model determines which group or _class_ does an observation belongs based on its attributes, doing so based by learning from known examples. Examples must include the factual (what happened) and counter-factual (what did not happen) in order to distinguish between potential fires from non-fires. 

Classification is nothing new in everyday life as we use our own mental classification models to contextualize the world around us. For example, marketers and advertisers are always looking to get product offerings in front of prospective customers and will often purchase lists of people and apply models based on past customer behavior in order to identify those who are most likely to be interested.^[ref needed] The criminal justice system has incorporated risk classifcation models to determine if those involved in alleged crime pose a flight risk if bail is posted.^[ref needed from Vera Institute]  On a more futuristic front, the technology behind self-driving cars uses a complex array of sensors and cameras that are processed by classifiers in order to distinguish between cars, people, motorbikers and cyclists. 

The same is true with fires.

By examining buildings that have and have not caught fire in the past, we are able to learn what pattern characteristics are associated with greater risk of fire. We can *score* or apply the learned pattern to new records to obtaining the probability of fire so that fire fighters could have a sneak peak of what may happen to other buildings as their conditions change. There may be thousands of variables that can play a role in predicting fires.

If fires are truly predictable, we can employ supervised learning to map how input variables can distinguish buildings that had fires from those that did not. Otherwise stated, which fire status class does a building likely belong?  Given a binary outcome $Y(Fire)$ we can determine class membership as a function of the building's characteristics: 

$$Y(Fire) = f(\text{Building characteristics, Location, Complaints, ...})$$

It sounds simple enough, but as it turns out, there are many different ways that algorithms can associate building characteristics to fires.  We can illustrate the complexities of this task under three hypothetical scenarios, plotting two input variables on the X and Y axes and color-coding two classes in purple and light blue. The solid black lines represent the true *decision boundary*, or the threshold at which an observation is classified in a specified class. 

At first, the idea of classifying records may seem straight forward, but can become a rabbit hole. In policy, we tend to start from a normative theory of how a phenomenon functions -- perhaps a simple explanation backed by linear logic. For example, a building that is bigger and older may be a reasonable working hypothesis for identifying high risk buildings. But as we investigate more and deeper, we may add exceptions to the rule as we discover cases that do not conform. Those exceptions may improve the number of correctly classified records, but detract from the simple narrative. 


```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Types of classification problems", fig.height = 2}
###############
#Prep diagrams#
###############
  
  #Base data
  set.seed(234)
  n <- 2000
  examp <- data.frame(x1 = runif(n), x2 = runif(n))
  examp$linear <- (examp$x1 < examp$x2) * 1
  examp$nonlinear <- examp$x1 < ( examp$x2 * 4 - 8*examp$x2^2 + 2*examp$x2^3) -0.1
  part1 <- examp$x1 < ( examp$x2 * 4 - 8*examp$x2^2 + 2*examp$x2^3) +0.2
  part2 <- examp$x2 >  1.2- ( examp$x1*0.6)
  part3 <- examp$x2 >  0.2+ ( examp$x1*1.4)
  examp$discont <- (part1 == TRUE | part2 == TRUE )*1
  examp$varied <- (part1 == TRUE | part2 == TRUE | part3 == TRUE)*1
  
  #Create grid
  grid <- expand.grid(x1= seq(0.001, 1, 0.001), x2 = seq(0.001, 1, 0.001))
  

###################
# CALIBRATE MODELS#
###################
  
#Logistic
  loglin <- glm(linear ~ x1 + x2, data = examp, family = binomial())
  logvaried <- glm(varied ~ x1 + x2, data = examp, family = binomial())
  logdiscont <- glm(discont ~ x1 + x2, data = examp, family = binomial())
  prob.grid.lin1 <- predict(loglin, grid, type = "response") 
  prob.grid.lin2 <- predict(logvaried, grid, type = "response") 
  prob.grid.lin3 <- predict(logdiscont, grid, type = "response") 
  
#RPART
  library(rpart)
  dectree <- rpart(linear ~ x1 + x2, data = examp, cp = 0)
  decvaried <- rpart(varied ~ x1 + x2, data = examp, cp = 0)
  decdiscont <- rpart(discont ~ x1 + x2, data = examp, cp = 0)
  prob.grid.rpart1 <- predict(dectree, grid)
  prob.grid.rpart2 <- predict(decvaried, grid)
  prob.grid.rpart3 <- predict(decdiscont, grid)
  
#Random Forest 
  library(randomForest)
   adatree <- randomForest(factor(linear) ~ x1 + x2, data = examp, mtry = 2)
   adavaried <- randomForest(factor(varied) ~ x1 + x2, data = examp, mtry = 2)
   adadiscont <- randomForest(factor(discont) ~ x1 + x2, data = examp, mtry = 2)
   prob.grid.gbm1 <- predict(adatree, grid, type = "response")
   prob.grid.gbm2 <- predict(adavaried, grid, type = "response")
   prob.grid.gbm3 <- predict(adadiscont, grid, type = "response")
   
#KNN
   library(kknn)
   gammod1 <- kknn(linear ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   gammod2 <- kknn(varied ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   gammod3 <- kknn(discont ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   prob.grid.gam1 <- fitted(gammod1)
   prob.grid.gam2 <- fitted(gammod2)
   prob.grid.gam3 <- fitted(gammod3)

   
  
################
# PROBLEM SPACE#
################
  
  par(mfrow = c(1,3), mar = c(0, 1.2, 0.9, 0))
  
  #Linear example
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  title(main="(1) Linear", line= -0.2, cex.lab=1.3, font=3)
   contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.lin1, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  
  #Non-linear
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
    title(main="(2) Non-Linear", line= -0.2, cex.lab=1.3, font=3)
    contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gbm2, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
    
  #Discontinuous
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
   title(main="(3) Discontinuous", line= -0.2, cex.lab=1.3, font=3)
     contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gbm3, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
```

The function that determines the decision boundary can take on many forms and each balances interpretability and accuracy. A simple linear boundary can be represented using a *logistic regression* -- the workhorse of the natural and social sciences. While the method provides an analytically convenient answer, the resulting decision boundary may miss finer, more disjointed patterns. Non-parametric methods offer a far more flexible solution to prediction, but at the cost of interpretability. A simple technique known as *k-nearest neighbors* is useful when k-number of similar, comparable observations can serve as a reference for informing the prediction of a given point. *Decision tree learning* along with its many variants such as *Random Forest* learn patterns by partitioning a sample into finer more homogeneous sub-samples. When faced with infinite distributions of data, each method responds to the circumstances differently.


```{r, echo = FALSE, fig.cap = "Linear, Non-Linear and Discontinuous Classification Problems.", fig.height = 5}

#####################   
# plot the boundary##
#####################
   
  par(mfrow = c(3,4), mar = c(0, 1.2, 0.9, 0))
   
  #Linear example
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.lin1, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Logistic Regression", line= 0, cex.main=1.2, font.main = 1)
  title(ylab="Linear", line= -0.2, cex.lab=1.3, font=3)

  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gam1, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="kNN", line= 0, cex.main=1.2, font.main = 1)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.rpart1, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Decision Tree", line= 0, cex.main=1.2, font.main = 1)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gbm1, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Random Forest", line= 0, cex.main=1.2, font.main = 1)
  
  
  #Non-linear
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.lin2, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(ylab="Non-Linear", line= -0.2, cex.lab=1.3, font=3)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gam2, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.rpart2, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gbm2, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  #Discontinuous
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.lin3, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(ylab="Discontinuous", line= -0.2, cex.lab=1.3, font=3)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gam3, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.rpart3, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)

  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.001, 1, 0.001), y = seq(0.001, 1, 0.001), 
          z=matrix(prob.grid.gbm3, nrow=1000), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
```

Each technique has its own set of assumptions that in turn make it better suited for certain policy problems and certain types of data. But there are basic considerations that underlie our use of any  classifier, namely  (1) the idea of separability, (2) balancing interpretability with prediction, and (3) the different definitions of accuracy.


### Separability 
 A fire prediction algorithm needs to actually predict where fires and non-fires will be. A bail algorithm needs to be able to distinguish between those who are flight risks and those who are not. Simply training a classifier will not do. The success of a classifier fundamentally lies in if the classes are  *separable* based on its input variables -- basically the idea that the algorithm can learn from the underlying data and distinguish one class from another. Successfully distinguishing between classes can be measured through various forms of accuracy, but fundamentally is dependent on finding as many *True Positives* (TP) and *True Negatives* (TN) as possible.

Separability manifests itself differently depending on the data type. When the inputs are discrete variables, the goal is to find a set of variables that can best describe the target.  In the example below, we analyze how building violations are related to the target fires. The ideal situation is to place the mass of sample along the diagonal with TNs (no fire - no violation) and TPs (fire - violation). In this case, whether a building had a building violation can accurately distinguish 80% of observations.



```{r, echo = FALSE, message = FALSE, warning = FALSE}
#Load libraries
  library(kableExtra)
  library(knitr)

#Sample size
  n <- 1200
  
#Generate simulated fire data
  id <- 1:n
  y <- rep(NA, n)
  
  set.seed(20000)
  yhat <- 0.8/(1 + exp(-id/100 +7 +rnorm(n, 0, 0.3)))
  
  for(i in 1:n){
    set.seed(i*200)
    y[i] <- (runif(1) < yhat[i])*1
  }
  y <- y == 1
  
#Gen square footage
  set.seed(10000)
  sf <- round((sqrt(id)*1000 + 40000 + rnorm(n, 6000,4000)))/1000
  
#Gen Bldgs
  bldgs <- c(rep("Elevator Apt", 50000),
             rep("Warehouse", 4000),
             rep("School", 1000),
             rep("Theater", 300))
  
#Create data frame
  set.seed(102)
  df <- data.frame(building_id = 1:n, 
                   time = 2010,
                   fire = y, 
                   class = sample(bldgs, n, replace = TRUE), 
                   square_feet_k = sf, 
                   prev_violation = NA, 
                   gas_leaks = NA)
 
#Create gas leaks and prev vios
  for(i in 1:nrow(df)){
    set.seed(i)
    df$prev_violation[i] <- ifelse(df$fire[i], runif(1) < 0.5 + 
                                     (runif(1)*0.3) , runif(1) < 0.1)
    set.seed(i*2)
    a<- runif(1)
    set.seed(i*3)
    df$gas_leaks[i] <- ifelse(df$fire[i], a < 0.35 - (runif(1)*0.3), runif(1) < 0.1)
  }
 
#Yank out top 9 for show and tell
  set.seed(100)
  df2 <- df[order(runif(n)),]
  short <- df2[1:9,]
  temp <- data.frame(building_id = ".", 
                     time = ".",
                     fire = ".", 
                     class = ".", 
                     square_feet_k = ".", 
                     prev_violation = ".", 
                     gas_leaks = ".")
  short <- rbind(short,temp, temp, temp)
  # kable(short, 
  #       row.names = FALSE, 
  #       booktabs = TRUE, 
  #       caption = "Simulated fire prediction data set",
  #       col.names = c("ID", "Year", "Fire?", "Building Type", 
  #                     "Square Feet ('000)","Prior Building Violation", 
  #                     "Prior Gas Leaks")) 

#Confusion Matrix

  fire <- rep(NA,nrow(df))
  fire[df$fire] <- "Fire"
  fire[!df$fire] <- "No Fire"
  short <- ((table(fire, df$prev_violation)))
  short <- round(short*100/nrow(df),1)
  kable(short, 
        booktabs = TRUE, 
        caption = "Separability of two classes given a separable discrete variable.",
        col.names = c("No Violation","Violation - T"),
        row.names = T)  %>%
            kable_styling(latex_options = c("hold_position"))
```

Alternatively, a low separable case may will have a large proportion of observations along the other diagonal, indicating many *False Positive* (Type I error) and *False Negatives* (Type II error). We can see that the majority of cases have been incorrectly classified. Classification problems are a balancing act: it is unlikely that a model will yield 100% accuracy everytime and often times, we will need to weigh accuracy against classification error. As we will see later in the chapter, more True Positives may also lead to more False Positives and how we balance these are very much a policy decision. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}

#Confusion Matrix
  fire <- rep(NA,nrow(df))
  fire[df$fire] <- "Fire"
  fire[!df$fire] <- "No Fire"
  short <- ((table(fire, df$class=="Elevator Apt")))
  short <- round(short*100/nrow(df),1)
  kable(short, 
        booktabs = TRUE, 
        caption = "Separability of two classes given an non-separable discrete variable.",
        col.names = c("Elevator Apartment - F","Elevator Apartment- T"),
        row.names = T)  %>%
            kable_styling(latex_options = c("hold_position"))
```

When inputs are continuous variables, separability can be assessed in terms of means and dispersion of the target classes. For example, a low separability scenario (left) would be one where the input variable's distribution for each class substantially overlap, which suggests an absence of any distinguishing information. High separability (middle), in contrast, would have means that are significantly different from one another and the distributions themselves overlap minimally. But perhaps the neatest thing is perfect separability (right). In this case, we basically can produce a perfect definition of the outcome in question, in which case we do not need a classifier to model the relationship -- just a threshold to serve as the definition. 

```{r, fig.height = 2, warning=FALSE, message=FALSE, echo = FALSE, fig.cap = "Separability of two classes given a continuous variable."}

#Libs
  library(ggplot2)
  library(grid)
  library(gridExtra)

#Prep data
  a <- rnorm(1000,10,10)
  b <- rnorm(1000,12,10)
  c <- rnorm(1000,50,10)
  d <- rnorm(1000,100,10)
  sep.df <- data.frame(a, b, c, d)

#Plot
  lowsep <- ggplot(sep.df) + 
            geom_density(aes(a), fill = "navy", alpha = 0.3)  +
            geom_density(aes(b), fill = "orange", alpha = 0.3) + 
            ggtitle("Low Separability" ) + 
            theme(plot.title = element_text(size = 10,hjust = 0.5), 
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  legend.position="none",
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_blank(),
                  panel.grid.minor=element_blank(),
                  plot.background=element_blank())

    highsep <- ggplot(sep.df) + 
               geom_density(aes(a), fill = "navy", alpha = 0.3)  +
               geom_density(aes(c), fill = "orange", alpha = 0.3) +  
               ggtitle("High Separability" ) + 
               theme(plot.title = element_text(size = 10,hjust = 0.5), 
                     axis.line=element_blank(),
                     axis.text.x=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks=element_blank(),
                     axis.title.x=element_blank(),
                     axis.title.y=element_blank(),
                     legend.position="none",
                     panel.background=element_blank(),
                     panel.border=element_blank(),
                     panel.grid.major=element_blank(),
                     panel.grid.minor=element_blank(),
                     plot.background=element_blank())

    perfect <- ggplot(sep.df) + 
               geom_density(aes(a), fill = "navy", alpha = 0.3)  +
               geom_density(aes(d), fill = "orange", alpha = 0.3) +  
               ggtitle("Perfect Separability" ) + 
               theme(plot.title = element_text(size = 10,
                                               hjust = 0.5), 
                     axis.line=element_blank(),
                     axis.text.x=element_blank(),
                     axis.text.y=element_blank(),
                     axis.ticks=element_blank(),
                     axis.title.x=element_blank(),
                     axis.title.y=element_blank(),
                     legend.position="none",
                     panel.background=element_blank(),
                     panel.border=element_blank(),
                     panel.grid.major=element_blank(),
                     panel.grid.minor=element_blank(),
                     plot.background=element_blank())
#Plot
gridExtra::grid.arrange(lowsep, highsep, perfect, ncol = 3)
```

The bottom line about separability is that a good data scientist will check their assumptions. In policy, there will be prevailing theories and conventional wisdom that dictate how certain factors influence a phenomenon. But what may sound good may in actuality have little separability and in turn offer little predictive power. In those cases, it is worth revisiting and revising the assumptions. After all, the goal of a classifier is to classify. 


### Measures of accuracy 

Accuracy is central to evaluating if a classifier's results are useful. This requires understanding of a minimum of two ideas: what does probability have to do with accuracy and what exactly is the subtance of accuracy. 

__Predicted Probability__. Most classifiers output a form of conditional probability. Accuracies are derived from probabilities produced by the classifier, indicating whether a given observation is predicted to below to a given class (e.g. fire vs. no fire, Yankee fan vs. Red Sox fan, Sith vs. Jedi, Republican v. Democrat). In order to convert probabilities into a prediction of a class, we need to set a threshold. So, what is the classification threshold? 

*Short answer: It depends on the sample.*

For balanced samples in which the outcome variable's classes are represented in approximately equal proportion, the threshold is $pr(Y=1) \geq 0.5$ as the objective probability of occurrence is close to $0.5$. In reality, this may not be all that common scenario due to *class imbalance* and *rare events*. Class imbalance is the case in which the *minority class* -- the class with fewer observations -- is proportionally less prevalent in the sample than other classes. For example, political campaigns often deal with voter data in districts that are overwhelming in favor of one party over another. Rare events are infrequent events in which there is a super minority of observations. Fires and disasters tend to fall into this category. In some cases, fewer than $n = 200$ observations may lead to biased estimates.^[https://gking.harvard.edu/files/gking/files/0s.pdf]. We will revisit class imbalance and rare events later in this chapter, but for now, let's assume that the classification threshold is $Pr(Y=1) \geq 0.5$. 

__Accuracy__. Classifier accuracy measures rely on metrics derived from the *confusion matrix*, or a $n \times n$ table where the rows represent actual classes and columns represent predicted classes. For a two class problem, the confusion matrix is a $2 \times 2$ table.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#Create confusion matrix
  temp <- data.frame(first = c("Actual (-)", "Actual (+)"),
                     pred_min = c("True Negative (TN)", "False Negative (FN)"),
                     pred_max = c("False Positive (FP)", "True Positive (TP)"))
  row.names(temp) <- temp$first
  knitr::kable(temp[,2:3], 
        row.names = TRUE, 
        booktabs = TRUE, 
        caption = "Structure of confusion matrix",
        col.names = c("Predicted (-)","Predicted (+)")) 
```


Each cell of the confusion matrix is a building block required to calculate accuracy:

- The True Positive (TP) is the number of cases in which the actual positive observations ($Y = 1$) are correctly predicted (e.g. model predicts a fire and a fire actually occurs). Note that *P* is used to denote the total number of positive records.
- The True Negative (TN) is the number of  cases where the actual negative observation ($Y = 0$) are correctly predicted. Note that *N* is used to denote the total number of negative records.
- The False Positive (FP) is number of cases where the actual label was $Y = 0$, but the model classified a record as $\hat{Y} = 1$. This is also known as *Type I error*.
- The False Negative (FN) is number of cases where the actual label was $Y = 1$, but the model classified a record as $\hat{Y} = 0$. This is also known as *Type II error*.

In a perfectly balanced sample of $n = 100$, we would expect the $P = 50$ and $N = 50$. Likewise, a perfect predictions should yield $TP = P$ and $TN = N$ along the diagonal.  This is a rare case, but it  stands that the goal is to check if the download diagonal captures the majority of observations.


```{r, echo = FALSE, message = FALSE, warning = FALSE}
#Create confusion matrix
  temp <- data.frame(first = c("Actual (-)", "Actual (+)"),
                     pred_min = c(50, 0),
                     pred_max = c(0, 50))
  row.names(temp) <- temp$first
  knitr::kable(temp[,2:3], 
        row.names = TRUE, 
        booktabs = TRUE, 
        caption = "Confusion matrix for perfectly accurate predictions",
        col.names = c("Predicted (-)","Predicted (+)")) 
```

In contrast, a model with little predictive power will have the majority of observations in the upward diagonal. Below, the matrix suggests the trained model has little predictive power as it classified the vast majority of records as negative even when some should have been positive. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
#Create confusion matrix
  temp <- data.frame(first = c("Actual (-)", "Actual (+)"),
                     pred_min = c(40, 45),
                     pred_max = c(10, 5))
  row.names(temp) <- temp$first
  knitr::kable(temp[,2:3], 
        row.names = TRUE, 
        booktabs = TRUE, 
        caption = "Confusion matrix for poor predictions.",
        col.names = c("Predicted (-)","Predicted (+)")) 
```


Despite having well-defined building blocks of accuracy, the tricky thing is that there is not just one measure of accuracy or error. In fact, there are many measures of accuracy that emphasize very different predictive qualities.

^^ADD EXAMPLE HERE



| Measure | Formula | What It Answers |
|----------------+-----------------------+----------------------------|
| __Individual Measures__|
| True Positive Rate (TPR), Sensitivity, or Recall | $TPR = \frac{TP}{TP+FN}$ |  What proportion positive cases were correctly identified? |
| True Negative Rate (TNR) or Specificity| $TPR = \frac{TN}{TN+FP}$ |  What proportion negative cases were correctly identified? |
| False Positive Rate (FPR) | $FPR = \frac{FP}{FP+TN}$ |  What proportion of negative cases were incorrectly predicted as positive? This is also known as the false detection rate or Type I error. |
| False Negative Rate (FNR) | $FNR = \frac{FN}{TP+FN}$ |  Proportion of positive cases that were incorrectly predicted negative. This is also known as the false alarm rate or Type II error. |
| Predicted Positive Value (PPV) or Precision | $PPV = \frac{TP}{TP+FP}$ |  Proportion of positive cases that were incorrectly predicted negative.|
| __Overall Measures__|
| Accuracy (ACC) | $ACC = \frac{TP + TN}{n}$ |  What proportion of  records were correctly classified? |
| F1-Score (F1) | $F1 = \frac{2}{\frac{1}{TPR} \times \frac{1}{FPR}}$ |  Alternative method of calculating accuracy using a harmonic mean |
| Area Under the Curve (AUC) or Concordance statistic | $AUC = \int_0^1 ROC(u)du$ |  The AUC is the area under the  Receiving-Operating Characteristic (ROC) Curve. The ROC is derived by plotting values of TPR (Sensitivity) and FPR for varying probability cutoffs $c$.  |



### Interpretability versus prediction 

When selecting models for classification tasks, data scientists often times balance interpretability and predictive accuracy. Some situations require a narrative to communicate the insights. In the social and natural sciences, classification methods like logistic regression are favored for their use in *parameter estimation* to infer relationships between variables captured in coefficients. These coefficients are directly interpretable, showing how a variable can contribute or detract from the probability of an outcome holding all else constant. For example, a building built X decades ago is Y-times more likely to catch fire than a building built this decade holding all else constant. This simple factoid can facilitate narratives that communicate insight. But there is a tradeoff. While interpretable methods may extract the gist of the relationships, they may at times miss the finer variations in the data necessary for a reliable prediction. 

Alternatively, some audiences are far less interested in the story, but just want an accurate number. In other words, predictive accuracy is prized in which case it may be worth considering a body of exciting methods that have arisen from statistics and computer science that are optimized for the task. These other methods may be more versatile, adapt to scenarios where there are more variables than observations, find interactions between variables and nonlinear patterns, and optimize for robustness. In the tech sector, for example, data science pursuits are often a matter of how well classifiers can scale to service their customers and drive sales. Understanding which variables definitively drive the predictions can be useful for teams focused on communicating insights, but not as much for the technology side of the house.

The bottom line is that one's choice of classifier depends on how much one values interpretability versus accuracy.

### Fairness and transparency 

In the area of criminal justice, algorithms such as COMPAS are actively being used to predict the chance that someone accused of a crime will recidivate within two years. While classifiers may offer a degree of efficiency when sifting through a multitude of data and are relied upon by judges in passing sentences, there are significant concerns regarding their ethical use. In 2016, ProPublica conducted research that examines the fairness of risk ratings from COMPAS, finding that while the algorithm is able to correctly predict recidivism 61% of the time, black defendents are 1.9-times more likely than white counterparts to be labeled high risk but not actually re-offend.^[https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing] In other words, if two defendents with the same criminal history were scored by COMPAS and the only difference is their race, the two defendents would be treated differently. 

It is unsurprising that *fairness* and *transparency* are emerging areas of focus in the use of classifiers. Fairness can take on many definitions, but for the purpose of this text, we define it as whether two or more subpopulations receive equal treatment. Transparency means that the provenance of the underlying information and its method of producing predictions are available for review -- not that it fits a neat normative, but that it can be traced and scrutized. 
 


### Common techniques

There are hundreds of classification algorithms that are easily accessible using modern data science software, but six algorithms cover much of the use cases that are relevant for public policy. Each method has its strengths and weaknesses, and each is more appropriate in certain contexts than others. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/use_case_table_v2.csv")
  colnames(usescases) <- c("Method", "Common Uses",  "Example Data")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "Overview of classifiers", 
                 justify = "left")

```

## K-Nearest Neighbors (KNN)

As hurricanes become more intense and leave a trail of destruction, city governments will need to be able to more efficiently triage requests for help. Let's take the example of Hurricane Sandy and its effect on NYC. One of the main services offered by cities is the management and care of its trees. A downed tree can cause property damage, bodily harm and traffic disruptions. Due to the high wind and lush foliage during Sandy, many trees fell.

In NYC, the Department of Parks and Recreation is responsible for tree removal. When a resident makes a call to the city's services hotline 311, a work order is created and a tree removal team is dispatched. This may be a transactional process: one call for tree removal, one tree is then removed. As it takes time for crews to move and set up, a first-in/first-out queuing process can be inefficient. Imagine if 20 of 100 blocks in a neighborhood were flagged for tree removal. It would make sense to use that call data to identify other blocks that may also have downed trees.

We would expect that downed trees are more likely to occur in *pockets* and proximity is the best indicator of activity. As the city knows where residents call for tree- and non-tree-related issues, we can use the location of the calls to triangulate on likely problem areas as well as anticipate pockets of yet-to-be-reported downed trees, or at least serve that is a reasonable working theory.

For this task of predicting based on proximity, k-Nearest Neighbors (KNN) can help.

### Under The Hood

K-nearest neighbors (KNN) is a non-parametric, instance-based algorithm that is based on a simple idea: *observations that are closer together are more likely to be similar*.  The method is non-parametric as it does not directly use its inputs $x$ to determine the value of $y$, but rather assumes that inputs serve as ways to quantify how close points are. It is instance-based as each prediction is determine on a case-by-case basis using its surrounding neighbors. 

The technique is simple. For each case $y_i$:

1. _Distance_. First, we calculate the distance $d$ to all other records with known outcomes. Distance most commonly takes the form of Euclidean distance, which is appropriate with continuous values. For cases where the underlying data are boolean or binary, Manhattan distance may be more appropriate. In effect, the input variables $X$ serve as sets of coordinates to triagulate which points are closer to a given example. Note that treating variables like coordinate sets implies two things. First, all variables have equal importance -- no single variable should weigh in on distance more than any other. Second, the scale of the observations would need to be the same.

$$ d = \text{distance} = \sqrt{\sum_{i=1}^n(x_{i} - x_{0})^{2} }$$ 



2. _Voting_. For the $k$ nearest observations to a given observation, calculate the proportion of observations in each class $j$ in $Y$.  This procedure yields a conditional probability for each observation, which is converted into a predicted class through *majority voting* -- assign an observation to the class that is most represented in the neighborhood.   There are various flavors of the voting calculation that account for distance from a given observation. 


| Voting Type | Formula | Interpretation|
|-----------------+------------------------------------+-----------------------------------------|
| Rectangular | $Pr(Y = j) = \frac{1}{k}\sum_{i=1}^k I(y^i = j)$ | Calculate the proportion of $j$ based on $k$ nearest neighbors.|
| Inverse |$Pr(Y = j) = \sum_{i =1}^k w(d)(y^i = j)$  where $w(d) = \frac{1}{d_i\sum_{i=1}^k(\frac{1}{d_i})}$|Calculate the weighted proportion of $j$ based on the inverse distance to $k$ nearest neighbors.|


3. _Tuning_. The method is sensitive to the value of $k$, requiring tuning -- or testing different values of $k$. When $k = 10$, the conditional probability for $y_i$ reflects the 10-nearest neighbors. When $k = n$, the conditional probability is the sample mean. 


The above process yields the results for just one value of $k$. *Is that value of $k$ the right one?* Like many other algorithms, KNNs are an iterative procedure, requiring tuning of *hyperparameters* -- or values that are starting and guiding assumptions of a model. In the case of KNNs, $k$ is a hyperparameter and we do not precisely know the best value of $k$. Tuning hyperparameters involves a grid search in which a range of possible hyperparameters at equal intervals are tested and compared to find the optimal.

To illustrate how tuning works, we have extracted a raster data from the US Department of Agriculture (USDA) CropScape data layer derived from satellite imagery.^[https://nassgeodata.gmu.edu/CropScape/] In particular, we focus on a small area in Kansas that grows corn (yellow) and soybeans (green). More often than not, data will only contain a sample of the full picture. In the case of spatial data, KNNs are quite useful for filling in the full picture. Suppose we only have a 10% sample of the farmland -- how important is $k$? We can see that as the value of $k$ grows, the corn fields become overly represented as it is the majority class in the sample. The TPR is highest when the TNR is highest, demonstrating that there is a balancing act when choosing the value of $k$.

```{r, fig.height=4, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "Comparison of prediction accuracies for various values of k."}

#LOAD LIB
  library(pacman)
  p_load(raster, sp, rgdal, kknn)
  
#LOAD DATA
  crops <- raster("data/cropscape.png")

#SET UP DATA
  vec <- as.data.frame(crops, xy = TRUE)
  
  set.seed(123)
  kg <- kmeans(vec$cropscape, 3)
  cll <- kg$cluster
  labs <- rep("corn", length(cll))
  labs[cll == 2] <- "soybeans"
  
  color <- rep("#ffff00", length(cll))
  color[cll == 2] <- rgb(0,1,0)
  color[cll == 1] <- "#95b1cc"
  
  
  vec <- cbind(vec, color, labs, cll)

 
# plot BASE example
  par(mfrow = c(2,3), mar = c(0, 1.2, 0.9, 0))
  
  plot(vec$x, vec$y, pch = 16, cex = 0.3, col = as.character(vec$color),
       axes = FALSE, legend = FALSE, asp = 1,  xlab = "", ylab = "")
  title(main="Farmland (Actual)", line= 0, cex.main=0.9, font.main = 1)
  
  #Randomize pulls and train for various k
  set.seed(123)
  train <- vec[runif(nrow(vec)) <= 0.1,]
   plot(train$x, train$y, pch = 16, cex = 1, col = as.character(train$color),
       axes = FALSE, legend = FALSE, asp = 1,  xlab = "", ylab = "")
  title(main="Farmland (5% sample)", line= 0, cex.main=0.9, font.main = 1)
  
   logger <- data.frame()
  
  for(i in c(1,5,10,100)){
  library(kknn)
  k1 <- kknn(color ~ x + y, 
              train = train, 
              test = vec,
              k = i,
              kernel = "rectangular")
  
  est <- table(vec$labs == "corn", k1$fitted.values == "#ffff00" )
  tpr <- round(100*est[2,2]/sum(vec$labs == "corn"),2)
  fpr <- round(100*est[1,2]/sum(vec$labs != "corn"),2)
  
  plot(vec$x, vec$y, pch = 16, cex = 0.3, col = as.character(k1$fitted.values),
       axes = FALSE, legend = FALSE, asp = 1, xlab = "", ylab = "")
      title(main=paste0("k = ", i, ": TPR = ", tpr, ", FPR = ", fpr), line= 0, cex.main=0.9, font.main = 1)
      
      logger <- rbind(logger,
                      data.frame( k = i,
                                  tpr = tpr,
                                  fpr = fpr))
  
  }

    
```

### Tips of the trade

__Tuning__.  As the accuracy of a KNN model is dependent on finding the optimal value of $k$, it makes the process far simpler to think about the tuning process as a grid search. A systematic way of testing for $k$ is try test all values from $k=1$ to $k=\sqrt{n}$ in multiples of one's choosing. The idea is to get a ballpark sense of what works, then hone in on the best value of $k$. Log the values of $k$ that are tested and compare the accuracies to find the optimum. For example, searching for values of $k$ that provide the best prediction in the cropland example suggests that TPR is near its max value at $k=1$ and the FPR is minimized at the same value, thus choosing the minimum value of $k$ would likely be adequate.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "TPR (blue) and FPR (orange) for values of k.", fig.height = 2}

plot(logger$k, logger$tpr, type = "l", col = "blue", ylim = c(0,100), ylab = "Percent", xlab = "k")
points(logger$k, logger$tpr, col = "blue", pch = 16)
lines(logger$k, logger$fpr, col = "orange")
points(logger$k, logger$fpr, col = "orange", pch = 16)
```


There are a number of other factors that influence the performance of the algorithm:

- _Scale_. A trained KNN is influenced  by the scale of its inputs. If one input has a scale from 0 to 10,000 and another ranges from 0.1 to 0.3, a KNN will lean more heavily on the latter variable. To ensure equal weights, it is necessary to transform variables into a standardized scale:

$$ scaled = \frac{x_i - \mu}{\sigma}$$ 

where the result transformed variable is mean centered and standardized. The new unit is standard deviations.
    
- _Grids_. Similar to the scale issue, KNNs are particularly effective in data that are distributed on a grid -- measurements along a continuous scale at equal incremenets, but may be a poor choice when the data are mixed data formats such as integers and binary.

- _Symmetry_. It's key to remember that neighbors around each point will not likely be uniformly distributed. While kNN does not have any probabilistic assumptions, the position and distance of neighboring points may have a skewing effect. 


__Usage__. KNNs are great in some cases; Not so much in others. 

KNNs are commonly associated with imputation of missing values and scenarios where proximity of observations has some bearing on the predictive accuracy. But setting up the KNNs requires some care.

As scale matters, data sets with mixed data types (discrete, continuous) need to be transformed into the same units. Discrete variables can be converted into a dummy variable matrix. Continuous variables can be binned into discrete levels, then converted into a dummy variable matrix as well. This effectively means that all variables are in terms of 0/1 and a Minkowski distance may be more appropriate to relate distances than Euclidean. 

KNNs are best used when data sets are relatively smaller with fewer variables as each distance calculation is computationally taxing. Furthermore, as more variables are added, the importance of any one variable is diluted -- it may be worth trying another algorithm to sift through the data.

Lastly, KNNs are not interpretable as it is a nonparametric approach. It should be instead be viewed as a processing method to fill in the gaps. 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/knn_sw.csv")
  colnames(usescases) <- c("Useful Properties", "Challenges")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "The good and ugly of KNNs.", 
                 justify = "left")

```


### DIY: Anticipating the extent of damage from a storm

We now turn back to our motivating example. Suppose the location of all calls for non-emergency help from the day of Hurricane Sandy are captured in NYC's 311 system, yet there are still neighborhoods that likely have downed trees but have not reported it. *From what we know, how can we guess the disposition of other parts of the city to have a fuller picture?*

__Prepare the data__. Some calls for help are associated with downed trees while others may concern non-emergency issues. By this logic, we could assume that a downed tree would be called in if a call were made at all. 

Our data set contains $n=7513$ observations, each of which is a 1000 foot by 1000 foot area. When plotted, the data set captures the outline of NYC.

```{r, message = FALSE, warning = FALSE, fig.height = 3}
#Load data (need persistent link)
  nyc <- read.csv("data/sandy_trees.csv")

#Plot
  plot(x = nyc$xcoord, y = nyc$ycoord, 
       xlab = "x coordinate", ylab = "y coordinate",
       cex = 0.3,  pch = 15, asp = 1, col = "grey")
```

For simplicity, we will focus on the two largest and geographically connected boroughs, Brooklyn and Queens. The remaining boroughs of the city are separated by rivers, which may be better represented using separate models. We subset our data using the `boro` variable to focus, scale the geographic coordinates using the `scale` function, and  map the resulting subset.

```{r, message = FALSE, warning = FALSE}

#Subset
  nyc <- subset(nyc, boro %in% c("QN", "BK"))

#Standardize input variables
  nyc$xcoord <- scale(nyc$xcoord)
  nyc$ycoord <- scale(nyc$ycoord)
  
#Plot
  plot(x = nyc$xcoord, y = nyc$ycoord, 
       xlab = "x coordinate", ylab = "y coordinate",
       cex = 0.3, pch = 15, asp = 1, col = "grey")
```





__Train__. While this is a retrospective analysis, we simulate the process of producing the complete map as if we had partial information. The `nyc` data frame is split into a `train` set, keeping only locations where the target variable `tree.sandy` are available. A quick tabulation shows that the $n=1550$ of the $n=1946$ training set observations have a downed tree reported.

```{r}

#Subset training sample
  train <- subset(nyc, !is.na(tree.sandy), 
                  select = c("ycoord", "xcoord", "tree.sandy"))

#Split out
  table(train$tree.sandy)
```

The test set is the entirety of Brooklyn and Queens. The `tree.next7` variable flags any location that had a report of a downed tree over the seven days after the hurricane.

```{r}
  test <- subset(nyc, 
                 select = c("ycoord", "xcoord", "tree.next7"))
  
```

With the data in the right shape, we load the `kknn` library:

```{r}
#Call "class" library
  library(kknn)
```


The KNN algorithm needs to be calibrated for the best $k$ using the training set, then applied to a test set. To do this, we will use the `kknn` library. The training portion uses the `train.kknn()` function to conduct k-folds cross validation, then the scoring uses `kknn()`. While both functions can be fairly easily written from scratch (and we encourage new analysts to write their own to intimately understand the assumptions), we will plow forth with using the library.

In order to find the optimal value of $k$, we will execute the `train.kknn()` function, which accepts the following arguments:

`train.kknn(formula, data, kmax, kernel, distance, kcv)`

- `formula` is a formula object (e.g. "`no.coverage ~ .`").
- `data` is a matrix or data frame of training data.
- `kmax` is the maximum number of neighbors to be tested
- `kernel` is a string vector indicating the type of distance weighting (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations, "gaussian" imposes a normal distribution on distance, "inv" is inverse distance).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).
- `kcv` is the number of partitions to be used for cross validation.

The flexibility of `train.kknn()` allows for test exhaustively and find the best parameters. Below, we conduct 20-folds cross validation testing between $k=1$ and $k = 100$ neighbors using two kernels (rectangular and inverse) that impact the voting step. This simple command does much of the hard work by running the KNN algorithm 2000 times (20 cross-validation models for each $k$ and *kernel* combination), then surfaces the best parameters. We store the results in `fit.cv`.


```{r, message = FALSE, warning = FALSE, results = 'hide', echo = TRUE}

#Set seed to ensure cross validation is replicable
  set.seed(100)

#Run with 20-folds cross validation
  fit.cv <- train.kknn(tree.sandy ~ ycoord + xcoord , 
                      data = train, 
                      kcv = 20, 
                      distance = 1, kmax = 100, 
                      kernel = c("rectangular", "inv"))

```


Within `fit.cv` is a `best.parameters` element that KNNs perform the best when $k = $ `r fit.cv$best.parameters$k` using an inverse distance kernel.


```{r, message=FALSE, warning = FALSE, fig.cap = "20-fold cross validated errors for k = 1 to k = 100"}

#Plot Cross Validation
   plot(fit.cv)
   
#Retrieve best parameters
   best <- fit.cv$best.parameters
```

With the KNN algorithm tuned, we can now proceed to scoring the test set using the `kknn()` function. The function syntax is as follows:

`kknn(formula, train, test, k, kernel, distance)`

- `formula` is a formula object (e.g. "`no.coverage ~ .`").
- `train` is a matrix or data frame of training data.
- `test` is a matrix or data frame of test data.
- `k` is the number of neighbors.
- `kernel` is the type of weighting of distance (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g.  1 = binary, 2 = euclidean,).

Notice that in the following code block, we train the KNN and apply it to the test sample all in one step as the KNN itself does not learn patterns, but just applies a simple calculation following a pre-specified routine. This is a marked difference compared with other algorithms covered in this chapter.

```{r, message=FALSE, warning = FALSE}

#Apply tune KNN parameters
   fit <- kknn(tree.sandy ~ ycoord + xcoord, 
               train = train, 
               test = test,
               k = best$k,
               kernel = best$kernel)

#Produce 
    test$prob <- fit$fitted.values
    test$tree.next7[is.na(test$tree.next7)] <-0
```


__Evaluate__. With all the right pieces computed, we can examine how closely the predictions based on tree downing patterns on the day of Hurricane Sandy compare with where trees were reported to have fallen over the 7 days that followed. During the storm, approximately `r paste0(round(100*sum(!is.na(nyc$tree.sandy))/nrow(nyc)), "%")` percent of the focus area made a call, of which `r paste0(round(100*sum(nyc$tree.sandy, na.rm=T)/sum(!is.na(nyc$tree.sandy))),"%")` reported a downed tree. This appears as a cloud of points capturing the gist of the downed tree pattern. 

```{r, message=FALSE, warning = FALSE, fig.cap = "Graphical comparison of actual and predicted areas with reported downed trees. Red indicates at least one tree was reported in a given 0.359 square-mile area", fig.height = 4, fig.width = 8}

  
 par(mfrow = c(1,3))

  plot(train[,2:1], main = "(1) Calls during storm",
       col = rgb(train$tree.sandy , 0, 1- train$tree.sandy, 1), 
       cex = 0.4, pch = 16, asp = 1)
  
  plot(test[,2:1], main =  "(2) Predicted probabilities",
       col = rgb(test$prob, 0, 1 - test$prob, 1), 
       cex = 0.4, pch = 16, asp = 1)
  
  plot(test[,2:1], main =  "(3) Actual next 7 days",
       col = rgb(test$tree.next7, 0, 1 - test$tree.next7, 1), 
       cex = 0.4, pch = 16, asp = 1)
    
```

Using the predicted probabilities for the test sample, we calculate the TPR and FPR using both a naive cutoff threshold ($p=0.5$), finding a high TPR but a FPR of almost similar magnitude. Using such prediction would not provide any insight to field crews.

```{r}
tab <- table(test$prob >= 0.5, test$tree.next7)
tpr <- tab[2,2]/sum(test$tree.next7)
fpr <- tab[2,1]/sum(test$tree.next7 == 0)
print(paste0("TPR = ", tpr, ", FPR = ", fpr))
```

However, a more informed cutoff based on the first day's probability of a downed tree (`r paste0("p = ",round(sum(nyc$tree.sandy, na.rm=T)/sum(!is.na(nyc$tree.sandy)),2))`) yields slightly more balanced results -- sacrifiing some true positives for far fewer false positives.

```{r}
tab <- table(test$prob >= 0.8, test$tree.next7)
tpr <- tab[2,2]/sum(test$tree.next7)
fpr <- tab[2,1]/sum(test$tree.next7 == 0)
print(paste0("TPR = ", tpr, ", FPR = ", fpr))
```

The test model accuracy can also be calculated by taking the Area Under the Curve (AUC) of the Receiving-Operating Characteristic. The ROC calculates the TPR and FPR at many thresholds, that produces a curve that indicates the general robustness of a model. The AUC is literally the area under that curve, which is a measure between 0.5 and 1 where the former indicates no predictive power and 1.0 indicates a perfect model. 

In order to visualize the ROC, we will rely on the `plotROC` library, which is an extension of `ggplot2`. We will create a new data frame `input` that is comprised of the labels for the test set `ytest` and the predicted probabilities `test.prob`. 

```{r, warning=FALSE, message=FALSE,}
#Load libraries
  library(ggplot2)
  library(plotROC)

#Set up test data frame
  input <- data.frame(ytest = test$tree.next7, 
                      prob = test$prob)
```

We then will first create a ggplot object named `base` that will contain the labels (`d = `) and probabilities (`m = `), then create the ROC plot using  `geom_roc()` and `style_roc()`. A ROC curve for a well-performing model should sit well-above the the 45 degree diagonal line, which is the reference for an AUC of 0.5 (the minimum expected for a positive predictor). However, as the curve is below the 45 degree line, we may have a seriously deficient model. 

```{r, message = FALSE, warning=FALSE, fig.height = 3, fig.cap = "ROC curve out of sample"}
#Base object
  roc <- ggplot(input, aes(d = ytest, m = prob)) + 
         geom_roc() + style_roc()
  
#Show result
  roc
```


As estimated using `calc_auc()`, the out-of-sample AUC is `r round(calc_auc(roc)$AUC, 3)`, which is not a bad start. While we are able to fill impute the status of downed trees in other parts of the city, it is helpful to remember that the output of the KNN needs to match the intended use. If a limited number of field crews are deployed, then it may more sense to use the probabilities to prioritize neighborhoods. Otherwise, if additional resources could be hired, then knowing the total number of likely affected areas could inform how much to budget for the downed trees effort.

```{r, message=FALSE, warning = FALSE, fig.height = 2}
  calc_auc(roc)$AUC
```

Despite the promising result, we should be cognizant that KNNs generally are not the algorithm of choice of modelers unless there is relatively little data. We should thus ask: _Is there a better classifier?_ 


### Practice Exercises

The US Census Bureau's American Community Survey provides an in-depth view of life in America. One of the many features that are captured in the survey is healthcare coverage. Apply the above methods to predict healthcare coverage in the US State of Georgia in the year 2009.

The data can be obtained here [link to go here]: 

```{r, echo = FALSE, message=FALSE, warning=FALSE}
  library(digIt)
  health <- digIt("acs_health")
```

1. Randomly split the sample into a 50% training and 50% test set.

2. Predict healthcare `coverage` using continuous variables such as age (`age`) and `wage`. 

3. Calculate the performance on the test sample.


##Logistic Regression

__NEED A NEW BEGINNING__
For much of the natural and social sciences, the goal of classification is inference. Inference of how much specific factors are associated with an observed phenomenon -- not just prediction. The association typically are furnished with probabilistic qualities that allow an analyst to gauge how certain the pattern is. The output, in turn, lend themselves to building narratives. 

The statistically-driven narrative are part of our daily lives. Nowadays, it would not surprise one to hear that a smoker has X-times higher chance of developing cancer than a non-smoker.^[https://www.cdc.gov/cancer/lung/basic_info/risk_factors.htm] ^^MORE STATS EXAMPLES NEEDED HERE^^

These short empirical tid bits are rooted in a method known as *logistic regression*. Like ordinary least squares, logistic regressions are the workhorse of the social and natural sciences for inferring the marginal effects of input factors holding all else constant.

### Under The Hood

Before we dive into the particulars, we will cut straight to the chase: logistic regression is is best suited for cases where we believe that the decision plane between two or more classes is a straight line. It imposes strong linear assumptions on a problem, meaning that the method may gloss over any pattern that is non-linear or discontinuous. That aside, logistic regression has the ability to contribute to statistically-driven narratives and has a long tradition of probabilistic inference backing it.

Let's assume that the classes of a target variable $y$ can be dinstinguished using some linear combination of input variables $x1$ and $x2$. Upon graphing the features and color coding using the labels, you see that the points are clustered such that purple points represent to $z = 1$ and gold points represent $z = 0$. 


```{r, echo = FALSE, message=FALSE, warning=FALSE}
  library(digIt)
  health <- digIt("acs_health")
  
#Create index of randomized booleans of the same length as the health data set
  set.seed(100)
  rand <- runif(nrow(health)) 
  rand <- rand > 0.5
  
#Create train test sets
  train <- health[rand == T, ]
  test <- health[rand == F, ]
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 3, fig.width= 4, fig.cap = "A linearly separable problem."}
#Margin Example
margin_size <- -0.2
set.seed(123)
df <- data.frame(x = runif(1500),
                 y = runif(1500),
                 supports = NA)
  
  
#Best boundary
  df$z <- -1 + 3*df$x
  df$perp <- 0.6578033 + df$x*-0.5 
  df$perp[df$x >= 0.6951213] <- NA
  df$perp[df$x <= 0.4711213] <- NA
  
#Cut out
  df <- df[which((df$y > df$z + margin_size | df$y < df$z - margin_size | !is.na(df$supports))), ]
  df$group <- 0
  df$group[df$y < df$z - margin_size] <- 1
  df$group[df$x >0.6] <- 1
  df$cols <- "blue"
  df$cols[df$group == "Side B"] <- "green"
  
  
#Plot
# library(ggplot2)
# 
# ggplot(df, aes(group=factor(group))) + 
#     geom_point(aes(x = x, y = y,  colour = factor(group)))  +
#     ylim(0,1) + xlim(0,1) + 
#     ylab("x1") + xlab("x2") + scale_colour_manual(values=c("purple", "gold")) +
#     theme(plot.title = element_text(size = 10), 
#           axis.line=element_blank(),
#           axis.text.x=element_blank(),
#           axis.text.y=element_blank(),axis.ticks=element_blank(),
#           legend.position="none",
#           panel.background=element_blank(),panel.border=element_blank(),
#           panel.grid.major=element_blank(),
#           panel.grid.minor=element_blank(),plot.background=element_blank())
  
par(mfrow = c(1,1), mar = c(0, 1.2, 0.9, 0))
  vec <- rep("purple", nrow(df))
  vec[df$group==1] <- "gold"
  
  #Linear Boundary
  plot(df$x, df$y, col =vec, pch = 16, axes = F, xlab = "x1", ylab = "x2")
  lines(df$x, df$z+0.2)
  
```


We can express the relationship between $z$, $x_1$, and $x_2$ as a linear model similar to OLS:

$$z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$$

where $z$ is a binary outcome and, like OLS, $\beta_k$ are coefficients that are learned using a *Maximum Likelihood Estimation* or MLE. The simple idea of MLE is that coefficients can be  adjusted so the coefficients can work together to maximize the liklihood of mimicking the target. While the innards of MLE are beyond the scope of this text, for more in-depth treatment on the topic, refer to *Elements of Statistical Learning* ^[Ref needed] or *Introduction to Statistical Learning.*^[Ref needed]

The challenge with binary outcomes in a linear framework is that we run the risk that $\hat{y}$ would exceed the binary bounds of 0 and 1 and would thus make little sense. Imagine if the predicted $\hat{y}$ were -103 or +4 -- *What would that mean in the case of a binary variable? How would we interpret a positive value versus a negative?*  This is an obvious shortcoming of a linear model. Statisticians have cleverly solved the bounding problem by inserting the predicted output into a logistic function:

$$F(z) = \frac{1}{1+ e^{-z}}$$

For a random variable $x$ that takes on values from -10 to +10, the logit transformation converges to +1 where $x > 0$ and to 0 where $x < 0$. This S-shaped curve, or *sigmoid*, bounds $\hat{y}$ to a 0/1 range.  

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height = 2, fig.cap = "A sigmoid."}
set.seed(123)
l <- data.frame(x = seq(-10,10,.1))
l$logit <- 1/(1+exp(-l$x))
  
plot(l$x, l$logit, xlab = "x", ylab = "F(x)", col = "orange", type = "l")
points(l$x, l$logit,  col = "orange", pch = 16, cex = 0.5)
abline(v = 0, lty = "dashed")

```

By substituting the linear model output $z$ into the logistic function, we bound the output between 0 and 1 and interpret the result as a conditional probability:

$$p = Pr(Y=1|X) = F(z) = \frac{1}{1+ e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 )}}  $$

This may seem to be a convoluted set of formulas, but it serves a convenient purpose. Unlike many of the techniques in this book, logistic regression is directly interpretable so as long as we believe that the decision boundary is linear. To be able to state that, for example, smokers have a 15 to 30-times higher chance of lung cancer than non-smokers, we use coefficients as a means to contextualize relative effects of input variables. This requires some basic math. The odds of an event are defined as the following:

$$odds = \frac{p}{1-p}= \frac{F(z)}{1-F(z)}=e^z$$

In its purest form, probability $p$ can be calculated without a model, but to hold all covariates constant, we can fit the output of a logistic regression into this framework where $F(z)$ is a probability of some event $z = 1$and $1-F(z)$ is the probability of $z = 0$. The odds can be re-arranged as:

$$pr(success) = \frac{e^{(\beta_0 + \beta_1 x_1 + \beta_2 x_2 )}}{1+e^{(\beta_0 + \beta_1 x_1 + \beta_2 x_2 )}}$$
$$pr(failure) = \frac{1}{1+e^{(\beta_0 + \beta_1 x_1 + \beta_2 x_2 )}}$$

Typically, we deal with *odds* in terms of *log odds* as the exponentiation may be challenging to work with:

$$log(odds)=log(\frac{p}{1-p})= \beta_0 + \beta_1 x_1 + \beta_2 x_2 $$

where *log* is a natural logarithm transformation. This relationship is particularly important as it allows for conversion of probabilities into odds and vice versa. 

The underlying coefficients of the logistic regression can be interpretted using *Odds Ratios* or *OR*. Odds ratios essential express a marginal unit comparison. Since $odds = e^{z} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}$, then we can express an odds ratio as a marginal 1 unit increase in $x_1$ comparing $odds(x+1)$ over $odd(x+0)$:

$$OR = \frac{e^{\beta_0 + \beta_1 (x_1+1) + \beta_2 x_2}}{e^{\beta_0 + \beta_1 (x_1+0) + \beta_2 x_2}} = e^{\beta_1}$$

After some arithmetic, it turns out the OR is simply equal to $e^{\beta_1}$, which can be interpreted as a multiplicative effect or a percentage effect ($100 \times (1-e^{\beta_1})\%$). More simply, this means that one can obtain the ballpark effect of a regression coefficient by exponentiating it. For example, if a logistic regression were trained to relate wages and citizenship to whether people have health care insurance, the result may look as follows:

$$y(\text{no coverage}) = 0.468 - 0.048 \times wage + 0.372 \times \text{non-citizen} $$

The coefficients provide little information other than the fact that a positive cofficient is associated with an increase in the chance of $y$. However, by exponentiating the cofficients, the odds of coverage are as follows for each variable:

- $OR_{wage} = e^{-0.048} = 0.953$ translates to -4.68% lower chance of not having health coverage for every $1000 increase in wages. Otherwise stated, more that one earns, better the chance of having health coverage.
- $OR_{non-citizen} = e^{0.372} = 0.451$ translates to 45% higher chance of not having health coverage among non-citizens.



### Tips of the trade

Training a logistic regression is easy to do using a statistical analysis software, but is also easily used incorrectly. Tuning is a matter of finding the optimal combination of variables and generally starts from a hypothesis of how variables are related to a target. As tuning process unfolds, it invariably becomes a series of trial and error tests. While this forces data scientists to start from a theory and hypothesis, it also highlights some of the core challenges of modeling.

Suppose an analyst is tasked with understanding what drives health care insurance coverage in the United States and settles on a four-variable specification that lends itself to a compelling narrative. On the surface, the results sounds promising. However, it may seem less so when considering that the four variables were selected from among 100 possible variables -- the results reflect only one of 3.9 million possible four variable specifications. *How does one know if the specification is the best?* 

The possibilities of tuning an accurate model are be seemingly endless, but are far more manageable when put in the context of the policy objective.  As logistic regression is one of the few truly interpretable statistical learning algorithms, the goals can be broadly divided between *interpretability* (focus on $\hat{\beta}$) and *prediction* (focus on $\hat{y}$).



```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/glm_sw.csv")
  colnames(usescases) <- c("Useful Properties", "Challenges")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "The good and ugly of logistic regression.", 
                 justify = "left")

```

__Interpretability__. As we have seen previously, the coefficients lend themselves to articulating how $X \rightarrow Y$ as the ultimate goal is causal inference. Causality is a challenging thing to prove, thus the process of producing reliable and trustworthy parameter estimates takes into account the basic assumptions of logistic regression. Failing to account for the basic assumptions of the technique will lead to misleading results. There are many assumptions, but we focus on a few key considerations:

In virtually all introductory texts that cover logistic regression, *multicollinearity* is flagged and rightfully so. As a refresher, the condition is one in which two or more input variables are not only correlated with the target variables, but amongst each other as well. The consequence is that coefficients will behave oddly. The magnitude of the coefficients may seem unreasonably large. At times, the direction of the relationship is not what one may expect. The answer lies within what the coefficients represent: they are the average effect of $x$ on $y$, partially isolated holding all else constant. Thus, if two or more variables have identical or very similar information, the algorithm is not able to precisely distill each variable's effects when coefficients are learned during maximum likelihood estimation process. This explains the odd behavior in coefficients and makes coefficients invalid for interpretation. Interestingly, the predictions $\hat{y}$ will still be usable. The best option is to conduct variable selection in advance to minimize double counting of signal. Note that while multicollinearity may interfere with the coefficients, the prediction $\hat{y}$ is still valid.

In information-rich environments, the *ill-posed problem* becomes more common: there may be more variables than observations. Like ordinary least squares, logistic regression cannot be solved when $k > n$. In the social sciences, subject matter intuition is the guiding force in choosing a parsimonious specification. More recently, regularization methods such as Least Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression have be relied upon to efficiently surface correlated variables. These methods are not without their problems, however. LASSO, for example, can force coefficient of noisy variables to zero, but coefficients do not have standard errors -- thus for causal inference is a challenge.

For more in-depth treatment on the subject, consider reviewing [Need recommendations here].

__Prediction__. Unlike interpretation, prediction is purely focused on accuracy. Automated techniques are often  employed to conduct variable selection, but there is one core modeling consideration that matters significantly for logistic regression: *class imbalance*. The idea of class imbalance is simple -- when the classes in target $y$ are not represented in relatively even proportions.^[ref needed] We can imagine that many  problems are imbalanced:

- fire-related fatalities (only 3,400 fire deaths in 2017^[https://www.usfa.fema.gov/data/statistics/] vs. 2.7 million total deaths in the US in 2016^[https://www.cdc.gov/nchs/data/hus/2017/019.pdf])
- homelessness (approximately 0.17-percent of the US population^[https://www.bbc.com/news/world-us-canada-42248999])
- health care coverage (8.7% of US population is uninsured in 2017^[https://www.census.gov/data/tables/time-series/demo/health-insurance/historical-series/hic.html])

The smaller, less represented class is referred to as the *minority class*. While a logistic regression can be estimated, the size of the predicted values $\hat{y}$ will typically hover around average probability that an event will occur. Thus, if a model is trained to predict homelessness, a representative sample of the US will yield $\hat{y}=0.17$ as the intercept in the linear model will have shrunken.^[Ref from Gary King from Harvard?] This, in turn, makes evaluating accuracy more challenging to evaluate as the classification threshold is no longer $c = 0.5$. There is not one solution to addressing the class imbalance problem, but rather many possible avenue. Here are a few common strategies:

- *AUC*: If the goal is to rank observations (e.g. prioritization of risky buildings), consider using the AUC derived from the ROC curve as it summarizes the overall accuracy across multiple thresholds. 
- *Adjusted cutoff*: If the goal is to definitively flag observations for closer inspection, then it is worth considering to adjust the classification threshold $c$ to a value that optimizes desired accuracy. For example, setting the threshold to the average probability can better balance the TPR and FPR. In these cases, it also becomes necessary to assign value to how much more or less a false positive is worth compared with one additional true positive prediction.
- *Over-sampling/Under-sampling*: If the goal is to produce a balanced prediction, there is a whole host of methods for adjusting the composition of the sample. The simplest is over-sampling in which the minority class observations are replicated until it is of equal proportion as the majority class. The inverse is also done: under-sampling the majority class so that it is of equal proportion to the minority. In either case, the threshold $c$ can remain at 0.5, but there is a higher chance of overfitting.^[ref required] Alternatively, Synthetic Minority Over-Sampling Technique (SMOTE) produces new observations that look like the minority class that add fresh signal to the sample while minimizing overfitting.^[https://jair.org/index.php/jair/article/view/10302] 



### DIY: Expanding Health Care Coverage

__Background__. Universal healthcare has become a basic human right in many countries. In the United States, this is not currently a guarantee, shrouded in heated political debate and controvery whether its a matter of human rights or a matter in which an individual may choose his or her fate. Regardless of the politics, there is data on healthcare coverage.

According to the American Community Survey [ACS](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_pums_csv_2009&prodType=document), an annual survey of approximately 3.5% of the US population as conducted by the US Census Bureau, over 22.4% of residents of the U.S. state of Georgia were without healthcare coverage in 2009. That is a fairly sizable proportion of the population -- for every ten people, between two to three did not have coverage. If you read the news in 2010, a new law to provide [affordable healthcare](http://www.nytimes.com/2010/03/24/health/policy/24health.html?mcubz=1) came into effect to help the uninsured. 

Imagine that you are hypothetically tasked with getting the word out and drive recruitment in the state of Georgia. There is a hiccup, however. While commercial registries exist with people's demographic and personal contact information, most statistics on coverage are based on surveys, thus we do not precisely know _who_ does not have insurance. A brute force approach could be to reach out to everyone under the sun though we can easily infer a wasted effort as 776 of every 1000 people are already covered. *How do we get to the 224 people who are not already insured?* For marketers, this is a classic targeting problem.

Data needs to enable the prediction and classification of a population into two classes: covered and not covered. By correctly classifying people as covered and not covered, decision makers and outreach staff can mobilize targeted outreach. From a data science perspective, the real objective is to be able to identify and replicate re-occurring patterns in the training data, then generalize the insights onto a sample or population that is not contained in the sample.

Given the label $y(Coverage)$, we can use logistic regression to not only infer what is associated with coverage, but also train a model to prioritize who should be contacted about receiving coverage: 

$$y(Coverage) = f(\text{Sex, Age, Education, Marital Status, Race, Citizenship})$$

__Data__. Based on the Census American Community Survey (ACS), we will illustrate how to construct a logistic regression. First, we will load the ACS sample, which has been balanced -- meaning that both covered and non-covered survey respondents are represented in equal proportions in the sample. 


```{r, message=FALSE, warning=FALSE}
# Load ACS health care data 
  load("data/acs_health.Rda")
```

Next, we will simulate a train and test to determine how well our models can do when applied in the wild. 

```{r, message=FALSE, warning=FALSE}
# Randomly assign
  set.seed(100)
  rand <- runif(nrow(health)) > 0.5
  
# Create train test sets
  train <- health[rand == T, ]
  test <- health[rand == F, ]
```


__Function__. Training a logistic regression can be easily performed using the `glm()` function, which is a flexible algorithm class known as Generalized Linear Models. Using this one method, multiple types of linear models can be estimated including ordinary least squares for continuous outcomes, logistic regression for binary outcomes and Poisson regression for count outcomes. 

At a minimum, three parameters are required:

`glm(formula, data, family)`

where:

- `formula` is a formula object. This can take on a number of forms such as a symbolic description (e.g. $y = \beta_0 + \beta_1 x_1+ \beta_2 x_2 + \epsilon$ is represented as `y ~ x1 + x2`). 
- `data` is a data frame containing the target and inputs.
- `family` indicates the probability distribution used in the model. Distributions typically used for GLMs are _binomial_ (binary outcomes), _poisson_ (count outcomes), _gaussian_ (continuous outcomes - same as OLS), among others.

The `family` refers to the type of probability distributions that underlie each estimation method. In the case of logistic regression, the probability family is *binomial*. 

__Focusing on interpretation__. In the social sciences and in public policy, the focus of regression modeling is typically placed on identifying an effect or an associated relationship that describes the process being studied. To tease out the effects, logistic regression analyses tend to center around what is colloqially known as a *build up*. Blocks of conceptually-related variables are tested in order to infer their important and effect on the target outcome. As we are focused on insurance coverage, we specify four models: 

- _Personal_: $coverage = f(log(age) + race + sex)$
- _Economic_: $coverage = f(wage + employment)$
- _Social_: $coverage = f(citizenship + marital + schooling)$
- _Combined_:  $coverage = f(log(age) + race + sex + wage + employment + citizenship + marital  + schooling)$

It is worth highlighting that the inclusion of certain characteristics, such as race and sex, can improve the accuracy of a model. However, when used for prediction -- particularly for targeting -- one needs to consider the ethics and fariness of including these variables in application.

```{r, message = FALSE, warning = FALSE}

# Estimated GLM models
  glm_pers <- glm("no.coverage ~ log(age) + race + sex", 
                      data = train, family = binomial)

  glm_econ <- glm("no.coverage ~ wage + esr", 
                  data = train, family = binomial)
  
  glm_soc <- glm("no.coverage ~ cit + mar + schl", 
                 data = train, family = binomial)
  
  glm_all <- glm("no.coverage ~ log(age) + wage + schl + esr + cit + mar + race + sex", 
                 data = train, family = binomial)

```

The regression outputs chronicles every pertinent aspect of each model, such as the direction of the relationships (e.g. positive or negative weights), their statistical significance (e.g. p-value or t-statistics), and the relative fit of the model (e.g. the lowest Akaike Information Criterion or AIC provides _relative_ model fit comparison). For example, an analyst may point out that education has an effect on coverage by interpreting the coefficient point estimates. In the combined model, education attainment coefficients are are estimated relative to people who hold a graduate degree, thus indicating that people who :
 
-  did not finish high school have a _6.58-times_ higher chance of not having health coverage ($ e^{w = 1.884} = 6.58$)
-  hold a high school degree have a _4.91-times_ higher chance of not having health coverage ($ e^{w = 1.592} = 4.91$)
-  hold a college degree are relatively better off than the previous two groups with a _1.79-times_ higher chance of not having health coverage ($ e^{w = 0.584} = 1.79$)
 
All coefficients are statistically significant. While it is valid to evaluate models on this basis, it is necessary to remember that this is not the same as evaluating a model for predictive use cases as  predictive accuracy is not assessed on the basis of coefficients. 
 
```{r, echo = FALSE, results = 'asis', warning=FALSE, message=FALSE}
 library(stargazer)
 stargazer(glm_pers, glm_econ, glm_soc, glm_all,
           column.labels = c("Personal", "Economic", "Social", "All"),
           dep.var.caption = "Coefficient table of four alternative logistic regression specifications.")
```
 
__Focus on prediction__. Prediction, in contrast, emphasizes absolute generalizable accuracy and requires model validation techniques like k-cross validation. We can rely on the `boot` library to generate cross-validated accuracy estimates through the `cv.glm()` function:

`cv.glm(data, glmfit, cost, K)`

where:

- `data` is a data frame or matrix.
- `fit` is a glm model object.
- `cost` specifies the cost function for cross validation. 
- `K` is the number of cross validation partitions.

Note that the cost function needs to take two vectors. The first is the observed responses and the second is the predicted responses. For example, the cost function could be the overall accuracy rate (*ACC*):

$$ ACC = \frac{FP+FN}{TP+FP+TN+FN}$$

as translated into R code below, assuming that the threshold is the sample probability of being uncovered.

```{r, message = FALSE, warning = FALSE}
costAccuracy <- function(y, yhat){
  #
  # Returns vector of accuracy, TPR, and FPR using sample 
  # 
  # Args:
  #   y = vector of actual target
  #   yhat = vector of predicted probabilities
  #

  #Components
  ymean <- mean(y)
  p <- sum(y == 1)
  n <- sum(y == 0)
  tp <- sum((y == 1) & (yhat >= ymean))
  fp <- sum((y == 0) & (yhat >= ymean))
  tn <- sum((y == 0) & (yhat < ymean))
  tot <- length(y)
  
  #scores
  acc <- (tp + tn) / tot
  tpr <- tp / p
  fpr <- fp / n
  
  return(c(acc, tpr, fpr))
}
```
 
Using `cv.glm()`, we test each of the specifications using 10-folds cross validation to obtain an accurate estimate on model performance

```{r,  message = FALSE, warning = FALSE}
# Load boot library
  library(boot)

# Estimate k-folds 
  pers <- cv.glm(data = train, glmfit = glm_pers, cost = costAccuracy, K = 10)
  econ <- cv.glm(data = train, glmfit = glm_econ, cost = costAccuracy, K = 10)
  soc <- cv.glm(data = train, glmfit = glm_soc, cost = costAccuracy, K = 10)
  all <- cv.glm(data = train, glmfit = glm_all, cost = costAccuracy, K = 10)
```

The accuracy results are stored in the `cv.glm()` objects, specifically in the first three elements in the `delta` component (e.g. `pers$delta`). The order of the model accuracies are not different than what can be inferred from the AICs in the regression table: The fully loaded specification yields the best overall predictive accuracy.

```{r,  message = FALSE, warning = FALSE, echo = FALSE}

temp <- rbind(data.frame(mod = "Personal", t(pers$delta[1:3])),
              data.frame(mod = "Economic", t(econ$delta[1:3])),
              data.frame(mod = "Social", t(soc$delta[1:3])),
              data.frame(mod = "Fully Loaded (All)", t(all$delta[1:3])))

knitr::kable(temp, col.names = c("Model", "Accuracy", "TPR", "FPR"),
             caption = "Cross-validated prediction accuracies.")
```


__Scoring__. In order to obtain the score the test set, we use `predict()`:

`predict(object, newdata, response)`

where:

- `object` is a GLM model object.
- `newdata` is a data frame. This can be the training data set or the test set with the same format and features as the training set.
- `response` indicates the type of value to be returned, whether it is the untransformed "link" or the probability "response".

We will now apply `predict()` to score the responses for each `train` and `test` samples.

```{r, warning=FALSE, message=FALSE}
  pred.glm.train <- predict(glm_all, train, type = "response")
  pred.glm.test <- predict(glm_all, test, type = "response")
```

Lastly, to calculate the prediction accuracy, we will once again rely on the combination of `ggplot2` and `plotROC` libraries for the AUC. Interestingly, the test set AUC is greater than that of the train set. This occurs occassionally and is often times due to the luck of the draw. ^^NEED A BIT MORE TO CLOSE OUT THE SECTION

```{r, message = FALSE, warning = FALSE}
#load libraries
  library(plotROC)
  library(ggplot2)

#Set up ROC inputs
  input.glm <- rbind(data.frame(model = "train", 
                                d = train$no.coverage, 
                                m = pred.glm.train), 
                    data.frame(model = "test", 
                               d = test$no.coverage,  
                               m = pred.glm.test))
  
#Graph all three ROCs
  roc.glm <- ggplot(input.glm, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc() 

#AUC
  calc_auc(roc.glm)[,2:3]
```



### Practice Exercises

1. Can logistic regression be applied to the downed tree problem from the KNN section? Apply the method to the downed trees data. How do the accuracies compare and why?


## LASSO regression

[expand]

## Decision Tree Learning

Mobile technologies have lowered the bar to using lightweight sensors that measure the physical world and have opened new applications of data in daily life. From a smart phone's accelerometer, itâ€™s possible to track distinct patterns in oneâ€™s activity based on the fluctuations in acceleration ($\frac{m}{s^{2}}$). In fact, many of these technologies have become commonly available, enabling physical fitness activity monitoring to characterizing transportation quality. Below is a set of exercise measurements from an smartphone accelerometer that lasted approximately 6.5 minutes and graphed at a frequency of 5 hertz (five readings per second). 

Can you visually identify distinct patterns? What makes those patterns distinct? 


```{r, echo=F, warning=FALSE, message=FALSE, fig.height = 2, fig.cap = "Accelerometer data collected from an iPhone."}

  temp <- read.csv("data/exercise.csv")
  temp$accel <- sqrt(temp$user_acc_x.G.^2 + temp$user_acc_y.G.^2 + temp$user_acc_z.G.^2)
  temp <- temp[!is.na(temp$accel) & temp$accel!="",]
  #print(nrow(temp))

  temp$timestamp.unix. <- round(temp$timestamp.unix./0.2,0)*0.2
  temp <- temp[,c("timestamp.unix.","accel")]
  temp_val <- aggregate(x = temp$accel, by=list(temp$timestamp.unix.), FUN=mean, na.rm=TRUE)
  colnames(temp_val) <- c("time","accel")

  temp_val <- temp_val[(nrow(temp_val)-2300):nrow(temp_val),]
  temp_val$time <- temp_val$time - min(temp_val$time) 
  
  temp_val <- temp_val[order(temp_val$time),]
  temp_val[,1] <- as.POSIXct(as.numeric(as.character(temp_val[,1])),origin="2016-01-01")
  
  plot(temp_val$time, temp_val$accel, xlab = "Acceleration", ylab = "Time (Seconds)", type = "l", col = "navy", cex = 0.4)
  
```

It becomes immediately apparent that the methods covered thus far are not suitable for the task at hand. If we manually extract samples from these periods, we can quantify the patterns in terms of their central tendencies. Idle periods have near zero acceleration, walking periods have acceleration around 0.2 with tight dispersion, running periods hover around 0.6 +/- 0.2, and descending stairs vary widely. Using simply the level of acceleration may not accurate as at least two types of motion have overlapping distributions.


```{r, echo=F, warning=FALSE, message=FALSE, fig.height = 3, fig.cap = "Acceleration by type of movement."}

library(gridExtra)

#Cut parts out
  idle <- temp_val[250:350,]
  walk <- temp_val[45:175,]
  run <- temp_val[1135:1200,]
  stairs <- temp_val[1400:1700,]

#Plot Graphs
  p = ggplot(idle,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line(colour = "navy") + ylim(0, 1) + ggtitle(paste("Idle: mu = ", round(mean(idle[,2]),2),", +/- = ",round(sd(idle[,2])*1.96,2),", max = ",round(max(idle[,2]),2))) + theme(plot.title = element_text(size = 8)) + theme_minimal()
  
  p1 = ggplot(walk,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line(colour = "navy") + ylim(0, 1) + ggtitle(paste("Walk: mu = ", round(mean(walk[,2]),2),", +/- = ",round(sd(walk[,2])*1.96,2),", max = ",round(max(walk[,2]),2)))+ theme(plot.title = element_text(size = 8))+ theme_minimal()
  
  p2 = ggplot(run,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line(colour = "navy") + ylim(0, 1) + ggtitle(paste("Run: mu = ", round(mean(run[,2]),2),", +/- = ",round(sd(run[,2])*1.96,2),", max = ",round(max(run[,2]),2)))+ theme(plot.title = element_text(size = 8))+ theme_minimal()
  
  p3 = ggplot(stairs,aes(x=time, y= accel  ))  + geom_point(size=1) + geom_line(colour = "navy") + ylim(0, 1) + ggtitle(paste("Descend Stairs: mu = ", round(mean(stairs[,2]),2),", +/- = ",round(sd(stairs[,2])*1.96,2),", max = ",round(max(stairs[,2]),2)))+ theme(plot.title = element_text(size = 8))+ theme_minimal()
  
  grid.arrange(p,p1,p2,p3, ncol=2)
```

Decision tree learning can help bring clarity. Trees are designed to look at inputs and partition the sample into smaller more homogeneous cells with respect to the target. This recursive partitioning allows a tree to resemble an inverted tree: moving away from the base of the tree, the tree trunk splits into two or more large branches, which then in turn split into even smaller branches, eventually reaching even small twigs with leaves. 

Decision trees use recursive partitioning to learn patterns, doing so using central concepts of _information theory_. There are a number of decision tree algorithms that were invented largely in the 1980s and 1990s, including the ID3 algorithm, C4.5 algorithm, and Classification And Regression Trees for Machine Learning (CART). All these algorithms follow the same framework that includes the following elements: (1) nodes and edges, (2) attribute tests, and (3) termination criteria.


### Under the hood

__Anatomy of a decision tree__. The tree is comprised of nodes and edges. Nodes (circles) contain records. Edges (lines) show dependency between nodes and is the result of an *attribute test* -- or a process that finds the optimal criterion to subset records into more homogeneous groups of the target variable.  The node at the top of the tree is known as the *root* and represents the full population. Each time a node is split, the result is two nodes -- each of which is referred to as a *child node*. A node without any child nodes is known as a *leaf*.  The goal is to grow a tree from the root node into as many smaller child nodes that contain more of one class than another.

![Anatomy of a Decision Tree](data/tree_anatomy.png)

__Attribute tests__.  Decision trees split nodes based on finding thresholds along the input variables. There can be seemingly infinite number of potential variable-threshold combinations -- which is best? Drawing from *information theory*, we can apply an *entropy* calculation to help sift through all the candidate splits to find the one that provides the most information. 

We can understand this process through a hypothetical example of a health care service in a small town. Suppose there is a list that contains both users and non-users of the service. For each person, the inventory captures whether a given person is employed, has income over $20k, and lives on the west side or east side of town. Each of the features are plotted in the pie chart below. 50% of town residents use the health service, but which of the features is best at separating users from non-users?

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap = "A visual comparison of low separability (1 and 3) and high separability (2).", fig.height = 2.5}
  
  #Libraries
  library(ggplot2)
  library(gridExtra)
  
  #Get fabricated data
  customers <- read.csv("data/entropy_example.csv")
  customers$id <- 1:nrow(customers)
  
  #Target
  customers$temp <- ""
  tab <- as.data.frame(table(customers$temp, customers$user))
  colnames(tab) <- c("customer", "user", "count")
  
  pie_0 <- ggplot(tab, aes(customer, user)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + ylab("Service User") +  ggtitle("(1)") + xlab("")+
             scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  #employ
  customers$temp <- ""
  tab <- as.data.frame(table(customers$temp, customers$employ))
  colnames(tab) <- c("customer", "employ", "count")
  
  pie_1 <- ggplot(tab, aes(customer, employ)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + ylab("Employed") +  ggtitle("(2)") + xlab("")+
             scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  #Income
  customers$temp <- ""
  tab <- as.data.frame(table(customers$temp, customers$income))
  colnames(tab) <- c("customer", "income", "count")
  
  pie_2 <- ggplot(tab, aes(customer, income)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + ylab("Income") +  ggtitle("(3)") + xlab("")+
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  #Area
  customers$temp <- ""
  tab <- as.data.frame(table(customers$temp, customers$area))
  colnames(tab) <- c("customer", "area", "count")
  
  pie_3 <- ggplot(tab, aes(customer, area)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + ylab("Area") +  ggtitle("(4)") + xlab("")+
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  grid.arrange(pie_0,pie_1, pie_2, pie_3,  ncol = 4)
```


To answer that question, we can visualize cross-tabulations between the target and covariates. The objective is to identify the matrix where the circles are the largest along any diagonal -- this would indicate that given usership, a feature is able to serve as a criterion that separates users from non-users. Of the three graphs below, graph #2 is able to separate a relatively large proportion of users from non-users. For a relatively low-dimensional dataset (fewer attributes), a visual analysis is accomplishable. However, at scale, undertaking this process manually may be onerous and prone to error.



```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap = "A visual comparison of low separability (1 and 3) and high separability (2)."}
  
  #employ
  tab <- as.data.frame(table(customers$user, customers$employ))
  colnames(tab) <- c("customer", "employ", "count")
  
  pie_1 <- ggplot(tab, aes(customer, employ)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Customer?") + ylab("Employed?") +  ggtitle("(1)") + 
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  
  #Income
  tab <- as.data.frame(table(customers$user, customers$income))
  colnames(tab) <- c("customer", "income", "count")
  
  pie_2 <- ggplot(tab, aes(customer, income)) + 
              geom_point(aes(size = count), colour = "navy")  +  ggtitle("(2)") +  
              theme_bw() + xlab("Customer?") + ylab("Income") +   
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1) 
  
  #Revenue
  tab <- as.data.frame(table(customers$user, customers$area))
  colnames(tab) <- c("customer", "area", "count")
  
  pie_3 <- ggplot(tab, aes(customer, area)) + 
              geom_point(aes(size = count), colour = "navy")  +  ggtitle("(3)") +  
              theme_bw() + xlab("Customer?") + ylab("Side of town?") +   
              scale_size_continuous(range=c(3,15)) + 
              theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)
  
  grid.arrange(pie_1, pie_2, pie_3,  ncol = 3)
```

Enter attribute tests within decision tree learning framework make sifting through information quite convenient.  The process starts from the root node where the algorithm examines each input feature to find the one that maximizes separability at that node: 

```
  Let Sample = S, Target = Y, Input Features = X
      For each X:
          Calculate the attribute test statistic comparing X and Y
          Store statistic
      Compare and identify Xi that yields the greatest separability
      Split S using input feature that maximizes separability
      Iterate process on child node
```

Upon finding the optimal feature for a given node, the decision tree algorithm splits the node into two child nodes based on the optimal feature, then moves onto the next node (often times a child node) and runs the same process to find the next split. There are a number of attribute tests, of which we will cover two: *Information Gain* and  *Gini Impurity*.

__Information gain__ is a form of *Entropy*, which is a measure of purity of information. Based on these distinct states of activity, entropy is defined as:

$$\text{Entropy} = \sum{-p_{i} log_2(p_{i})}$$

where $i$ is an index of states, $p$ is the proportion of observations that are in state $i$, and $log_2(p_i)$ is the Base 2 logarithm of the proportion for state $i$. Information Gain (IG) is variant of entropy, which is the entropy of the root node *less* the average entropies of the child nodes.

$$\text{IG} = \text{Entropy}_\text{root} - \text{Avg Child Entropy}$$
How does this work in practice? Starting from the root node, we need to calculate the root entropy, where the classes are based on the classes of the target `usership`.

$\qquad \text{Entropy}_\text{usership} = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad  = (-\frac{6}{12} log_2(\frac{6}{12})) + (-\frac{6}{12} log_2(\frac{6}{12}))$

$\qquad \qquad \qquad \qquad \qquad  = 1.0$

Then, the attribute test is applied to the root node by calculating the weighted entropy for each proposed child node. Using the `income` feature, the calculation is as follows:

- Split the root node into two child nodes using the `income` class. This yields the following subsamples as shown in the table below:

| | < $20k | > $20k|
|--------+---------+----------|
|No | 0 | 6 |
|Yes | 5 | 1 |
|Total | 5 | 7 |

- For each child node (the columns in the table), calculate entropy:

$\qquad \text{Entropy}_\text{income < 20k } = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad  = -\frac{5}{5} log_2(\frac{5}{5}) = 0$


$\qquad \text{Entropy}_\text{income > 20k } = (-p_{user} log_2(p_{\text{user}})) - (-p_{\text{non-user}} log_2(p_{\text{non-user}}))$

$\qquad \qquad \qquad \qquad \qquad = -\frac{6}{7} log_2(\frac{6}{7}) + -\frac{1}{7} log_2(\frac{1}{7}) = 0.5916728$

- Calculate the weighted average entropy of children:

$\qquad \text{Entropy}_\text{income split} = \frac{5}{12}(0) +  \frac{7}{12}(0.5916728) = 0.3451425$

- Then calculate the information gain:

$\qquad \text{IG}_\text{income} = \text{Entropy}_\text{root} -  \text{Entropy}_\text{income split}$

$\qquad \qquad \qquad \qquad \qquad = 1 - 0.3451425 = 0.6548575$

- We then can perform the same calculation on all other features (e.g. employment, part of town) and compare results. The goal is to *maximize* the IG statistic at each decision point. In this case, we see that income is the best attribute to use for splitting. This split is easily interpretable: "The majority of users of health services can be predicted to earn less than $20,000."

| Measure | IG |
|---------+------|
|Employment| 0.00 | 
|Income | 0.6548575 |
|Area of Town|0.027119 |


__Gini Impurity__ is closely related to the entropy with a slight modification:

$$\text{Gini Impurity} = \sum{p_{i}(1-p_{i})} = 1 - \sum{p_{i}^2}$$
Using Gini Impurity as an attribute test is also similar to Information Gain: 

$$\text{Gini Gain} = \text{Gini}_\text{root} - \text{Weighted Gini}_\text{child}$$

### (3) Stopping Criteria + Tree Pruning
Both Gini Gain and Information Gain attribute tests can be recursively applied until there are no longer input features available to split the data. This is also known as a "fully grown tree" or an "unpruned tree". While the terminal leafs may yield a high degree of accuracy in training, trees may grow to epic and complex proportions that have leaf sizes are often times too small to provide accurate and generalizable results. While fully grown trees are considered to have low bias, their out-of-sample performance may be high in variance. There [theoretically] exists some optimal balancing point where trees are complex enough to capture statistical patterns, but are not too complex to yield misleading results.

Fortunately, the methodologists who invented decision tree learning have designed two approaches to balance accuracy and generalizability: stopping criteria and pruning.

Recall that a leaf is defined as a node with no child nodes. Otherwise stated, a leaf is a terminal node in which no additional attribute testing is conducted -- it's placed out of commission. Stopping criteria are employed to determine if a node should be labeled a leaf during the growing process, thereby stopping tree growth at a given node. These criteria are specified before growing the tree and take on a number of different forms including: 

- A node has fewer records than a pre-specific threshold;
- The purity or information gain falls below a pre-specified level or is equal to zero;
- The tree is grown to n-number of levels (e.g. Number of levels of child nodes relative to the root exceeds a certain threshold).

While stopping criteria are useful, the results in some studies indicate their performance may be sub-optimal. The alternative approach involves growing a tree to its fullest, then comparing the prediction performance given tree complexity (e.g. number of nodes in the tree) using cross-validation.  In the example graph below, model accuracy degrades beyond a certain number of nodes. Thus, optimal number of nodes is defined as when cross-validation samples (e.g. train/test, k-folds) reaches a minimum across samples. Upon finding the optimal number of nodes, the tree is _pruned_ to only that number of nodes. 

```{r, echo=FALSE, warning = FALSE, message = FALSE, fig.height = 2}
set.seed(1020)
n = 15
tree.error <- data.frame( trees = 1:n,
                          v1 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v2 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v3 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v4 = 50*(1+cos((1:n)/3)) + runif(n)*10,
                          v5 = 50*(1+cos((1:n)/3)) + runif(n)*10)

ggplot(tree.error) + 
  geom_line(aes(x = trees, y = v1), colour = "orange") + 
  geom_line(aes(x = trees, y = v2), colour = "navy") +
  xlab("Number of nodes") + ylab("Error") + 
  geom_vline(xintercept = 10, colour = "red") + 
  geom_line(aes(x = trees, y = v3), colour = "green") + 
  geom_line(aes(x = trees, y = v4), colour = "purple") +
  geom_line(aes(x = trees, y = v5), colour = "lightblue")  + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())
 
```


### Tips of the trade

Like any technique, decision trees have strengths and weaknesses:

| Strengths  | Weakness|
|--------------------------------------+--------------------------------|
| - Rules (e.g. all the criteria that form the path from root to leaf) can be directly interpreted. | - Data sets with large number of features will have overly complex trees that, if left unpruned, may be too voluminous to interpret. |
| - Method is well-suited to capture interactions and non-linearities in data. | - Trees tend to overfitted at the terminal leafs when samples are too small. |
| - Technique can accept both continuous and continuous variables without prior transformation. | |
| - Feature selection is conducted automatically | |


### DIY: Predicting 
To put decision trees into practice, we will use the same `train` and `test` data frames introduced in the GLM section. There are a number of R implementations of decision trees, the most popular of which is the `rpart` library:
```{r, message = FALSE, warning=FALSE}
  library(rpart)
```

The main function within the library comes with flexible capabilities to grow decision trees: 

`rpart(formula, method, data, cp, minbucket, minsplit)`

where:

- `formula` is a formula object. This can take on a number of forms such as a symbolic description (e.g. $y = f(x_1, x_2, ...)$ is represented as "`y ~ x1 + x2`""). 
- `method` indicates the type of tree, which are commonly either a classification tree "class" or regression tree "anova". Split criteria can also be custom written.
- `data` is the data set in data frame format.
- `cp` is a numeric indicates the complexity of the tree. $cp = 1$ is a tree without branches, whereas $cp = 0$ is the fully grown, unpruned tree. If $cp$ is not specified, `rpart()` defaults to a value of 0.01.
- `minbucket` is a stopping criteria that specifies the minimum number of observations in any terminal leaf.
- `minsplit`  is a stopping criteria that specifies the number of observation in a node to qualify for an attribute test.

As a first pass, we'll run `rpart()` with the default assumptions. Note that in `rpart()` automatically conducts k-folds cross-validation for each level of tree growth. If one were to use `summary()` or `str()` to check the structure of the output object named `fit`, the inner workings would likely be found to be quite exhaustive and rather complex. Fortunately, the `printcp()` method can be used to obtain a summary of the overall model accuracy for tree at different stages of growth. Key features of the `printcp()` output include:

- A listing of the variables actually used in construction (note that `cit`)
- In the table, `CP` indicates the tree complexity, `nsplit` is the number of splits, `rel error` is the prediction error in the training data, `xerror` is the cross-validation error, and `xstd` is the standard error.

```{r, message = FALSE, echo = FALSE, warning=FALSE}
#cp = 0
  fit <- rpart(no.coverage ~ age + wage + cit + mar + schl + esr , 
               method = "class", data = train)

#Lowest xerror
  best.error <- as.vector(min(fit$cptable[,4]))
  best.splits <- as.vector(fit$cptable[,2][which(fit$cptable[,4]==min(fit$cptable[,4]))])
  best.sd <- as.vector(fit$cptable[,5][which(fit$cptable[,4]==min(fit$cptable[,4]))])

  opt.error <- best.error + best.sd
  opt.select <- as.vector(fit$cptable[,1][which(fit$cptable[,4] <= opt.error)])[1]
  opt.xerror <- as.vector(fit$cptable[,4][which(fit$cptable[,4] <= opt.error)])[1]
  opt.select.split <- as.vector(fit$cptable[,2][which(fit$cptable[,4] <= opt.error)])[1]
  
```

To choose the best tree, a _rule of thumb_ is to first find the tree with the lowest cross-validation `xerror`, then find the tree that has the lowest number of splits that is still within one standard deviation `xstd` of the best tree^[Hastie et. al (2001)]. The idea behinds this rule of thumb takes advantage of uncertainty: the true value lies somewhere within a confidence interval, thus any value within a tight confidence interval of the best value is approximately the same. In this first model, the best tree has `r paste0("nsplit = ", best.splits)` and `r paste0("xerror = ", best.error)`. By applying the rule, the upper bound of acceptable error is `r paste0("xerror = ", round(best.error,6)," + ", round(best.sd, 6), " = ", opt.error)`. As it turns out, the tree with `r paste0("nsplit = ", opt.select.split)` is within one standard deviation and is thus the best model. 


```{r, eval = FALSE}

#Fit decision tree under default assumptions
  fit <- rpart(no.coverage ~ age + wage + cit + mar + schl + esr, 
               method = "class", data = train)
  
#Tools to review outpu
  printcp(fit)
```

The model's learned rules contained in `fit` can be plotted with `plot()`, but it takes a bit of work to get the plot into a presentable format. The substitute is using the `rpart.plot` library, which auto-formats the tree and color codes nodes based on the concentration of the target variable.

```{r, fig.height = 3, fig.cap = "Decision tree using default parameters."}
#Plot
  library(rpart.plot)
  rpart.plot(fit, shadow.col="gray", nn=TRUE)
```

```{r, message = FALSE, echo = FALSE, warning=FALSE}
#cp = 0
  fit.0 <- rpart(no.coverage ~ age + wage + cit + mar + schl + esr , 
               method = "class", data = train, cp = 0)

#Lowest xerror
  best.error <- as.vector(min(fit.0$cptable[,4]))
  best.splits <- as.vector(fit.0$cptable[,2][which(fit.0$cptable[,4]==min(fit.0$cptable[,4]))])
  best.sd <- as.vector(fit.0$cptable[,5][which(fit.0$cptable[,4]==min(fit.0$cptable[,4]))])

  opt.error <- best.error + best.sd
  opt.select <- as.vector(fit.0$cptable[,1][which(fit.0$cptable[,4] <= opt.error)])[1]
  opt.xerror <- as.vector(fit.0$cptable[,4][which(fit.0$cptable[,4] <= opt.error)])[1]
  opt.select.split <- as.vector(fit.0$cptable[,2][which(fit.0$cptable[,4] <= opt.error)])[1]
  
```

While this answer is valid, it should be noted that the CP lower threshold is 0.01, which is the default value. For robustness, we should run the model once more, this time specifying $cp = 0$ to obtain the full, unpruned tree (see below). Applying the error minimization rule once more, the minimum `r paste0("xerror = ", round(best.error, 6))`, which corresponds to `r paste0("nsplit = ", best.splits)`. The maximum $xerror$ within one standard deviation is `r paste0("xerror = ", round(best.error, 6), " + ", round(best.sd, 6), " = ", round(opt.error,6) )`, which corresponds to `r paste0("nsplit = ", opt.select.split)` with `r paste0("xerror = ", round(opt.xerror, 6))` and `r paste0("cp = ", round(opt.select,6))`

```{r, eval = FALSE}
#cp = 0
  fit.0 <- rpart(no.coverage ~ age + wage + cit + mar + schl + esr , 
               method = "class", data = train, cp = 0)
  printcp(fit.0)
```

```{r, echo = FALSE}
  printcp(fit.0)
```

At this point, we'll re-run the decision tree once more with the updated $cp$ value, assign the decision tree object to `fit.opt`, and plot the resulting decision tree. Notice how the rendered tree is significantly more complex relative to the default and interpretation may be more challenging with a plethora of criteria. 

```{r, fig.height = 4, fig.cap = "Decision tree for optimized complexity."}
  fit.opt <- rpart(no.coverage ~ age + wage + cit + mar + schl + esr, 
               method = "class", data = train, cp = opt.select)
  rpart.plot(fit.opt, shadow.col="gray", nn=TRUE)
```

In lieu of a thorough review of the learned rules, we may rely on a measure of variable importance, that is defined as follows:

$$\text{Variable Importance}_k = \sum{\text{Goodness of Fit}_\text{split, k} + (\text{Goodness of Fit}_\text{split,k}\times \text{Adj. Agreement}_\text{split})}$$
Where *Variable Importance* for variable $k$ is the sum of *Goodness of Fit* (e.g. Gini Gain or Information Gain) at a given split involving variable k. In otherwords, a variable's importance is the sum of all the contributions variable $k$ makes towards predicting the target. Below, we can see that the measure can be extracted from the `fit.opt` object. As it turns out, `age` is the most important factor.

```{r}
#Extract variable importance list from fit object
  fit.opt$variable.importance
```


Using the `plotROC` package once again, we calculate the AUC score for each model to assess predictive performance on both the training and test set. One particularly striking difference is the switch in position of the $optimal$ and $cp = 0$ curves: $cp = 0$ is higher in the training set, but are at the approximate safe height in test. This indicates that $cp = 0$ notably overfits, likely to the extra low bias of unpruned leafs.

```{r, fig.height = 3, message = FALSE, warning = FALSE, fig.cap = "ROC curves for train and test sets."}
#plotROC
  library(plotROC)
  library(gridExtra)

#Predict values for train set
  pred.opt.train <- predict(fit.opt, train, type='prob')[,2]
  pred.0.train <- predict(fit.0, train, type='prob')[,2]
  pred.default.train <- predict(fit, train, type='prob')[,2]

#Predict values for test set
  pred.opt.test <- predict(fit.opt, test, type='prob')[,2]
  pred.0.test <- predict(fit.0, test, type='prob')[,2]
  pred.default.test <- predict(fit, test, type='prob')[,2]
  
#Set up ROC inputs
  input.test <- rbind(data.frame(model = "optimal", d = test$no.coverage, m = pred.opt.test), 
                  data.frame(model = "CP = 0", d = test$no.coverage,  m = pred.0.test),
                  data.frame(model = "default", d = test$no.coverage,  m = pred.default.test))
  input.train <- rbind(data.frame(model = "optimal", d = train$no.coverage,  m = pred.opt.train), 
                  data.frame(model = "CP = 0", d = train$no.coverage,  m = pred.0.train),
                  data.frame(model = "default", d =  train$no.coverage,  m = pred.default.train))
  
  
#Graph all three ROCs
  roc.test <- ggplot(input.test, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  + ggtitle("Test")
  roc.train <- ggplot(input.train, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("Train")
  
#Plot
  grid.arrange(roc.train, roc.test, ncol = 2)
  
```

Lastly, we can extract the AUC statistics using `calc_auc()`. As multiple AUCs were calculated, we will need to extract the labels for the AUCs from the `input` file in order to produce a a 'prettified' table using `xtable`. The resulting table below presents the results of the three models that were trained. For all models, we should expect that the training AUC will be greater than the test AUC. This is generally true, but occassionally the test AUC may be greater and is largely a matter of how the data was sampled.

Starting from the top of the table:

- *Full grown*. The unpruned tree is the most complex model, which means the model has a higher chance of overfitting. This is characterized by an artificially inflated training AUC and a large drop in test AUC. As seen, the AUC drops from 0.88 to 0.826 in the test sample. The unreliable results of an unpruned tree are likely due to the algorithm's sensitivity to irregular noise at leafs. 
- *Optimal*. The optimal tree achieves a consistent $AUC = 0.83$ with minimal loss of accuracy as an appropriate level of complexity was precisely tuned.
- *Default*. An underfit model will have consistently low performance in both training and testing.  As we can see, these patterns are played out in the table below containing AUCs for each the default decision tree, the optimal model complexity and the fully grown tree.

As the result of tuning towards an optimal model, we can see that the decision tree yields a marked improvement over the kNN model's $AUC = 0.44$. For a social science problem, this is considered to be a decent result.


```{r, results = 'asis', message = FALSE, warning = FALSE}
#Assemble a well-formatted table
  tab <- data.frame(model = unique(input.test$model), 
                    train = round(calc_auc(roc.train)$AUC,3), 
                    test = round(calc_auc(roc.test)$AUC,3))

```
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.cap = "Comparison of decision tree models"}
  knitr::kable(tab, booktab = FALSE)
```



### Random Forests   


```{r, message=FALSE, warning=FALSE}
# Load ACS health care data
  library(digIt)
  health <- digIt("acs_health")
  
# Convert characters into discrete factors
  factor_vars <- c("coverage", "mar", "cit", "esr", "schl")
  for(var in factor_vars){
    health[,var] <- as.factor(health[,var])
  }
  
# Randomly assign
  set.seed(100)
  rand <- runif(nrow(health)) > 0.5
  
# Create train test sets
  train <- health[rand == T, ]
  test <- health[rand == F, ]
  
```


In much of modern data references, we see more uncertainty being characterized. When a hurricane approaches the US Eastern Seaboard, forecasters often map the "cone of uncertainty" that provides the possible range of motion of a storm based on the results of many forecasted simulations. In presidential elections, often times the most polling results are ones that ensemble or average the results of many other similarly conducted polls. The reliance on predictions from a group of models with the same aim may very well improve prediction quality. In statistical learning, average the results of multiple models is known as *ensemble learning* or *ensembling* for short.

Single models may imposes biases on data and may be well-suited in specific situations. Ensemble methods combine the results of many models to obtain more stable results.  For example, the curve in graph #1 can be approximated using a decision tree algorithm. The result of a single tree only loosely fits the curve in a jagged fashion (#2). That one tree may impose biases on the data, perhaps through how the tree is pruned or the assumption that the jagged approximation is appropriate, which may then translate into greater variance in predictions. One could imagine that the structure of that one tree may have happened by chance, and under different situations, the fit could be better. 

Bootstrapping can help. Recall from elementary statistics that bootstrapping is defined as any statistical process that involves sampling records with replacement. By bootstrapping a sample, we treat a sample like a population, we can expose and characterize the qualities of an estimator under various scenarios already available in the data, which in turn produces an empirical probability distribution for predictions using the estimator. We can bootstrap the decision tree by (1) sampling the data with replacement up to the full size of the sample, then (2) run the decision tree. The result of repeating the process 50 times is (graph #3) produces a result that appears to be more organic and more accurate. This process of _bootstrapping_ and _aggregating_ the results is referred to as _bagging_.

```{r, message=FALSE, warning = FALSE, fig.height = 2.5, echo = FALSE, fig.cap = "Comparison of results of applying a single model to fit a curve versus an ensemble of models."}

library(rpart)
library(gridExtra)
library(ggplot2)

set.seed(100)
x <- 1:100
y <- 5 + sin(x/20) + 2*cos(x/10)
df <- data.frame(x, y)

fit <- rpart(y ~ x, data = df)
df$yhat <- predict(fit, df)

base <- ggplot(df) + geom_line(aes(x = x, y = y))  + 
  ggtitle("(1) Actual" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

single_tree<- ggplot(df) + geom_line(aes(x = x, y = y)) +
  geom_line(aes(x = x, y = yhat), colour = "orange") + 
  ggtitle("(2) Single Tree" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

for(k in 1:50){
  temp <- df[sample(df$x, 100, replace=T),1:2]
  fit <- rpart(y ~ x, data = temp)
  yhat <- predict(fit, df)
  df <- cbind(df, yhat)
}

colnames(df)[4:ncol(df)] <- paste0("yhat",1:50)

many <- ggplot(df) + geom_line(aes(x = x, y = y)) + geom_line(aes(x = x, y = yhat), colour = "orange") +
  geom_line(aes(x = x, y = yhat2), colour = "red") + geom_line(aes(x = x, y = yhat3), colour = "orange") +
  geom_line(aes(x = x, y = yhat4), colour = "red") + geom_line(aes(x = x, y = yhat5), colour = "orange") +
  geom_line(aes(x = x, y = yhat5), colour = "red") + geom_line(aes(x = x, y = yhat6), colour = "orange") +
  geom_line(aes(x = x, y = yhat7), colour = "red") + geom_line(aes(x = x, y = yhat8), colour = "orange") +
  geom_line(aes(x = x, y = yhat9), colour = "red") + geom_line(aes(x = x, y = yhat10), colour = "orange") +
  geom_line(aes(x = x, y = yhat11), colour = "red") + geom_line(aes(x = x, y = yhat12), colour = "orange") + 
  geom_line(aes(x = x, y = yhat13), colour = "red") + geom_line(aes(x = x, y = yhat14), colour = "orange") +
  geom_line(aes(x = x, y = yhat15), colour = "red") + geom_line(aes(x = x, y = yhat16), colour = "orange") +
  geom_line(aes(x = x, y = yhat17), colour = "red") + geom_line(aes(x = x, y = yhat18), colour = "orange") +
  geom_line(aes(x = x, y = yhat19), colour = "red") + geom_line(aes(x = x, y = yhat20), colour = "orange")  + 
  geom_line(aes(x = x, y = yhat21), colour = "red") + geom_line(aes(x = x, y = yhat22), colour = "orange") + 
  geom_line(aes(x = x, y = yhat23), colour = "red") + geom_line(aes(x = x, y = yhat24), colour = "orange") +
  geom_line(aes(x = x, y = yhat25), colour = "red") + geom_line(aes(x = x, y = yhat26), colour = "orange") +
  geom_line(aes(x = x, y = yhat27), colour = "red") + geom_line(aes(x = x, y = yhat28), colour = "orange") +
  geom_line(aes(x = x, y = yhat29), colour = "red") + geom_line(aes(x = x, y = yhat30), colour = "orange") +
  geom_line(aes(x = x, y = yhat31), colour = "red") + geom_line(aes(x = x, y = yhat32), colour = "orange") + 
  geom_line(aes(x = x, y = yhat33), colour = "red") + geom_line(aes(x = x, y = yhat34), colour = "orange") +
  geom_line(aes(x = x, y = yhat35), colour = "red") + geom_line(aes(x = x, y = yhat36), colour = "orange") +
  geom_line(aes(x = x, y = yhat37), colour = "red") + geom_line(aes(x = x, y = yhat38), colour = "orange") +
  geom_line(aes(x = x, y = yhat39), colour = "red") + geom_line(aes(x = x, y = yhat40), colour = "orange")  + 
  geom_line(aes(x = x, y = yhat41), colour = "red") + geom_line(aes(x = x, y = yhat42), colour = "orange") + 
  geom_line(aes(x = x, y = yhat43), colour = "red") + geom_line(aes(x = x, y = yhat44), colour = "orange") +
  geom_line(aes(x = x, y = yhat45), colour = "red") + geom_line(aes(x = x, y = yhat46), colour = "orange") +
  geom_line(aes(x = x, y = yhat47), colour = "red") + geom_line(aes(x = x, y = yhat48), colour = "orange") +
  geom_line(aes(x = x, y = yhat49), colour = "red") + geom_line(aes(x = x, y = yhat50), colour = "orange")  + 
  ggtitle("(3) 50 Models" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())


df$average <- rowMeans(df[,4:ncol(df)])

avg <- ggplot(df) + geom_line(aes(x = x, y = y)) +
  geom_line(aes(x = x, y = average), colour = "blue") + 
  ggtitle("(4) Ensemble Average" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(base, single_tree,  many, avg, ncol = 2)
```

Applying bagging to decision trees may not necessarily be enough to develop a well-balance prediction. In the social sciences and public policy, it is generally assumed that a model's specification is a choice left to the analyst; However, it may also be a source of methodological bias. 

_Random forests_ can help. The technique, as crystallized in Breiman (2001), is an extension of decision trees using a modified form of bootstrapping and ensemble methods to mitigate overfitting and bias issues. Not only are individual records bootstrapped, but input features are bootstrapped such that if $K$ variables are in the training set, then $k$ variables are randomly selected to be considered in a model such that $k < K$. Each bootstrap sample is exhaustively grown using decision tree learning and is left as an unpruned tree. The resulting predictions of hundreds of trees are ensembled. The logic is described below.

__Pseudo-code__
```
Let S = training sample, K = number of input features
  1. Randomly sample S cases with replacement from the original data.
  2. Given K features, select k features at random where k < K.
  3. With a sample of s and k features, grow the tree to its fullest complexity.
  4. Predict the outcome for all records.
  5. Out-Of-Bag (OOB). Set aside the predictions for records not in the s cases.
Repeat steps 1 through 5 for a large number of times saving the result after each tree.
Vote and average the results of the tree to obtain predictions. 
Calculate OOB error using the stored OOB predictions. 
```

The *Out-Of-Bag* (OOB) sample is a natural artifact of bootstrapping: approximately one-third of observations are naturally left un-selected, which can be used as the basis of calculating each tree's error and the overall model error. Think of it as a convenient built in test sample.

_How about interpretation?_ Unlike decision trees, it is not a simple task to deduce rules or criteria that describe the target variable. Instead, random forests use *variable importance*, which, like for a decision tree, measures the contribution of a feature to the homogeneity of a classifier. Unlike decision trees, variable importance for a Random Forest is calculated as the mean decrease in the Gini coefficient of a split relative to the Gini coefficient of the root node. Gini coefficients measures homogeneity on a scale of 0 to 1, where 0 is perfect homogeneity and 1 is perfect heterogeneity. The Gini changes are summed for each variable and normalized. 


```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Random Forests construct hundreds of trees sampling from both observations and features, then combine the trees into one prediction through voting.", fig.height = 3}

library(rpart.plot)
library(rpart)

id <- 1:100
y <- 5 + sin(id/20) + 2*cos(id/10)
df <- data.frame(id, y)

for(i in 1:100){
  x <- y + runif(length(id))*10
  df <- cbind(df, x)
  colnames(df)[i+2] <- paste0("X",i)
}


par(mfrow = c(1,5))
for(k in 1:3){
  temp <- df[sample(1:nrow(df), replace=T), 2:ncol(df)]
  sampled.vars <- sample(colnames(temp)[2:ncol(temp)], 3)
  fit <- rpart(y ~ ., data = df[, sampled.vars], cp = 0)
  rpart.plot(fit, shadow.col="gray", nn=TRUE, 
             main = paste0("Tree ", k, "\n uses ", paste(sampled.vars, collapse = ", "),""), cex.main = 1.1)
}

plot(c(0), pch = 19, col = "white", xaxt = "n", yaxt = "n", frame.plot=FALSE, xlab = "", ylab="")
text(1, 0, ". . .", cex = 5)
for(k in 500){
  temp <- df[sample(1:nrow(df), replace=T), 2:ncol(df)]
  sampled.vars <- sample(colnames(temp)[2:ncol(temp)], 3)
  fit <- rpart(y ~ ., data = df[, sampled.vars], cp = 0)
  rpart.plot(fit, shadow.col="gray", nn=TRUE, 
             main = paste0("Tree ", k, "\n uses ", paste(sampled.vars, collapse = ", ","")), cex.main = 1.1)
}
```


#### Tuning
Whereas methods like regression have a closed form solution, Random Forest require tuning as optimal models need to be searched for under different conditions. The principal tuning parameters include: Number of features and number of trees.

- _Number of input features_. As $k$ number of parameters need to be selected in each sampling round, the value of $k$ needs to minimize the error on the OOB predictions. 
- _Number of trees_ influences the stability the Variable Importance metric that is commonly used to infer variable influence in decision tree learning. More trees help to stabilize the Variable Importance estimate. To determine the number of trees, keep adding trees to a sample until the OOB error for a randomly select set of trees is approximately equal to that of the ensemble.

####Random Forests in Practice

Like decision trees, much of Random Forests rely on easy to use methods made available through the `randomForest` library. There are a couple of ways to run the algorithm, including:

`randomForest(formula, data, method, mtry, ntree)`

where: 
- `formula` is an object containing the specification to be estimated. Note that 
- `data` is a data frame.
- `mtry` is the number of variables to be randomly sampled per iteration. Default is $\sqrt{k}$ for classification trees.
- `ntree` is the number of trees. Default is 500.

Using the same formula as the `rpart()` function, we can train a naive Random Forest and check the OOB error. Approximately 75.6% of observations in the OOB sample were correctly classified using 2 randomly selected variables in each of the 500 trees.

```{r, warning=FALSE, message = FALSE}
#Load randomForest library
  library(randomForest)

#Only complete obs
  train <- na.omit(train)

#Run Random Forest
  spec <- as.formula("no.coverage ~  age + wage + cit + mar + schl + esr")
  fit.rf <- randomForest(spec, data = train, mtry = 2, ntree = 500)

#Check OOB error
  fit.rf
```

```{r, echo = FALSE}
imp <- importance(fit.rf)
imp <- data.frame(var = row.names(imp), score = imp)
imp <- imp[order(-imp[,2]), ]

```
Using the `importance()` method, we can see the Mean Decrease Gini, which calculates the mean of Gini coefficients. `age` has the largest value of `r imp[1,2]`, indicating that age is the best predictor of coverage; However, the values themselves do not have any meaning outside of a comparison with other Gini measures.

```{r, warning=FALSE, message = FALSE}
importance(fit.rf)
```

By default, the `randomForests` library sets the number of trees to equal 500. By plotting the fit object, we can see how OOB error and the confidence interval converges asymptotically as more trees are added to the ensemble. Otherwise stated, more trees will help up to a certain point and the default is likely more than enough. 

```{r, warning=FALSE, message = FALSE, fig.height = 4}
  plot(fit.rf)
```

As we know that $n = 500$ trees is more than enough, we will now need to tune the tree for the number of variables. To tune the algorithm, we will use the `tuneRF()` method. The method searches for the optimal number of variables per split by incrementally adding variables. While it's a useful function, it is relatively verbose. In addition to the target and input features, a number of other parameters need to be specified:

`tuneRF(x, y, ntreeTry,  mtryStart, stepFactor, improve, trace, plot)`

where: 
- `x` is a data frame or matrix of input features.
- `ntreeTry` is the number of trees used in each iteration of tuning.
- `mtryStart` is the number of variables to start.
- `stepFactor` is the number of additional variables tested per iteration.
- `improve` is the minimum relative improvement in OOB error for the search to go on.
- `trace` is a boolean that indicates where to print the search progress. 
- `plot` is a boolean that indicates whether to plot the search results. 


Below, we conduct a search from `mtryStart = 1` with a `stepFactor = 2`. The search result indicates that _2_ variables per split are optimal.
```{r, warning=FALSE, fig.height=4, fig.cap = "Random Forest tuning result (m = number of features, OOB Error = out of sample error)."}
#Search for most optimal number of input features
  fit.tune <- tuneRF(x = train[,3:ncol(train)], y =  train[,2], ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, plot = TRUE)

#Extract best parameter
  tune.param <- fit.tune[fit.tune[, 2] == min(fit.tune[, 2]), 1]
```


Using the optimal result, we can plug back into the `randomForest()` method and re-run. However, as the default model already has the same parameters as the optimal model, we can proceed to calculating the model accuracy. Comparing the training and test models for the Random Forest algorith, we see a large drop in the AUC between train and test, indicating quite a bit of overfitting. 

```{r, fig.height = 2, message = FALSE, warning = FALSE}
#plotROC
  library(plotROC)

#Predict values for train set
  pred.rf.train <- predict(fit.rf, train, type='prob')[,2]

#Predict values for test set
  pred.rf.test <- predict(fit.rf, test, type='prob')[,2]

  
#Set up ROC inputs
  input.rf <- rbind(data.frame(model = "train", d = train$no.coverage, m = pred.rf.train), 
                  data.frame(model = "test", d = test$no.coverage,  m = pred.rf.test))
  
#Graph all three ROCs
  roc.rf <- ggplot(input.rf, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("Train")

#AUC
  calc_auc(roc.rf)
```



