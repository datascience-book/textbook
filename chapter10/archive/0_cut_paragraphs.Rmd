---
title: "Cut paragraphs"
author: "Jeff Chen"
date: "4/11/2019"
output: pdf_document
---



## Interpretability versus prediction 

When selecting models for classification tasks, data scientists often times balance interpretability and predictive accuracy. Some situations require a narrative to communicate the insights. In the social and natural sciences, classification methods like logistic regression are favored for their use in *parameter estimation* to infer relationships between variables captured in coefficients. These coefficients are directly interpretable, showing how a variable can contribute or detract from the probability of an outcome holding all else constant. For example, a building built X decades ago is Y-times more likely to catch fire than a building built this decade holding all else constant. This simple factoid can facilitate narratives that communicate insight. But there is a tradeoff. While interpretable methods may extract the gist of the relationships, they may at times miss the finer variations in the data necessary for a reliable prediction. 

Alternatively, some audiences are far less interested in the story, but just want an accurate number. In other words, predictive accuracy is prized in which case it may be worth considering a body of exciting methods that have arisen from statistics and computer science that are optimized for the task. These other methods may be more versatile, adapt to scenarios where there are more variables than observations, find interactions between variables and nonlinear patterns, and optimize for robustness. In the tech sector, for example, data science pursuits are often a matter of how well classifiers can scale to service their customers and drive sales. Understanding which variables definitively drive the predictions can be useful for teams focused on communicating insights, but not as much for the technology side of the house.

The bottom line is that one's choice of classifier depends on how much one values interpretability versus accuracy.

## Fairness and transparency 

In the area of criminal justice, algorithms such as COMPAS are actively being used to predict the chance that someone accused of a crime will recidivate within two years. While classifiers may offer a degree of efficiency when sifting through a multitude of data and are relied upon by judges in passing sentences, there are significant concerns regarding their ethical use. In 2016, ProPublica conducted research that examines the fairness of risk ratings from COMPAS, finding that while the algorithm is able to correctly predict recidivism 61% of the time, black defendents are 1.9-times more likely than white counterparts to be labeled high risk but not actually re-offend.^[https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing] In other words, if two defendents with the same criminal history were scored by COMPAS and the only difference is their race, the two defendents would be treated differently. 

It is unsurprising that *fairness* and *transparency* are emerging areas of focus in the use of classifiers. Fairness can take on many definitions, but for the purpose of this text, we define it as whether two or more subpopulations receive equal treatment. Transparency means that the provenance of the underlying information and its method of producing predictions are available for review -- not that it fits a neat normative, but that it can be traced and scrutized. 
 
