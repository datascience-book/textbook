--- 
title: "Clustering"
author: "Jeff Chen"
date: '`r Sys.Date()`'
output:
  html_document: default
  pdf_document: default

---


# Intro

## Everything is related to everything else

In a 1970 article, Geographer Waldo Tobler wrote "Everything is related to everything else, but near things are more related than distant things."^[Tobler W., (1970) "A computer movie simulating urban growth in the Detroit region". Economic Geography, 46(Supplement): 234-240.]  Tobler was getting at the idea that people and phenomena tend to _cluster_ together -- things that are clustered have short distances from one another relative to other things. Perhaps the easiest way to see the effect of spatial dependence is in night time satellite imagery. Across the US' 35 largest cities, the urban landscape takes shape with lights clustered along streets and in certain parts of town, sometimes clustering in the main thoroughfares and other cases in residential areas and major roadways. We can see the clusters, but how does one measure it? 

![Night time imagery from the NOAA-NASA Suomi NPP Satellite's Visible Infrared Imaging Radiometer Suite (VIIRS)](assets/clustering/img/satellite_imagery.png)

The idea of clustering can extend beyond just time and space. In marketing, consumers are regularly grouped into clusters that represent distinct behaviors and preferences.  For example, hotel-goers of high end resorts will be more likely part of a specific affluent customer segment than those who choose to stay at a budget motel, which in turn form the basis of characterizing demand segments. In looking at markets, certain industries may be viewed as a cluster of economic activity as they rise and fall together due to their dependence on one another or their products are complements in the market. In epidemiology, outbreaks of a disease tend to be physically clustered together. In some law firms, data scientists may develop topic modeling algorithms to automatically tag and cluster hundreds of thousands of documents for improved search. *Unsupervised learning* can help. It is a branch of machine learning that deals with unlabeled data to identify statistically-occurring patterns -- let the data fall where they may. 

Building upon measures of similarity and distance, this chapter provides a short survey of types of unsupervised learning and its uses.

## Technical Foundations

The fundamental idea of clustering methods is to express a set of attributes in two or more discrete groups. A visual inspection of the probability distribution of a data series often will give clues as to what natural clusters may lie within. For example, the bi-modal and quad-modal distributions below can be easily grouped into clear groups. Visually, the goal is to find the center of mass of sub-distributions, then assign values that are closest to a proposed center.

```{r, echo = FALSE, fig.height = 3, fig.cap = "A multi-modal distribution naturally yield two clusters", message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
distro <- data.frame(x = c(rnorm(100000, 300, 10), rnorm(100000, 400, 10)), fill = 1)
distro$cluster <- as.factor(kmeans(distro$x, 2)$cluster)

a0 <- ggplot(distro, aes(x = x, fill = "red")) + ggtitle("Original Distribution") + 
      geom_density(stat= "density",adjust = 2) + xlab("")  + ylab("Bi-modal") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())

a1 <- ggplot(distro, aes(x = x, fill = cluster)) + ggtitle("Clustered") + 
      geom_density(stat= "density",adjust = 2) + xlab("")  + ylab("") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())

distro <- data.frame(x = c(rnorm(10000, 0, 20), 
                           rnorm(10000, 340, 20), 
                            rnorm(10000, 500,20), 
                           rnorm(10000, 1000, 20)), fill = 1)
set.seed(200)
distro$cluster <- as.factor(kmeans(distro$x, 4)$cluster)

b0 <- ggplot(distro, aes(x = x, fill = "red")) +
      geom_density(stat= "density",adjust = 0.01) + xlab("")  + ylab("Multi-modal") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())

b1 <- ggplot(distro, aes(x = x, fill = cluster)) + 
      geom_density(stat= "density",adjust = 0.01) + xlab("")  + ylab("") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())


grid.arrange(a0, a1, b0, b1, nrow = 2, ncol = 2)

```

The same visual process can easily guide clustering in two and three dimensions. Generally, greater distance between the masses -- or separability -- allows for less ambiguous cut offs between two groups.  



```{r, echo = FALSE, fig.cap = "Two- and three- dimensional clusters.", fig.height=3, message=FALSE, warning=FALSE}

par(mfrow = c(1,2), oma = c(0,0,0,0))
library(ggplot2)
library(gridExtra)
n <- 100
distro <- data.frame(x = c(rnorm(n, 300, 40), rnorm(n, 400, 30)), 
                     y = c(rnorm(n, 300, 40), rnorm(n, 400, 30)), 
                     fill = factor(c(rep("green",n), rep("red", n))))

plot(distro$x, distro$y, col = distro$fill, pch = 19, ylab = "", xlab = "", sub = "2D", frame.plot=FALSE, xaxt='n', yaxt='n', cex = 0.8)


library(scatterplot3d)
n <- 30
distro <- data.frame(x = c(rnorm(n, 300, 30), rnorm(n, 800, 50), rnorm(n, 700, 80)), 
                     y = c(rnorm(n, 400, 30), rnorm(n, 300, 50), rnorm(n, 700, 40)),
                     z = c(rnorm(n, 300, 20), rnorm(n, 400, 50), rnorm(n, 700, 40)),
                     fill = as.character(c(rep("slategrey",n), rep("lightblue", n), rep("darkblue",n))))
scatterplot3d(distro$x, distro$y, distro$z, color = distro$fill, pch = 19, cex.symbols = 0.5,
              scale.y=.75, type="h", lty.hplot=2, 
              xlab = "", ylab = "", zlab = "", axis = FALSE,
              sub = "3D")

```

The task of clustering becomes complicated when subdistributions overlap in space -- how to tell one from another?  Imagine attempting to find clusters in four-dimensional space let alone n-dimensional space; the visual approach is no longer an option.

```{r, echo = FALSE, fig.height = 3, fig.cap = "Case of mixed distributions"}
library(ggplot2)
library(gridExtra)
distro <- data.frame(x = c(rnorm(100000, 300, 20), rnorm(100000, 400, 30), rnorm(100000, 350, 40), rnorm(100000, 500, 30)) , fill = 1)
distro$cluster <- as.factor(kmeans(distro$x, 2)$cluster)

ggplot(distro, aes(x = x, fill = "red")) + ggtitle("") + 
      geom_density(stat= "density",adjust = 2) + xlab("")  + ylab("") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())

```

Clustering algorithms are designed to explore underlying patterns when labeled data are not available. The number of strategies used to cluster data points is as numerous as the approaches used to characterize similarity. Some methods such as _k-means_ are focused on finding a fixed number of centroids, or finding center masses of clusters. More agglomerative approaches like _hierarchical clustering_  examine pairwise distances between all points and group points together first in order to capture a hierarchy of relationships. While there are many other techniques, we focus on these two methods given their ease of use and versatility.



### K-Means

The _k-means_ clustering is a technique to identify clusters of observations by treating features as coordinates in n-dimensional space. The _k_ in k-means is specified by the analyst -- it is the number of clusters that will be returned upon running the algorithm. _k_ is not a known quantity and will need to be optimized by the analyst. 

The technique is fairly straight forward to optimize and is one that is iterative as shown in the pseudocode below:

```
  Initialize k centroids 
  Repeat following until convergence:
    Calculate distance between each record n and centroid k
    Assign points to nearest centroid 
    Update centroid coordinates as average of each feature per cluster
```

The first step involves selecting $k$-number of random centroids from the feature space and giving each centroid a label. For each observation in the data, calculate the Euclidean distance ($ d(x_1,x_2) = \sqrt[2]{\sum_{i=1}^n|z_1-z_2|^2}$) to all initial centroids, then assign each point to the closest centroid. This is known as the *assignment* step -- all points take the label of its closest centroid. It is unlikely that this initial assignment is likely suboptimal, thus the algorithm will *update* the centroid coordinates by calculating the mean value of each feature within each cluster.  Upon doing so, this assignment-update procedure is iteratively repeated until the centroid coordinates no longer change between iterations (see illustration below).


```{r, echo = FALSE, message=FALSE, warning=FALSE,fig.cap = "Illustration of k-means algorithm from initialization to convergence"}

library(raster)
path <- paste0(getwd(),"/assets/clustering/img")
plotRGB(brick(paste0(path,"/kmeans_example.jpg")))


```



Central to algorithm is goal to find some set of $k$ coordinates that minimize the within-cluster sum of squares (WSS):

$$arg min \sum_{j=1}^k\sum_{i=1}^n ||x_{i,j} - \mu_j||^2$$

where the sum of the distance $x$ of each point $i$ in cluster $j$ to its corresponding centroid of $j$. Distance is calculated in terms of all input features $x$ and the $j^{th}$ cluster centroid $\mu$.


#### Assumptions  {-}
While k-means is a simple algorithm, its performance and effectiveness is guided by a number of key assumptions at each step of computation.

- _Scale_. As k-means treats features as coordinates, each feature is assumed to have equal importance, which in turn means that results may be inadvertently biased simply by the scale and variances of underlying features. To remedy this problem, input features should be mean-centered standardized ($\frac{x_i-\mu}{\sigma}$) or otherwise transformed to reduce scaling effects. Note, however, that the influence of scaling may not always be removed. For example, a data set containing both continuous and binary features would likely perform quite poorly as Euclidean distances are not well-suited for binary. Thus, where possible, apply k-means when the formats are homogeneous, doing so using Euclidean L2-distances for continuous and binary distances for matrices of discrete features.


- _Missing Values_. K-Means do not handle missing values as each data point is essentially a coordinate. Thus, often times k-means models are usually reserved for complete data sets.

- _Stability of Clusters_. The initialization step of the algorithm chooses $k$ initial centroids at random. The initial random selection is known to lead to suboptimal and unstable clusters. The instability in the results can be observed when running the algorithm for some value of $k$ multiple times, sometimes leading to different cluster composition: holding $k$ constant between model runs, a record $i = 1$ may be in the same cluster as $i = 10, 23, 40$ in one set of results, but only with $i = 23$ in another model run. The stability of clusters may be due to a number of things, such as a suboptimal choice of $k$, a high number features that add noise to the optimization process, among others.


```{r, echo = FALSE, message=FALSE, warning=FALSE, fig.cap = "Comparison of a suboptimal result and optimal result"}

library(raster)
path <- paste0(getwd(),"/assets/clustering/img")
plotRGB(brick(paste0(path,"/kmeans_example_error.jpg")))


```


- _Choice of K_. Selecting the best value of $k$ is arguably a subjective affair: there is a lack of consensus regarding how to identify $k$.  One method known as the _Elbow method_ chooses $k$ at the inflection point where an additional cluster does not significantly reduce the variance explained or reduction of error. The simplest method of identifying the inflection point can be seen by plotting the percent WSS over all values of $k$ that were tested. This approach is deceptively simple as the inflection point might not manifest itself in some data sets. 

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3, fig.cap = "Elbow method: Choose k at the inflection point"}
library(ggplot2)
library(gridExtra)
distro <- data.frame(x = c(rnorm(100000, 300, 20), rnorm(100000, 400, 30), rnorm(100000, 350, 40), rnorm(100000, 500, 30)) , fill = 1)


a0 <- ggplot(distro, aes(x = x, fill = "red")) + ggtitle("Distribution") + 
      geom_density(stat= "density",adjust = 2) + xlab("")  + ylab("") + 
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none", axis.title.x=element_blank(),
            axis.text.x=element_blank(), axis.ticks.x=element_blank(),
            axis.text.y=element_blank(), axis.ticks.y=element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())

placeholder <- data.frame()
for(i in 1:10){
  cluster <- kmeans(distro$x, i)
  placeholder <- rbind(placeholder, data.frame(k = i, wss =   cluster$tot.withinss/cluster$totss))
}

a1 <- ggplot(placeholder, aes(x = k, y = wss)) +
  geom_vline(aes(xintercept= 3), colour = "grey") + 
  geom_line(colour = "red") + ggtitle("Within Sum of Squares by K") +
  geom_point(colour = "red") + ylab("%WSS") + 
  xlab("Value of k")+ 
  scale_x_continuous(breaks = (seq(1, 10, by = 1))) +
      theme(plot.title = element_text(size = 10, hjust = 0.5), 
            legend.position="none",panel.border = element_blank(),
          panel.background = element_blank()) 

grid.arrange(a0, a1, ncol = 2)
```

An alternative, but far more computationally intensive approach involves calculating the _silhouette_, which is compares estimates the similarity of a given observation $i$ as compared to observations within and outside the cluster. The silhouette $s(i)$ is defined as:

$$s(i) = \frac{b_i-a_i}{max(a_i,b_i)}$$

where $a_i$ is the Euclidean distance between a point $i$ to other points in the same cluster, $b_i$ is the minimum distance between $i$ and any other cluster the sample. The values of $s(i)$ fall between -1 and 1, where 1 indicates that an observation is well-matched with its cluster and -1 indicates that fewer or more clusters may be required to achieve a better match. Note that silhouettes do not scale well with very large data sets as a $n \times n$ similarity matrix (e.g. distance between all points to all points). Often times, a smaller sample should be used to enable the use of this method.

For a step-by-step walkthrough of the application of the k-means algorithm, see _How much of the ground is covered in [vegetation/buildings/economic activity]?_ in the DIY section of this chapter.

### Hierarchical clustering 

Whereas k-means initializes on random centroids, hierarchical clustering take a more computationally costly ground-up approach:

```
Calculate distance d between all points

All points are start as their own clusters (singletons)
Do until there is only one cluster:
  Find the closest pair of clusters in terms of linkage distance
  Merge into a single cluster 
  Recalculate distances from new cluster to all other clusters
Stop when all points are in one cluster

```


For a step-by-step walkthrough of the application of the hierarchical clustering algorithm, see _How do I characterize the demand for [products/services]? _ in the DIY section of this chapter.






##DIY

