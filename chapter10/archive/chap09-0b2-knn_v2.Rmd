---
output:
  html_document: default
  pdf_document: default
---
### K-Nearest Neighbors (kNN)

As hurricanes become more intense and leave a trail of destruction, city services will need to be able to more efficiently triage requests for help. Let's take the example of Hurricane Sandy and its effect on NYC. One of the main services offered by cities is the management and care of its trees. A downed tree can cause property damage, bodily harm and traffic disruptions. Due to the high wind and lush foliage during Sandy, many trees fell.

In NYC, the Department of Parks and Recreation is responsible for tree removal. When a resident makes a call to the city's services hotline 311, a work order is created and a tree removal team is dispatched. This may be a transactional process: one call for tree removal, one tree is then removed. As it takes time for crews to move and set up, a first-in/first-out queuing process can be inefficient. Imagine if 20 of 100 blocks in a neighborhood were flagged for tree removal. It would make sense to use that call data to identify other blocks that may also have downed trees.

We would expect that downed trees are more likely to occur in *pockets* and proximity is the best indicator of activity. As the city knows where residents call for tree- and non-tree-related issues, we can use the location of the calls to triangulate on likely problem areas as well as anticipate pockets of yet-to-be-reported downed trees, or at least serve that is a reasonable working theory.

For this task of predicting based on proximity, k-Nearest Neighbors (kNN) can help.

#### Under The Hood

K-nearest neighbors (KNN) is a non-parametric, instance-based algorithm that is based on a simple idea: *observations that are closer together are more likely to be similar*. The method is non-parametric as it does not directly use its inputs to determine the value of $y$. It is instance-based as each prediction is determine on a case-by-case basis using a pre-defined procedure. 

The procedure is simple. For each case $y_i$:

1. _Distance_. First, we calculate the _distance_ to all other records with known outcomes. Distance most commonly takes the form of Euclidean distance (below), which is appropriate with continuous values. For cases where the underlying data are boolean or binary, Manhattan distance may be more appropriate. In effect, the input variables $X$ serve as sets of coordinates to triagulate which points are closer to a given case. 

$$ \text{Euclidean distance} = \sqrt{\sum_{i=1}^n(x_{i} - x_{0})^{2} }$$ 



2. _Voting_. For the $k$ nearest observations, take the average of $y$ (below). Basically, for each class $j$ in $Y$, we take the average of $k$ observations that are closest based on $X$. In effect, this procedure yields a local conditional probability for each observation, which is converted into a predicted class through *majority voting*.   

$$Pr(Y = j|X) = \frac{1}{k}\sum_{i \in A} I(y^i = j)$$


3. _Tuning_. The method is sensitive to the value of $k$, requiring tuning -- or testing different values of $k$. When $k = 10$, the conditional probability for $y_i$ reflects the 10-nearest neighbors. When $k = n$, the conditional probability is the sample mean. 


The procedure described above yields the results for just one value of $k$. However, kNNs, like many other algorithms, are an iterative procedure, requiring tuning of *hyperparameters* -- or values that are starting and guiding assumptions of a model. In the case of kNNs, $k$ is a hyperparameter and we do not precisely know the best value of $k$. Often times, tuning of hyperparameters involves a *grid search*, a process whereby a range of possible hyperparameters is determined and the algorithm is tested at equal intervals from the minimum to maximum of that tuning range. 

To illustrate this, a two-dimensional dataset with a target $y$ that takes of values $0$ and $1$ has been plotted below. Graph (1) plots the points, color-coded by their labels. Graph (2), (3), and (4) show the results of a grid search along intervals of a $log_{10}$ scale, where the background is color-coded as the predicted label for the corresponding value of $k$. In addition to $k$, two measures are provided above each graph to help contextualize predictions: the True Positive Rate or $TPR$ and the True Negative Rate or $TNR$. 

The $TPR$ is defined as #$TPR = \frac{\text{Number of values that were correctly predicted}}{\text{Number of actual cases values}}$#. The $TNR$ is similarly defined as #$TNR = \frac{\text{Number negative values that were correctly predicted}}{\text{Number of actual negative values}}$#. Both are measures bound between 0 and 1, where higher values indicate a higher degree of accuracy. A high $TPR$ and low $TNR$ indicates that the algorithm is ineffective in distinguishing between positive and negative cases. The same is true with a low $TPR$ and high $TNR$. This is exactly the case in Graph (4) where all points are classified as $Y = 1$, which is empirically characterized by $TNR = 0.02$ and $TPR = 1$.


<Redo the graph below>
```{r, fig.height=4, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "Comparison of prediction accuracies for various values of k."}
#Prep diagrams

#Base data
set.seed(234)
examp <- as.data.frame(expand.grid(x1 = seq(0.01,1,0.01), x2 = seq(0.01,1,0.01)))

examp$linear <- (examp$x1 < examp$x2) * 1 
part1 <- examp$x1 < ( examp$x2 * 4 -(8+rnorm(nrow(examp),0.4,0.4))-examp$x2^2 + 2*examp$x2^3 )
part2 <- examp$x2 >  0.7 +rnorm(nrow(examp),0.3,0.1) - ( examp$x1*0.9)
part3 <- examp$x2 >  0.2 -runif(nrow(examp))*0.4+runif(nrow(examp))*0.4+ ( examp$x1*1.4)
examp$discont <- ( part2 == TRUE )*1


#ADaptive 
library(kknn)
  k1 <- kknn(discont ~ x1 + x2, 
              train = examp, 
              test = examp,
              k = 3,
              kernel = "rectangular")
  k10 <- kknn(discont ~ x1 + x2, 
             train = examp, 
             test = examp,
             k = 10,
             kernel = "rectangular")
  k100 <- kknn(discont ~ x1 + x2, 
              train = examp, 
              test = examp,
              k = 30,
              kernel = "rectangular")
  
prob.grid.gbm1 <- k1$fitted.values
prob.grid.gbm2 <- k10$fitted.values
prob.grid.gbm3 <- k100$fitted.values


# plot the boundary
par(mfrow = c(2,2), mar = c(0, 1.2, 0.9, 0))

#Example
plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
     font.main = 1,  pch = 16, cex = 0.3, main = "Example Data",
     xaxt = "n", yaxt = "n", asp = 1, bty="n")
points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.3)
points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.3)


#k = 3

est <- table(examp$discont,prob.grid.gbm1 >= 0.5)
tpr <- round(100*est[2,2]/sum(examp$discont),2)
fpr <- round(100*est[1,2]/sum(examp$discont==0),2)

plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
     font.main = 1,  pch = 16, cex = 0.3, 
     xaxt = "n", yaxt = "n", asp = 1,bty="n")
points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.3)
points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.3)
contour(x=seq(0.01, 1, 0.01), y = seq(0.01, 1, 0.01), 
        z=matrix(prob.grid.gbm1, nrow=100), levels=0.1,
        col="black", drawlabels=FALSE, lwd=1.3, add=T)
title(main=paste0("k = 3, TPR = ", tpr, ", FPR = ", fpr), line= 0, cex.main=0.9, font.main = 1)


#k = 2
est <- table(examp$discont,prob.grid.gbm2 >= 0.5)
tpr <- round(100*est[2,2]/sum(examp$discont),2)
fpr <- round(100*est[1,2]/sum(examp$discont==0),2)
plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
     font.main = 1,  pch = 16, cex = 0.4, 
     xaxt = "n", yaxt = "n", asp = 1,bty="n")
points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.4)
points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.4)
contour(x=seq(0.01, 1, 0.01), y = seq(0.01, 1, 0.01), 
        z=matrix(prob.grid.gbm2, nrow=100), levels=0.1,
        col="black", drawlabels=FALSE, lwd=1.3, add=T)
title(main=paste0("k = 10, TPR = ", tpr, ", FPR = ", fpr), line= 0, cex.main=0.9, font.main = 1)


#k = 100
est <- table(examp$discont,prob.grid.gbm3 >= 0.5)
tpr <- round(100*est[2,2]/sum(examp$discont),2)
fpr <- round(100*est[1,2]/sum(examp$discont==0),2)
plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
     font.main = 1,  pch = 16, cex = 0.4, 
     xaxt = "n", yaxt = "n", asp = 1,bty="n")
points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.4)
points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.4)
contour(x=seq(0.01, 1, 0.01), y = seq(0.01, 1, 0.01), 
        z=matrix(prob.grid.gbm3, nrow=100), levels=0.1,
        col="black", drawlabels=FALSE, lwd=1.3, add=T)
title(main= paste0("k = 100, TPR = ", tpr, ", FPR = ", fpr), line= 0, cex.main=0.9, font.main = 1)

    
```

#### Which K is the right K?

The accuracy of a KNN model is dependent on finding the right value of $k$. Thus, to optimize for accuracy, try multiple values of $k$ and compare the resulting accuracy values. It is helpful to first see that when $k = n$, kNNs are simply the sample statistic (e.g. mean or mode) for the whole dataset. Below, the True Positive Rate (TPR, blue) and True Negative Rate (TNR, green) have been plotted for values of $k$ from 1 to $n$. The objective is to ensure that there is a balance between TPR and TNR such that predictions are accurate. Where $k > 20$, the TPR is near perfect. For values of $k < 10$, TPR and TNR are more balanced, thereby yielding more reliable and accurate results.


```{r, fig.height=3, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "True Positive Rate (TPR = blue) and True Negative Rate (FPR = green) performance for varying values of k"}
  library(ggplot2)
  library(class)
  set.seed(100)
 
  calc <- data.frame()
  for(i in seq(1,50,5)){
    res <- kknn(discont ~ x1 + x2, 
                train = examp, 
                test = examp, k = i)
    est <- table(examp$discont,res$fitted.values >= 0.5)
    tpr <- est[2,2]/sum(examp$discont)
    fpr <- est[2,1]/sum(examp$discont==0)
    calc <- rbind(calc, data.frame(k = i, tpr = tpr, fpr = fpr))
  }
  
  # ggplot() + 
  #   geom_line(aes(x = k, y = tpr), data = calc, colour = "blue") + 
  #   geom_line(aes(x = k, y = tnr), data = calc, colour = "green") + 
  #   ylab("Rate") + xlab("K neighbors") 
  #   
  
  plot(calc$k, calc$tpr, type = "l", col = "blue", ylim = c(0,1), ylab = "Percent", xlab = "k")
  lines(calc$k, calc$fpr, col = "green" )
```

There are other factors that influence the selection of $k$:

- _Scale_. kNNs are strongly influenced by the scale and unit of values of $x$ as ranks are dependent on straight Euclidean distances. For example, if a dataset contained  measurements of age in years and wealth in dollars, the units will over emphasize income as the range varies from 0 to billions whereas age is on a range of 0 to 100+. To ensure equal weights, it is common to transform variables into standardized scales such as:
    - Range scaled or $$\frac{x - \min(x)}{\max(x)-\min(x)} $$ yields scaled units between 0 and 1, where 1 is the maximum value
    - Mean-centered or $$ \frac{x - \mu}{\sigma}$$ yield units that are in terms of standard deviations
    
- _Grids_. Similar to the scale issue, KNNs are particularly effective in data that are distributed on a grid -- measurements along a continuous scale at equal incremenets, but may be a poor choice when the data are mixed data formats such as integers and binary.
- _Symmetry_. It's key to remember that neighbors around each point will not likely be uniformly distributed. While kNN does not have any probabilistic assumptions, the position and distance of neighboring points may have a skewing effect. 


#### Tips

KNNs are efficient and effective under certain conditions:

- KNNs can handle target values that are either discrete or continuous, making the approach relatively flexible. However, best performance is achieved when the input features should are in the same scale (e.g. color values in a grid).
- They are best used when there are relatively few features as distances to neighbors need to be calculated for each and every record and need to be optimized by searching for the value of $k$ that optimizes for accuracy. In cases where data is randomly or uniformly distributed in fewer dimensions, a trained KNN is an effective solution to filling gaps in data, especially in spatial data. 
- KNNs are not interpretable as it is a nonparametric approach -- it does not produce results that have a causal relationship or illustrate. Furthermore, kNNs are not well-equipped to handle missing values.



#### Use Case: Anticipating the extent of damage from a storm

As it common and proper, the kNN algorithm needs to be calibrated for the best $k$ using the training set, then applied to a test set. To do this, we will use the `kknn` library. The training portion uses the `train.kknn()` function to conduct k-folds cross validation, then the scoring uses the `kknn()`. While both functions can be fairly easily written from scratch (and we encourage new users to write their own as was demonstarted in the previous chapter), we will plow forth with using the library.

To start, we will load the `kknn` library:

```{r}
#Call "class" library
  library(kknn)
```

Next, we'll load in our data set

```{r, message = FALSE, warning = FALSE, fig.height = 3}
#Load data (need persistent link)
  nyc <- read.csv("data/sandy_trees.csv")
```

 and take a look at what the geographic distribution of the data. 
 
```{r}
#Plot
  plot(x = nyc$xcoord, y = nyc$ycoord, 
       xlab = "x coordinate", ylab = "y coordinate",
       cex = 0.3,  pch = 15, asp=1)
```

In New York City, only the two largest boroughs, Brooklyn and Queens, share the same land mass. The rest are separated by rivers. For a more consistent data set, we subset our data using the `boro` variable to focus.

```{r, message = FALSE, warning = FALSE}

#Subset
  nyc <- subset(nyc, boro %in% c("QN", "BK"))

#Standardize input variables
  nyc$xcoord <- scale(nyc$xcoord)
  nyc$ycoord <- scale(nyc$ycoord)
  
#Plot
  plot(x = nyc$xcoord, y = nyc$ycoord, 
       xlab = "x coordinate", ylab = "y coordinate",
       cex = 0.3, pch = 15, asp=1)
```


```{r}

#Set up data
  train <- subset(nyc, !is.na(tree.sandy), 
                  select = c("ycoord", "xcoord", "tree.sandy"))
  test <- subset(nyc, 
                 select = c("ycoord", "xcoord", "tree.next7"))
  
  
```

In order to find the optimal value of $k$, we will execute the `train.kknn()` function, which accepts the following arguments:

`train.kknn(formula, data, kmax, kernel, distance, kcv)`

- `formula` is a formula object (e.g. "`coverage ~ .`").
- `data` is a matrix or data frame of training data.
- `kmax` is the maximum number of neighbors to be tested
- `kernel` is a string vector indicating the type of distance weighting (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations, "gaussian" imposes a normal distribution on distance, "inv" is inverse distance).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).
- `kcv` is the number of partitions to be used for cross validation.

The flexibility of `train.kknn()` allows for test exhaustively and find the best parameters. Below, we conduct 10-folds cross validation up to $k = 100$ for three kernel (rectangular, biweight and inverse) assuming L1-distances. While the command is simple, it runs the kNN algorithm for 2000 times (20 cross-validation models for each k - kernel combination).


```{r, message = FALSE, warning = FALSE, results = 'hide', echo = TRUE}

#Set seed to ensure cross validation is replicable
  set.seed(100)

#Run with 20-folds cross validation
  fit.cv <- train.kknn(tree.sandy ~ ycoord + xcoord , 
                      data = train, 
                      kcv = 20, 
                      distance = 1, kmax = 100, 
                      kernel = c("rectangular", "inv"))

```


The resulting model object contains the cross-validation error log in the `MISCLASS` attribute, which has been plotted below, as well as `best.parameters` that indicates that $k = $ `r fit.cv$best.parameters$k` using an inverse distance kernel yields the lowest error.


```{r, message=FALSE, warning = FALSE, fig.cap = "20-fold cross validated errors for k = 1 to k = 100"}

#Plot Cross Validation
   plot(fit.cv)
   
#Retrieve best parameters
   best <- fit.cv$best.parameters
```

With the KNN algorithm tuned, we can now use the `kknn()` function to score the test set. The function syntax is as follows:

`kknn(formula, train, test, k, kernel, distance)`

- `formula` is a formula object (e.g. "`coverage ~ .`").
- `train` is a matrix or data frame of training data.
- `test` is a matrix or data frame of test data.
- `k` is the number of neighbors.
- `kernel` is the type of weighting of distance (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).

Notice that in the following code block, we train the kNN and apply it to the test sample all in one step.

```{r, message=FALSE, warning = FALSE}

#Apply tune KNN parameters
   fit <- kknn(tree.sandy ~ ycoord + xcoord, 
               train = train, 
               test = test,
               k = best$k,
               kernel = "inv")

#Produce 
    test$prob <- fit$fitted.values
    test$tree.next7[is.na(test$tree.next7)] <-0
```


With all the right pieces computed, we can examine how closely the predictions based on tree downing patterns on the day of Hurricane Sandy compare with where trees were reported to have fallen over the 10 days that followed. In the first panel, we see that a cloud of points capture the gist of the downed tree pattern. About 25.7% of all cells in Brooklyn and Queens made at least one call to 311, of which 78% made a request to deal with a tree issue. 

```{r, message=FALSE, warning = FALSE, fig.cap = "Graphical comparison of actual and predicted areas with reported downed trees. Red indicates at least one tree was reported in a given 0.359 square-mile area", fig.height = 4, fig.width = 8}

  
 par(mfrow = c(1,3))

  plot(train[,2:1], main = "(1) Calls during storm",
       col = rgb(train$tree.sandy , 0, 1- train$tree.sandy, 1), 
       cex = 0.4, pch = 16, asp = 1)
  
  plot(test[,2:1], main =  "(2) Predicted probabilities",
       col = rgb(test$prob, 0, 1 - test$prob, 1), 
       cex = 0.4, pch = 16, asp = 1)
  
  plot(test[,2:1], main =  "(3) Actual next 10 days",
       col = rgb(test$tree.next7, 0, 1 - test$tree.next7, 1), 
       cex = 0.4, pch = 16, asp = 1)
    
```

Using the extracted probabilities, we now can calculate the accuracy using the True Positive Rate (TPR) using a probability cutoff of 0.5. Typically, one would expect a $2 \times 2$ matrix given a binary label where the accuracy rate can be calculated based on the diagonals. In this case, prediction accuracy was `r `, indicating that the model performs reasonably well.

The test model accuracy can also be calculated by taking the Area Under the Curve (AUC) of the Receiving-Operating Characteristic. The ROC calculates the TPR and FPR at many thresholds, that produces a curve that indicates the general robustness of a model. The AUC is literally the area under that curve, which is a measure between 0.5 and 1 where the former indicates no predictive power and 1.0 indicates a perfect model. 

In order to visualize the ROC, we will rely on the `plotROC` library, which is an extension of `ggplot2`. We will create a new data frame `input` that is comprised of the labels for the test set `ytest` and the predicted probabilities `test.prob`. 

```{r, warning=FALSE, message=FALSE,}
#Load libraries
  library(ggplot2)
  library(plotROC)

#Set up test data frame
  input <- data.frame(ytest = test$tree.next7, 
                      prob = test$prob)
```

We then will first create a ggplot object named `base` that will contain the labels (`d = `) and probabilities (`m = `), then create the ROC plot using  `geom_roc()` and `style_roc()`. A ROC curve for a well-performing model should sit well-above the the 45 degree diagonal line, which is the reference for an AUC of 0.5 (the minimum expected for a positive predictor). However, as the curve is below the 45 degree line, we may have a seriously deficient model. 

```{r, message = FALSE, warning=FALSE, fig.height = 3, fig.cap = "ROC curve out of sample"}
#Base object
  roc <- ggplot(input, aes(d = ytest, m = prob)) + 
         geom_roc() + style_roc()
  
#Show result
  roc
```


As estimated using `calc_auc()`, the out-of-sample AUC is `r   round(calc_auc(roc)$AUC, 3)`, which is generally a decent level of accuracy for this type of problem.

```{r, message=FALSE, warning = FALSE, fig.height = 2}
  calc_auc(roc)$AUC
```

Despite the promising result, we are reminded by there are a few one should ask the following question: _Is there a better classifier?_ 


#### Practice Exercises {-}

The US Census Bureau's American Community Survey provides an in-depth view of life in America. One of the many features that are captured in the survey is healthcare coverage. Apply the above methods to predict healthcare coverage in the US State of Georgia in the year 2009.

The data can be obtained here: 

```{r, echo = FALSE, message=FALSE, warning=FALSE}
  library(digIt)
  health <- digIt("acs_health")
```

1. Randomly split the sample into a 50% training and 50% test set.

2. Predict healthcare `coverage` using continuous variables such as age (`agep`) and `wage`. 

3. Calculate the performance on the test sample.
