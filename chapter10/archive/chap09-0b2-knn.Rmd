---
output:
  pdf_document: default
  html_document: default
---
# K-Nearest Neighbors (kNN)

As hurricanes become more intense and leave a trail of destruction, emergency services will need to be able to accommodate

## Formulation

Continuous values can also be handed using non-parametric means. K-nearest neighbors (KNN) is a  pattern recognition algorithm that is based on a simple idea: observations that are more similar will likely also be located in the same neighborhood. Given a class label $y$ associated with input features $x$, a given record $i$ in a dataset can be related to all other records using Euclidean distances in terms of $x$: 

$$ \text{distance} = \sqrt{\sum(x_{ij} - x_{0j})^{2} }$$ 

where $j$ is an index of features in $x$ and $i$ is an index of records (observations). For each $i$, a neighborhood of taking the $k$ records with the shortest distance to that point $i$. From that neighborhood, the value of $y$ can be approximated. Given a discrete target variables, $y_i$ is determined using a procedure called *majority voting* where the most prevalent value in the neighborhood around $i$ is assigned. For example, the ten closests points relative to a given point $i$ are provided:

```{r, echo=FALSE}
#Vector of top 10 
  neighbors <- c("a","a","a","c","c","d","a","c","a","e")
```

Choosing a value of $k = 4$ would mean that the subsample is made up of three _a's_ and one _b_. As _a_ makes up the majority, we can approximate $y_i$ as _a_, assuming points that are closer together are more related. For continuous variables, the mean of neighboring records is used to approximate $y_i$.

How does one implement this exactly? To show this process, we will write some pseudocode. It's an informal language to articulate and plan the steps of an algorithm or program, principally using words and text as opposed to formulae. There are different styles of pseudocode, but the general rules are simple: indentation is used to denote a dependency (e.g. control structures). For all techniques, we will provide pseudocode, starting with kNN:

__Pseudocode__
```
kNN( k, set, y, x){
  Pre-Process (optional):
    > Transform or standardize all input features
    
  Loop through each `item` in `set`{
    > Calculate vector of distances in terms of x from `item` to all other items in `set` 
    > Rank distance in ascending order 
    
    if target `y` is continuous:
      > Calculate mean of `y` for items ranked 1 through k
    else if target is discrete:
      > Calculate share of each discrete level for items ranked 1 through k
      > Use majority voting to derive expected value 
  }
}
```

The procedure described above yields the results for just one value of $k$. However, kNNs, like many other algorithms, are an iterative procedure, requiring tuning of *hyperparameters* -- or values that are starting and guiding assumptions of a model. In the case of kNNs, $k$ is a hyperparameter and we do not precisely know the best value of $k$. Often times, tuning of hyperparameters involves a *grid search*, a process whereby a range of possible hyperparameters is determined and the algorithm is tested at equal intervals from the minimum to maximum of that tuning range. 

To illustrate this, a two-dimensional dataset with a target $y$ that takes of values $0$ and $1$ has been plotted below. Graph (1) plots the points, color-coded by their labels. Graph (2), (3), and (4) show the results of a grid search along intervals of a $log_{10}$ scale, where the background is color-coded as the predicted label for the corresponding value of $k$. In addition to $k$, two measures are provided above each graph to help contextualize predictions: the True Positive Rate or $TPR$ and the True Negative Rate or $TNR$. 

The $TPR$ is defined as #$TPR = \frac{\text{Number of values that were correctly predicted}}{\text{Number of actual cases values}}$#. The $TNR$ is similarly defined as #$TNR = \frac{\text{Number negative values that were correctly predicted}}{\text{Number of actual negative values}}$#. Both are measures bound between 0 and 1, where higher values indicate a higher degree of accuracy. A high $TPR$ and low $TNR$ indicates that the algorithm is ineffective in distinguishing between positive and negative cases. The same is true with a low $TPR$ and high $TNR$. This is exactly the case in Graph (4) where all points are classified as $Y = 1$, which is empirically characterized by $TNR = 0.02$ and $TPR = 1$.


```{r, fig.height=4, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "Comparison of prediction accuracies for various values of k."}

  library(class)
  set.seed(100)
  n <- 150
  df <- data.frame(y = c(rep(1, n/3),rep(0, n/3),rep(1, n/3)),
                 x1 = c(rnorm(n/3, -0.7, 1), rnorm(n/3, 1, 0.7), rnorm(n/3, 0.5, 1)),
                 x2 = c(rnorm(n/3, 0, 1.5), rnorm(n/3, 0, 1), rnorm(n/3, -3.5, 0.8)),
                 sample = round(runif(n)))

  test <- expand.grid(x1 = seq(-3, 3, 0.1), x2 = seq(-5,3, 0.05))
  g0 <- ggplot() +
          geom_point(aes(x = x1, y = x2, colour = factor(y)), data = df, alpha = 1, size = 3, shape = 19) +
      xlim(-3, 3) + ylim(-5, 3)  + ggtitle("(1) Two classes")  + theme(plot.title = element_text(size = 11))
  
  for(i in c(1, 10, 100)){
    res <- knn(df[,2:3], test, df$y, k = i, prob=TRUE)
    test$y <- as.character(res)
    test$prob <- as.numeric(attr(res,"prob"))
   
    res2 <- knn(df[,2:3], df[,2:3], df$y, k = i)
    est <- table(df$y,res2)
    tpr <- est[2,2]/sum(df$y)
    tnr <- est[1,1]/sum(df$y==0)
    
    g <- ggplot() +
          geom_point(aes(x = x1, y = x2, colour = factor(y)), data = test, alpha = 0.15, size = 0.7, shape = 15) +
          geom_point(aes(x = x1, y = x2, colour = factor(y)), data = df, alpha = 1, size = 2, shape = 19) +
          xlim(-3, 3) + ylim(-5, 3) + ggtitle(paste0("(",log10(i)+1,")","k = ", i, "; TPR = ", tpr, "; TNR = ", tnr)) +theme(plot.title = element_text(size = 11))
    assign(paste0("g",i), g)
    test <- test[,c(-3,-4)]
    
  }
  
  library(gridExtra)
  grid.arrange(g0, g1, g10, g100)
    
```

### Which K is the right K?
The accuracy of a KNN model is principally dependent on finding the right value of $k$ directly determines what enters the calculation used to predict the target variable. Thus, to optimize for accuracy, try multiple values of $k$ and compare the resulting accuracy values. It is helpful to first see that when $k = n$, kNNs are simply the sample statistic (e.g. mean or mode) for the whole dataset. Below, the True Positive Rate (TPR, blue) and True Negative Rate (TNR, green) have been plotted for values of $k$ from 1 to $n$. The objective is to ensure that there is a balance between TPR and TNR such that predictions are accurate. Where $k > 20$, the TPR is near perfect. For values of $k < 10$, TPR and TNR are more balanced, thereby yielding more reliable and accurate results.


```{r, fig.height=3, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "True Positive Rate (TPR = blue) and True Negative Rate (TNR = green) performance for varying values of k"}
  library(ggplot2)
  library(class)
  set.seed(100)

  train <- df[df$sample==0, 1:3] 
  test <- df[df$sample==1, 1:3] 
 
  calc <- data.frame()
  for(i in seq(1,n,1)){
    res <- knn(train[,2:3], test[,2:3], train$y, k = i)
    est <- table(test$y,res)
    tpr <- est[2,2]/sum(test$y)
    tnr <- est[1,1]/sum(test$y==0)
    calc <- rbind(calc, data.frame(k = i, tpr = tpr, tnr = tnr))
  }
  ggplot() + 
    geom_line(aes(x = k, y = tpr), data = calc, colour = "blue") + 
    geom_line(aes(x = k, y = tnr), data = calc, colour = "green") + 
    ylab("Rate") + xlab("K neighbors") 
    
```

There are other factors that influence the selection of $k$:

- _Scale_. kNNs are strongly influenced by the scale and unit of values of $x$ as ranks are dependent on straight Euclidean distances. For example, if a dataset contained  measurements of age in years and wealth in dollars, the units will over emphasize income as the range varies from 0 to billions whereas age is on a range of 0 to 100+. To ensure equal weights, it is common to transform variables into standardized scales such as:
    - Range scaled or $$\frac{x - \min(x)}{\max(x)-\min(x)} $$ yields scaled units between 0 and 1, where 1 is the maximum value
    - Mean-centered or $$ \frac{x - \mu}{\sigma}$$ yield units that are in terms of standard deviations
    
- _Grids_. Similar to the scale issue, KNNs are particularly effective in data that are distributed on a grid -- measurements along a continuous scale at equal incremenets, but may be a poor choice when the data are mixed data formats such as integers and binary.
- _Symmetry_. It's key to remember that neighbors around each point will not likely be uniformly distributed. While kNN does not have any probabilistic assumptions, the position and distance of neighboring points may have a skewing effect. 


### Usage

KNNs are efficient and effective under certain conditions:

- KNNs can handle target values that are either discrete or continuous, making the approach relatively flexible. However, best performance is achieved when the input features should are in the same scale (e.g. color values in a grid).
- They are best used when there are relatively few features as distances to neighbors need to be calculated for each and every record and need to be optimized by searching for the value of $k$ that optimizes for accuracy. In cases where data is randomly or uniformly distributed in fewer dimensions, a trained KNN is an effective solution to filling gaps in data, especially in spatial data. 
- KNNs are not interpretable as it is a nonparametric approach -- it does not produce results that have a causal relationship or illustrate. Furthermore, kNNs are not well-equipped to handle missing values.

For a step-by-step walkthrough of regularized methods, see _What's a good way to fill-in missing data? _ in the DIY section.


### KNN 
```{r, echo = FALSE, message=FALSE, warning=FALSE}
  library(digIt)
  health <- digIt("acs_health")
```

As covered in Chapter 8, KNNs are a weak learning algorithm that averages the outcome variable of neighboring observations in order to produce predictions. treats variables as sets of coordinates.  observation $i$ and its can be related to all other records using Euclidean distances in terms of $x$: 

$$ \text{distance} = \sqrt{\sum(x_{ij} - x_{0j})^{2} }$$ 

where $j$ is an index of features in $x$ and $i$ is an index of records (observations). For each $i$, a neighborhood of taking the $k$ records with the shortest distance to that point $i$. From that neighborhood, the value of $y$ can be approximated. Given a discrete target variables, $y_i$ is determined using a procedure called *majority voting* where the most prevalent value in the neighborhood around $i$ is assigned. 

Recall that in the case of KNNs, all variables should be in the same scale such that each input feature has equal weight. A review of the data indicates that the health data is not in the appropriate form to be used. 

#### Data preparation: Mixed variable formats

Continuous variables can be discretized by binning records into equal intervals, then converting the bins into dummy matrices For simplicity, we'll bin the age and wage varaibles in the following manner:

- `age`: 10 year intervals.
- `wage`: $20,000 intervals, topcoded at $200,000.

Upon binning, each variable needs to be set as a factor.


```{r}
#Age
  health$age.bin <- round(health$agep / 10) * 10
  health$age.bin <- factor(health$age.bin)

#Wage
  health$wage.bin <- round(health$wage / 20000) * 20000
  health$wage.bin[health$wage.bin > 200000] <- 200000
  health$wage.bin <- factor(health$wage.bin)
  
```

For all discrete features including the newly added `age` and `wage` variables, we can convert them into dummy matrices (e.g. all except one level in a discrete feature is converted into a binary variable). The former can be easily achieved by using the `model.matrix()` method, which returns a binary matrix for all levels:

```{r, eval = FALSE}
  model.matrix(~ health$variable - 1)
```

As is proper in preparation of dummy variables, if there are $k$ levels in a given discrete variable, we should only keep $k-1$ dummy variables For example, citizenship is a two level variable, thus we only need to keep one of two dummies. It's common to leave out the level with the most records, but any level will do.

```{r}
#Make copy of health data frame
  knn_data <- health[, c("id","coverage")]

#Specify variables that need to be discretized
  discrete.vars <- c("cit", "mar", "schl", "wage.bin", "age.bin", "esr")
  
#Loop through and add dummy matrices to knn_data
  for(i in discrete.vars){
    dummy_mat <- model.matrix(~ health[,i] - 1)
    knn_data <- cbind(knn_data, dummy_mat)
  }

```

Now the data can be combined. Notice that the new dataset `knn_data` has 36 features.  Note that perform these transformations are necessary given mixed variable types; however, a datasets containing continuous variables only does not require any manipulation other than scaling.

```{r}
#Dimensions
  dim(knn_data)
```

#### Sample partition
As is proper, the next step is to partition the data. For simplicity, we'll create a vector that will split the data into two halves, denoting the training set as `TRUE` and the test set as `FALSE`. We then split the data into two objects contain the input features for each train and test sets.


```{r}
#Split into simple train-test design
  set.seed(100)
  rand <- runif(nrow(knn_data)) 
  rand <- rand > 0.5

  train <- knn_data[rand == T, 2:ncol(knn_data)]
  test <- knn_data[rand == F, 2:ncol(knn_data)]

```


#### Modeling
As it common and proper, the kNN algorithm needs to be calibrated for the best $k$ using the training set, then applied to a test set. To do this, we will use the `kknn` library. The training portion uses the `train.kknn()` function to conduct k-folds cross validation, then the scoring uses the `kknn()`. While both functions can be fairly easily written from scratch (and we encourage new users to write their own as was demonstarted in the previous chapter), we will plow forth with using the library.

To start, we will load the `kknn` library:

```{r}
#Call "class" library
  library(kknn)
```


In order to find the optimal value of $k$, we will execute the `train.kknn()` function, which accepts the following arguments:

`train.kknn(formula, data, kmax, kernel, distance, kcv)`

- `formula` is a formula object (e.g. "`coverage ~ .`").
- `data` is a matrix or data frame of training data.
- `kmax` is the maximum number of neighbors to be tested
- `kernel` is a string vector indicating the type of distance weighting (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations, "gaussian" imposes a normal distribution on distance, "inv" is inverse distance).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).
- `kcv` is the number of partitions to be used for cross validation.

The flexibility of `train.kknn()` allows for test exhaustively and find the best parameters. Below, we conduct 10-folds cross validation up to $k = 200$ for three kernel (rectangular, biweight and inverse) assuming L1-distances. While the command is simple, it runs the kNN algorithm for 2000 times (10 cross-validation models for each k - kernel combination).

```{r}
  pred.train <- train.kknn(factor(coverage) ~. , data = train, 
                           kcv = 10, 
                           distance = 1,
                           kmax = 400, 
                           kernel = c("rectangular", "biweight", "inv"))
```

The resulting model object contains the cross-validation error log in the `MISCLASS` attribute, which has been plotted below, as well as `best.parameters` that indicates that $k = 335$ using an inverse distance kernel yields the lowest error.

```{r, message=FALSE, warning = FALSE, fig.cap = "10-fold cross validated errors for k = 1 to k = 500"}
#Find optimal k and kernel
  plot(pred.train$MISCLASS[,c("biweight")], 
       type = "l", col = "orange", 
       ylab = "Classification error", xlab = "k")
    lines(pred.train$MISCLASS[,c("inv")], col = "red")
    lines(pred.train$MISCLASS[,c("rectangular")], col = "blue")
```

The result suggest that a combination of $k = 335$ using inverse distance yields the best result. With the kNN algorithm tuned, we can now use the `kknn()` function to score the test set. The function syntax is as follows:

`kknn(formula, train, test, k, kernel, distance)`

- `formula` is a formula object (e.g. "`coverage ~ .`").
- `train` is a matrix or data frame of training data.
- `test` is a matrix or data frame of test data.
- `k` is the number of neighbors.
- `kernel` is the type of weighting of distance (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).


```{r}
#Score train set
  out <- kknn(factor(coverage) ~. , train = train, test = test, 
              k = 335, kernel = "inv", distance = 1)

#Extract probabilities
  test.prob <- out$prob[,2]
  
#Convert probabilities to prediction 
  pred.class <- vector(length = length(test.prob))
  pred.class[test.prob < 0.5] <- "Coverage"
  pred.class[test.prob >= 0.5] <- "No Coverage"

#Confusion matrix
  table(test$coverage, pred.class)
```

Using the extracted probabilities, we now can calculate the accuracy using the True Positive Rate (TPR) using a probability cutoff of 0.5. Typically, one would expect a $2 \times 2$ matrix given a binary label where the accuracy rate can be calculated based on the diagonals. In this case, prediction accuracy was `r tab = table(test$coverage, pred.class); paste0(round(100*(tab[1] + tab[4])/nrow(test),1), "%")`, indicating that the model performs reasonably well.

The test model accuracy can also be calculated by taking the Area Under the Curve (AUC) of the Receiving-Operating Characteristic. The ROC calculates the TPR and FPR at many thresholds, that produces a curve that indicates the general robustness of a model. The AUC is literally the area under that curve, which is a measure between 0.5 and 1 where the former indicates no predictive power and 1.0 indicates a perfect model. 

In order to visualize the ROC, we will rely on the `plotROC` library, which is an extension of `ggplot2`. We will create a new data frame `input` that is comprised of the labels for the test set `ytest` and the predicted probabilities `test.prob`. 

```{r, warning=FALSE, message=FALSE}
#Load libraries

  library(ggplot2)
  library(plotROC)

#Set up test data frame
  input <- data.frame(ytest = test$coverage, 
                      prob = test.prob)
```

We then will first create a ggplot object named `base` that will contain the labels (`d = `) and probabilities (`m = `), then create the ROC plot using  `geom_roc()` and `style_roc()`. A ROC curve for a well-performing model should sit well-above the the 45 degree diagonal line, which is the reference for an AUC of 0.5 (the minimum expected for a positive predictor). However, as the curve is below the 45 degree line, we may have a seriously deficient model. 

```{r, message = FALSE, warning=FALSE, fig.height = 3, fig.cap = "ROC for k = 410 using inverse distance"}
#Base object
  roc <- ggplot(input, aes(d = ytest, m = prob)) + 
         geom_roc() + style_roc()
  
#Show result
  roc
```


To calculate the AUC, we can use the `calc_auc()` method, from which we find that `r paste0(round(calc_auc(roc)$AUC, 1))`, which is generally a decent level of accuracy. 

```{r, message=FALSE, warning = FALSE, fig.height = 2}
  calc_auc(roc)$AUC
```

Despite the promising result, there are a few one should ask the following question: _Is there a better classifier?_ 

