--- 
header-includes: \usepackage{rotating, graphicx}
output: 
  pdf_document:
    number_sections: true
description: Introduction
documentclass: book
link-citations: yes
bibliography:
- citations_p10.bib
site: bookdown::bookdown_site
biblio-style: apalike
---

# Elementary Predictive Strategies

## Map Data as Guestimates

On an early September day in 2013, the staff of Fairbanks International Airport had an unusual encounter: TSA agents intercepted a motorist headed for the airport. This was not just any routine traffic stop — it was on the tarmac of an airfield. A motorist unwittingly drove passed marked signs and lights, venturing down a taxiway and eventually across an active runway on the way to the airport car park (@fairbanks1, @fairbanks2). Later that same month, yet another motorist made the same mistake. What is the connection? Both incidents were the result of faulty driving directions from an iPhone app. Needless to say, the airport blocked the entrance to the tarmac and filed complaints with Apple. 
Incidents like these are not uncommon. In fact, competing apps like Waze have led drivers into danger, whether its impassable snow covered roads (@snow) or into the path of a wildfire (@fire). Part of the problem lies in the maintaining an up-to-date record of the built environment. Imagine that the address of every building, condition and speed limits of every road, status of traffic, among other real-time conditions need to be available for any user to use at a moment's. It is a massive challenge that has led companies to acquire other companies to improve data quality (@appleacquisition). Sometimes, companies will even use sensitive user data to fill data gaps (@applyrebuild). Keeping up-to-date data is a problem of scale -- it is simply too hard for humans to manually curate and keep information up to date for literally all of existence. 

Machine learning can help. 

Everyday, we already rely on high frequency and high resolution data to make even the smallest of decisions, yet we do so often without realizing that much of the data are filled with approximations produced by predictive models. This is the new paradigm of artificial intelligence that has taken the tech sector by storm and its predictive power has been increasingly seen in the service of social and public good. In recent memory, Microsoft illustrated that machine learning algorithms can perform a mapping task that would normally require years for a team of humans to perform.  Computer scientists trained a pair of algorithms to identify building footprints. One set of neural network algorithms were trained on five million satellite imagery tiles to identify pixels that belong to buildings, then an additional filter converted pixels into building polygons. (@buildings) In other words, one model examines images that contain buildings and non-buildings and predicts which pixels are likely be part of a building. Since there may be some rogue pixels making the building footprint jagged, a second algorithm converts pixels into polygons that resemble realistic building footprints. The methodology achieved a precision of 99.3% and recall of 93.5% — it is quite accurate and scalable. The algorithm was then set loose on satellite imagery for the entirety of the United States, depositing its findings into the first comprehensive national building database containing  125,192,184 building footprints (@buildings2).  A complete inventory of the  state of the built environment has never been available at this level of resolution and coverage, and could one day support real-time decision making. When faced with a hurricane, emergency services could have access to a more up-to-date  inventory of all structures in the path of destruction, enabling more accurate damage estimates and more informed mitigation strategies. Local governments can make better zoning decisions so cities can evolve intelligently. And perhaps such a database could support address canvasing for the US census.

What if algorithm can provide help distill a mass of data into a simple, more useful form. The economy, for example, is comprised of an extraordinarily large number of variables. Some are related, others are not. When faced with thousands of economic variables, a human analyst may be biased towards what they are familiar or their world perspective. A machine learning algorithm, in contrast, will seek out variables that have the greatest signal for predicting a measure of interest. This is what the U.S. Bureau of Economic Analysis (BEA) has experimented with in recent memory. The agency, which is responsible for estimating the Gross Domestic Product (GDP), is faced with a constant scheduling tango with their data sources -- some data are available in time for their advance estimate of GDP and others are not. When data are not available in time, economically-motivated projections carry the estimates forward until when there is an opportunity to incorporate the "gold copy" data. The risk of projection is the chance that it does not reflect the gold copy when it is available, leading to revisions in economic estimates -- a source of anxiety for economists and financial analysts. To reduce revisions to service sector estimates, BEA has developed an experimental approach that relies on *ensembles* of machine learning algorithms, such as *Random Forests* and *Regularized Regression*, to sift through thousands of alternative economic variables and predict economic growth before data is available.  This strategy has been shown that predictions can reduce revisions to economic estimates by billions of dollars (@beaforecast), which in turn can mitigate unnecessary market responses to revisions. Similar prediction strategies have been applied by tech companies to optimize their resources. Uber, for example, trained a Long-Short Term Memory (LSTM) algorithm to forecast ridership when faced with extreme events such as sports events and holidays. By improving their short-range forecasts across their platform, they can better optimize resource allocation to meet customer demand and manage budgets more efficiently (@uberlstm).

The pursuit of prediction has driven computer scientists and statisticians to constantly develop new strategies that maximize predictive accuracy at scale. [Machine learning algorithms sit within a data science pipeline]

[Image]

In fact, each type of machine learning algorithm excels under different conditions.  Let's revisit the case of classification. Below, we plot three distinct scenarios for a two-class classification problem: simple linear boundary, non-linear, and discontinuous. A *logistic regression* is a natural fit for a linear boundary given its root in linear regression. While it is the champion of parameter estimation, logistic regression is ill-fit for more complex relationships. Non-parametric techniques are far more flexible and can mold the decision boundary to the counters of the data. This gain in accuracy comes at the cost of interpretability. For example, *K-nearest neighbors* (kNN) performs classification by looking at neighborhoods of observations. Given a training sample, each record in the test sample is predicted using the most common label for the $k$ closest known records. In essence, kNN is driven by a majority or plurality vote from neighboring records.  Alternatively, *decision tree learning* such as *Classification and Regression Trees* (CART) and *Random Forest* algorithms assorb the patterns that it has learned by encoding as binary rules that split a sample into finer, more homogeneous partitions. Each algorithm has its strengths and weaknesses rooted in how it deals with and assimilates information. The ch

\vspace{12pt} 

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Types of classification problems", fig.height = 2}
################
#Prep diagrams#
###############
  
  #Base data
  set.seed(234)
  n <- 2000
  examp <- data.frame(x1 = runif(n), x2 = runif(n))
  examp$linear <- (examp$x1 < examp$x2) * 1
  examp$nonlinear <- examp$x1 < ( examp$x2 * 4 - 8*examp$x2^2 + 2*examp$x2^3) -0.1
  part1 <- examp$x1 < ( examp$x2 * 4 - 8*examp$x2^2 + 2*examp$x2^3) +0.2
  part2 <- examp$x2 >  1.2- ( examp$x1*0.6)
  part3 <- examp$x2 >  0.2+ ( examp$x1*1.4)
  examp$discont <- (part1 == TRUE | part2 == TRUE )*1
  examp$varied <- (part1 == TRUE | part2 == TRUE | part3 == TRUE)*1
  
  #Create grid
  grid <- expand.grid(x1= seq(0.005, 1, 0.005), x2 = seq(0.005, 1, 0.005))
  

##########
## CALIBRATE MODELS#
##########
  
#Logistic
  loglin <- glm(linear ~ x1 + x2, data = examp, family = binomial())
  logvaried <- glm(varied ~ x1 + x2, data = examp, family = binomial())
  logdiscont <- glm(discont ~ x1 + x2, data = examp, family = binomial())
  prob.grid.lin1 <- predict(loglin, grid, type = "response") 
  prob.grid.lin2 <- predict(logvaried, grid, type = "response") 
  prob.grid.lin3 <- predict(logdiscont, grid, type = "response") 
  
#RPART
  library(rpart)
  dectree <- rpart(linear ~ x1 + x2, data = examp, cp = 0)
  decvaried <- rpart(varied ~ x1 + x2, data = examp, cp = 0)
  decdiscont <- rpart(discont ~ x1 + x2, data = examp, cp = 0)
  prob.grid.rpart1 <- predict(dectree, grid)
  prob.grid.rpart2 <- predict(decvaried, grid)
  prob.grid.rpart3 <- predict(decdiscont, grid)
  
#Random Forest 
  library(ranger)
  examp1 <- examp
  examp1$linear <- as.factor(examp1$linear)
  examp1$varied <- as.factor(examp1$varied)
   examp1$discont <- as.factor(examp1$discont)
   
   adatree <- ranger((linear) ~ x1 + x2, data = examp1)
   adavaried <- ranger((varied) ~ x1 + x2, data = examp1)
   adadiscont <- ranger((discont) ~ x1 + x2, data = examp1)
   prob.grid.gbm1 <- predict(adatree, grid)$predictions
   prob.grid.gbm2 <- predict(adavaried, grid)$predictions
   prob.grid.gbm3 <- predict(adadiscont, grid)$predictions
   
#KNN
   library(kknn)
   gammod1 <- kknn(linear ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   gammod2 <- kknn(varied ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   gammod3 <- kknn(discont ~ (x1) + (x2), train = examp, test = grid, distance = 1, k = 1)
   prob.grid.gam1 <- fitted(gammod1, grid)
   prob.grid.gam2 <- fitted(gammod2, grid)
   prob.grid.gam3 <- fitted(gammod3, grid)

   
  
```


```{r, echo = FALSE, fig.cap = "Linear, Non-Linear and Discontinuous Classification Problems.", fig.height = 4}

############   
## plot the boundary#
###########
   
  par(mfrow = c(3,4), mar = c(0, 1.2, 0.9, 0))
   
  #Linear example
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.lin1, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Logistic Regression", line= 0, cex.main=1.2, font.main = 1)
  title(ylab="Linear", line= -0.2, cex.lab=1.3, font=3)

  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gam1, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="kNN", line= 0, cex.main=1.2, font.main = 1)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.rpart1, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Decision Tree", line= 0, cex.main=1.2, font.main = 1)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$linear == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$linear == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gbm1, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(main="Random Forest", line= 0, cex.main=1.2, font.main = 1)
  
  
  #Non-linear
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.lin2, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(ylab="Non-Linear", line= -0.2, cex.lab=1.3, font=3)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gam2, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.rpart2, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$varied == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$varied == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gbm2, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  #Discontinuous
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.lin3, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  title(ylab="Discontinuous", line= -0.2, cex.lab=1.3, font=3)
  
  
  plot(examp[,c("x1", "x2")], col= "lightblue", ylab = "", xlab ="",
       font.main = 1,  pch = 16, cex = 0.8, 
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gam3, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
  
  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8,  
       xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.rpart3, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)

  plot(examp[,c("x1", "x2")], col= "lightblue", 
       font.main = 1, cex.main = 0.9, pch = 16, cex = 0.8, xlab = "", ylab = "",
       xaxt = "n", yaxt = "n", asp = 1,bty="n")
  points(examp[examp$discont == 1, c("x1", "x2")], col= "purple", pch = 16, cex = 0.7)
  points(examp[examp$discont == 0, c("x1", "x2")], col= "lightblue", pch = 16, cex = 0.7)
  contour(x=seq(0.005, 1, 0.005), y = seq(0.005, 1, 0.005), 
          z=matrix(prob.grid.gbm3, nrow=200), levels=0.5,
          col="black", drawlabels=FALSE, lwd=1.5, add=T)
```

\vspace{12pt} 

You will no doubt have noticed that the algorithms mentioned sound quite exotic when compared with the plain vanilla regressions in Chapters 8 and 9. There are hundreds of machine learning algorithms that are commonplace in the modern data science workbench, many of which are well-suited for both classification and regression problems. In this chapter, we explore a number of these methods, illustrating their basic properties and their potential role in helping public and social missions.

\vspace{12pt} 

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = FALSE}
  usescases <- read.csv("data/use_case_table_v2.csv")
  colnames(usescases) <- c("Method", "Common Uses",  "Example Data")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "Overview of classifiers", 
                 justify = "left")

```

## K-Nearest Neighbors (KNN)

K-nearest neighbors (KNN) is a non-parametric algorithm built on a simple idea: *observations that are closer together are likely to be similar*.  Promixity of points matters as the disposition or label of each $y_i$ is inferred from its surrounding neighbors. These neighbors are identified by treating each input variables $x_k$ as coordinate, which in turn allows distance to be calculated between all points in the data set. KNN is like the spackle of data -- it can fill holes while retaining the shape of the surrounding surface to which it is applied. It is this maliable quality that makes KNN an ideal technique for impute missing values in a realistic, organic fashion.

### Under the hood

KNNs are very much like being on a road trip in the pre-GPS and pre-smartphone era, then stopping in a town for lunch. To find out where you are, you would typically ask the first $k$-number of people you see (usually just $k=1$) what town you current are in.  When KNN produces predictions, it first locates where a new point falls within the coordinate space of the training data, then constructs a prediction from $k$-nearest neighboring points in the training data. Each prediction $\hat{y}_i$ relies on training targets and inputs $\{(x_n, y_n) \}^N_{n=1}$ that serve as landmarks to triangulate where new points should fall -- like geolocation on a map. If the base map is reliable and densely populated, then there is enough information to make a precise guess for new points. 

The algorithm is quite simple: 

_1. Set k_. Prior to starting the algorithm, start by setting a value of $k$ that determines the number of surrounding points around a record $i$ that will be used to produce a prediction for $y_i$. The true value of $k$ is not known -- this will require some systematic trial and error. 

For each record $i$ in the test sample: 

_2. Calculate distance_. The collection of input variables $X$ serve as coordinates. To identify the neighborhood around each $i$ in the test sample, calculate the distance $d_{ij}$ from $i$ to each point $j$ in the training sample.  Distance most commonly takes the form of Euclidean distance ($\sqrt{\sum_{i=1}^n(x_{i} - x_{0})^{2} }$), which is appropriate with continuous values. For cases where the underlying data are discrete or binary, Manhattan distance ($\sqrt{\sum_{i=1}^n|x_{i} - x_{0}| }$) is more appropriate. 


_3. Vote!_. The prediction $\hat{y}_i$ is calculated from a vote of the $k$ nearest points to point $i$. In classification problems, the proportion $p_c$ of points that are in each class $c$ in $Y$.  The proportion $p_c$ is converted into a predicted class through *majority voting* -- assign an observation to the class that is most represented in the neighborhood. In regression contexts, $\hat{y}_i$ is simply the average of $y_i$ for the $k$-nearest neighors.

There are various flavors of voting. The procedure illustrated give all $k$ neighbors equal weight, but if its believed that closer points should be given more weight, then a variety of *kernels* such as inverse distance and biweights discount the importance of points that are farther away even if they are part of $k$. 

\vspace{12pt} 

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.cap = "Types of kernels used in voting.", results='asis'}
tab <- read.csv("data/kernels.csv")
pander::pander(tab, booktab = T, justify = "left",
               split.cell = 80, split.table = Inf,)

```

\vspace{12pt} 


_4. Tune k_. When $k = 10$ with a rectangular kernel, the conditional probability for $y_i$ reflects the 10-nearest neighbors. When $k = n$, the probability is equivalent to the sample mean. These two extremes illustrate that KNN is sensitive to the value of $k$ and the kernel, yet we do not truly know which is right. Tuning is a necessity to test different scenarios in order to optimize accuracy. 



\vspace{12pt} 

```{r, echo = FALSE, message = FALSE, warning=FALSE, fig.cap = "KNN algorithm process.", fig.height = 2}
#Illustration of kNN

require(ggplot2)
  n <- 200
  k <- 4
  
  #Create training data
    set.seed(100)
    x <- runif(n)
    
    set.seed(44)
    y <- runif(n)
  
    z <- ifelse(y > x*1.2 , 1, 0)
    z1 <- ifelse(y < 0.8, 1,0)
    z <- z*z1
    
    df <- data.frame(x, y, z)
    
  #Create test
    df_test <- data.frame(id = 1:3,
                          x = c(0.33, 0.63, 0.6),
                          y = c(0.6, 0.7, 0.5))
  
    
  #Create segments  
    segs <- data.frame()
    segs_all <- data.frame()
    for(i in 1:nrow(df_test)){
      d <- sqrt((df_test$x[i] - df$x)^2 + (df_test$y[i] - df$y)^2)
      segs <- rbind(segs,
                data.frame(id = i, 
                           x0 = df_test$x[i], 
                           y0 = df_test$y[i],
                           df[rank(d, ties.method = "random") <=k,]))
      segs_all <- rbind(segs_all,
                    data.frame(id = i, 
                               x0 = df_test$x[i], 
                               y0 = df_test$y[i],
                               df))
    }
    
  #Plot
    
  par(mfrow = c(1,3), mar = rep(3,4))
  #First plot
    plot(df$x, df$y, col = rgb(df$z, 0.8, 1- df$z, 0.4),
         pch = 20, cex = 2, xlab = "", ylab = "", 
         yaxt = "n", xaxt = "n",axes = FALSE,
         xlim = c(0.2, 0.7), ylim = c(0.3, 0.8), asp = 1)
    points(df_test$x, df_test$y, col = "black", 
           cex = 3, pch = 18)
    title(ylab="X1", xlab = "X2", line=1, cex.lab=1.2)
    title(main = "(1) Set up", line=1, cex.lab=1.2)
    
  #Second plot
    plot(df$x, df$y, col = rgb(df$z, 0.8, 1- df$z, 0.4),
         pch = 20, cex = 2, xlab = "", ylab = "", 
         yaxt = "n", xaxt = "n",axes = FALSE,
         xlim = c(0.2, 0.7), ylim = c(0.3, 0.8), asp = 1)
    segments(x0 = segs_all$x0,
             y0 = segs_all$y0,
             x1 = segs_all$x,
             y1 = segs_all$y, 
             col = rgb(segs_all$z, 0.8, 1 - segs_all$z, 0.1), 
             lwd = 1)
    points(df_test$x, df_test$y, col = "black", 
           cex = 3, pch = 18)

    title(ylab="X1", xlab = "X2", line = 1, cex.lab = 1.2)
    title(main = "(2) Calculate distance", line = 1, cex.lab = 1.2)
    
    
    #Third plot
      scored <- c(1, 0, 0)
      plot(df$x, df$y, col = rgb(df$z, 0.8, 1- df$z, 0.4),
           pch = 20, cex = 2, xlab = "", ylab = "", 
           xlim = c(0.2, 0.7), ylim = c(0.3, 0.8), asp = 1,
           yaxt = "n", xaxt = "n", axes = FALSE)
      segments(x0 = segs$x0,
               y0 = segs$y0,
               x1 = segs$x,
               y1 = segs$y, 
               col = rgb(segs$z, 0.8, 1 - segs$z, 1), 
               lwd = 2)
      points(df_test$x, df_test$y, col = rgb(scored, 0.8, 1- scored, 1), 
             cex = 3, pch = 18)
       title(ylab="X1", xlab = "X2", line = 1, cex.lab = 1.2)
      title(main = "(3) Vote using k-neighbors", line=1, cex.lab=1.2)
```

### In Practice

*__Tuning__*. Like many other algorithms, KNNs require tuning of *hyperparameters* in order to maximize predictive accuracy.  We do not know what is the true value of $k$ or the absolute best kernel to use, thus tuning of the hyperparameters is typically a grid search. The idea is to develop a ballpark sense of what works, then hone in on the best value of $k$. One search strategy involves testing all values from $k=1$ to $k=\sqrt{n}$ in multiples of one's choosing, keeping track of how each $k$ performs in terms of a loss function (e.g. TPR, FPR, F1-statistic). 

To illustrate this process, we have assembled a sample of USDA CropScape landcover data (@cropscape) for farmland that grows corn (yellow) and soybeans (green). More often than not, data will having missing values and capture only a fraction of the full picture. In our example below, we simulate this missingness by retaining a random sample of 10% of the land cover data. We naturally would like to know what is in the remaining 90%.  KNNs are an excellent choice for imputing data with a spatial component, but they depend heavily on the choice of $k$.  In the imputations below, we compare values of $k$ at 1, 5, 10, and 100. It is apparent that as the value of $k$ increases, the cornfields increasingly creep into areas where soy would be expected. Not only is this loss in accuracy reflected visually, but statistically as well. 


\vspace{12pt} 

```{r, fig.height=4, echo=FALSE, warning=FALSE, message = FALSE, fig.cap = "Comparison of prediction accuracies for various values of k."}

#LOAD LIB
  pacman::p_load(raster, sp, rgdal, kknn)
  
#LOAD DATA
  crops <- raster("data/cropscape.png")

#SET UP DATA
  vec <- as.data.frame(crops, xy = TRUE)
  
  set.seed(123)
  kg <- kmeans(vec$cropscape, 3)
  cll <- kg$cluster
  labs <- rep("corn", length(cll))
  labs[cll == 2] <- "soybeans"
  
  color <- rep("#ffff00", length(cll))
  color[cll == 2] <- rgb(0,1,0)
  color[cll == 1] <- "#95b1cc"
  
  
  vec <- cbind(vec, color, labs, cll)

 
## plot BASE example
  par(mfrow = c(2,3), mar = c(0, 1.2, 0.9, 0))
  
  plot(vec$x, vec$y, pch = 16, cex = 0.3, col = as.character(vec$color),
       axes = FALSE, legend = FALSE, asp = 1,  xlab = "", ylab = "")
  title(main="Farmland (Actual)", line= 0, cex.main=0.9, font.main = 1)
  
  #Randomize pulls and train for various k
  set.seed(123)
  train <- vec[runif(nrow(vec)) <= 0.1,]
   plot(train$x, train$y, pch = 16, cex = 1, col = as.character(train$color),
       axes = FALSE, legend = FALSE, asp = 1,  xlab = "", ylab = "")
  title(main="Farmland (10% sample)", line= 0, cex.main=0.9, font.main = 1)
  
   logger <- data.frame()
  
  for(i in c(1,5,10,100)){
  library(kknn)
  k1 <- kknn(color ~ x + y, 
              train = train, 
              test = vec,
              k = i,
              kernel = "rectangular")
  
  est <- table(vec$labs == "corn", k1$fitted.values == "#ffff00" )
  tpr <- round(100*est[2,2]/sum(vec$labs == "corn"),2)
  fpr <- round(100*est[1,2]/sum(vec$labs != "corn"),2)
  
  plot(vec$x, vec$y, pch = 16, cex = 0.3, col = as.character(k1$fitted.values),
       axes = FALSE, legend = FALSE, asp = 1, xlab = "", ylab = "")
      title(main=paste0("k = ", i, ": TPR = ", tpr, ", FPR = ", fpr), line= 0, cex.main=0.9, font.main = 1)
      
      logger <- rbind(logger,
                      data.frame( k = i,
                                  tpr = tpr,
                                  fpr = fpr))
  
  }

```


*__Normalization__*. Treating variables as coordinates implies that they all should have equal importance -- no single variable should weigh on the distance calculation more than any other. Of course, there are flavors of KNN where variables are weighted, but in the plain vanilla case, the scale of each variable should be normalized, at least for continuous variables. For example, if an input variable $x_1$ has a scale from 1 to 10,000 and $x_2$ ranges from 0.1 to 0.3, a KNN will rely more heavily on the latter variable. Normalization can be as simple as calculating the z-score for each $x_k$: 

$$ scaled = \frac{x_k - \mu_{x_k}}{\sigma_{x_k}}$$ 

where the transformed variable is mean centered with unit variance. 

There are, of course, cases when certain variables are more naturally suited for KNNs. They are particular useful for gridded data -- information recorded on equal intervals, including time series, sound, and imagery. But when the data contain mixed formats like categorical variables and integers, data should be transformed into the comparable units. Discrete variables can be converted into a dummy variable matrix while continuous variables can be binned into an ordinal scale. Manhattan distance would be a more suitable measure to relate records.


__*Speed and Importance*__. KNNs are best used when data sets are smaller with fewer variables as each distance calculation is computationally taxing. We can see that for a pair of training and test data sets, distance will need to be calculated $n_{train} \times n_{test}$ times, which can become more costly as the number of variables increases. Thus, KNN is a practical solution when the data are smaller. Furthermore, as more variables are added, the importance of any one variable is diluted. In situations where only a few variables contain meaningful information, the equal weighting assumption can introduce more noise than signal, relying on neighborhoods that might actually be far away.

Nonetheless, the simplicity of this approach makes it a popular choice for fill gaps in data.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/knn_sw.csv")
  colnames(usescases) <- c("Useful Properties", "Challenges")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "The good and ugly of KNNs.", 
                 justify = "left")

```


### DIY: Anticipating the extent of damage from a storm

Hurricane Sandy, for example, had a tremendous effect on New York City. Perhaps one of the most visible impact was the number of downed trees. A downed tree can cause property damage, bodily harm and traffic disruptions. Due to the high wind and lush foliage during Sandy, many trees fell. In NYC, the Department of Parks and Recreation is responsible for tree removal. When a resident makes a call to the city's services hotline 311, a work order is created and a tree removal team is dispatched. 

We would expect that downed trees are more likely to occur in *pockets* and proximity is the best indicator of activity. As the city knows where residents call for help, we know where downed trees have and have not been observed based on the nature of the 311 call. As only a fraction of neighborhoods may call on a given day, we can use the incoming data stream to update an imputed picture of the state of downed trees. With more complete information, city operations can efficiently and systematically triage the need for help. 

In this DIY, we examine the case of downed trees, using available 311 calls from the day after Hurricane Sandy to predict whether downed trees are reported in the proceeding seven days. The predicted probabilities can inform emergency response triage, guiding how areas in need are prioritized to help.

__Prepare the data__. The data are comprised of a pair of coordinates recorded in *state plane feet*, a grid cell `id`, a borough label `boro`, and a set of binary variables indicating if a downed tree was reported on the day of Hurricane Sandy (`tree.sandy`) or in the following seven days (`tree.next7`). 

The two binary variables were crafted under the assumptions that residents of a neighborhood would call to report downed trees if they are observed. If a call were made, then we could flag a neighborhood as one that experienced tree troubles. Alternatively, if complaints from a neighborhood are devoid of tree-related issues, we can assume that the area was unaffected. These assumptions were applied to NYC 311 call data, defining sub-neighborhoods as $1000ft \times 1000 ft$  grid cells -- $n=7513$ cells in all.

\vspace{12pt} 

```{r, message = FALSE, warning = FALSE, fig.height = 3}
#Load data
  nyc <- read.csv("data/sandy_trees.csv")
```

\vspace{12pt} 

For simplicity, we focus on the two largest and populous boroughs (`boro`), Brooklyn (BK) and Queens (QN), that share the same land mass and have similar geographic and geological characteristics. During the period of analysis, service requests were logged in a total of $n = 4477$ grid cells (59.6% of NYC) in Brooklyn and Queens. Of these, we only have information for 43.5% of grid cells at the passing of the hurricane, thus over half its not clear what has happened in the remaining 56.5% of the two boroughs. On the one hand, we can wait and see what has happened, or otherwise deploy a predictive strategy to paint a likely picture of the state of affairs.

\vspace{12pt} 

```{r, message = FALSE, warning = FALSE}
#Extract Queens and Brooklyn
  pacman::p_load(dplyr)
  nyc <- filter(nyc, boro %in% c("BK", "QN"))
```

```{r, message = FALSE, warning = FALSE, fig.cap = "Plot of X-Y coordinates for Brooklyn and Queens.", echo = FALSE}
  
  plot(x = nyc$xcoord, y = nyc$ycoord, 
       cex = 0.3, pch = 15, asp = 1, col = "grey",
       axes = FALSE, labs = FALSE, xlab = "", ylab = "")
```
\vspace{12pt} 


__Train__.  Our predictive approach is built on the assumption that data immediately available  after the storm (the target variable `tree.sandy`) contain enough information to make an informed guess about the status of the entire study area. The training sample (`train`) is constructed for where the variable `tree.sandy` is complete.  A quick tabulation shows that 79.7% ($n=1550$) of grid cells in the training set have at least one downed tree reported -- evidence that the storm had widespread impacts. The test set (`test`) is a collection of every grid cells in all of Brooklyn and Queens. This is simply a short-hand way of asking the kNN algorithm to produce a complete picture of what it can gleen from the data.

\vspace{12pt}

```{r}
#Extract the training and test samples
  train <- subset(nyc, !is.na(tree.sandy), 
                  select = c("ycoord", "xcoord", "tree.sandy"))
  test <- subset(nyc, 
                 select = c("ycoord", "xcoord", "tree.next7"))
  
#Split out
  table(train$tree.sandy)
```


With the data ready, we apply KNN to predict downed trees. While it is generally good practice to write algorithms from scratch at least once to fully understand its underpinnings, it is hard to ignore the efficiency gains made possible through the `kknn` library. The `class` library, which is the typical R library used for KNNs, is fit for purpose, but is limited to only classification problems and cross validation of the value of $k$. The `kknn` library provides additional flexibility to tune the type of kernel and consider not only classification problems, but continuous and ordinal targets as well. The library is furnished with a pair of functions to help tune a KNN and score new data sets.

\vspace{12pt} 

```{r}
  pacman::p_load(kknn)
```
\vspace{12pt} 

The `train.kknn` function implements k-folds cross validation to find the optimal set of hyperparameters. The function expects a few key inputs:

`train.kknn(formula, data, kmax, kernel, distance, kcv)`

- `formula` is a formula object (e.g. "`no.coverage ~ .`").
- `data` is a matrix or data frame of training data.
- `kmax` is the maximum number of neighbors to be tested
- `kernel` is a string vector indicating the type of distance weighting (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations, "gaussian" imposes a normal distribution on distance, "inv" is inverse distance).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g. 2 = euclidean, 1 = binary).
- `kcv` is the number of partitions to be used for cross validation.

For a first pass, we conduct 20-folds cross validation, searching between $k=1$ and $k = 100$ neighbors in combination with three kernels (rectangular, inverse, and biweight). This simple command does much of the hard work, running the KNN algorithm 6000 times (20 cross-validation models for each $k$ and *kernel* combination), then identifies the parameters that minimize classification error. The results are stored in the `fit_cv` object.

\vspace{12pt} 

```{r, message = FALSE, warning = FALSE}

#Set seed to ensure cross validation is replicable
  set.seed(100)

#Run with 20-folds cross validation
  fit_cv <- train.kknn(factor(tree.sandy) ~ ycoord + xcoord , 
                      data = train, 
                      kcv = 20, 
                      distance = 1, kmax = 100, 
                      kernel = c("rectangular", "inv", "biweight"))
```

\vspace{12pt} 

*What does the optimization process look like?* The `fit_cv` object contains diagnostics that are pertinent in identifying the best KNN model. But plotting the results, it is apparent that all kernels have comparable performance with error converging to a steady level beyond $k=20$. The optimal conditions, as stored in the `best.parameters` element, show that predictions are optimized when $k = $ `r fit_cv$best.parameters$k` using a rectangular kernel achieving a misclassification rate of under 20%. 

\vspace{12pt} 

```{r, message=FALSE, warning = FALSE, fig.height = 4, fig.cap = "20-fold cross validated errors for k = 1 to k = 100"}
   plot(fit_cv, cex = 0.5, cex.lab = 1)
```

\vspace{12pt}

By feeding the optimal conditions back to the `kknn` function, the algorithm can score and piece together a fuller picture of the extent of tree damage. The syntax is quite similar to the training function with some modifications: 

`kknn(formula, train, test, k, kernel, distance)`

- `formula` is a formula object (e.g. "`tree.sandy ~ .`").
- `train` is a matrix or data frame of training data.
- `test` is a matrix or data frame of test data.
- `k` is the number of neighbors.
- `kernel` is the type of weighting of distance (e.g. "rectangular" is unweighted, "biweight" places more weight towards closer observations).
- `distance` is a numerical value indicating the type of Minkowski distance. (e.g.  1 = binary, 2 = euclidean,).

Notice that both train and test are required by `kknn`. This is a consequence of KNN's instance-based learning -- it does not store relationships but rather applies a set of algorithmic logic to a test sample. 

\vspace{12pt}
```{r, message=FALSE, warning = FALSE}

#Retrieve best parameters
   best <- fit_cv$best.parameters

#Apply tune KNN parameters
   fit <- kknn(tree.sandy ~ ycoord + xcoord, 
               train = train, 
               test = test,
               k = best$k,
               kernel = best$kernel)

#Produce 
    test$prob <- fit$fitted.values
    test$tree.next7[is.na(test$tree.next7)] <-0
```
\vspace{12pt}


```{r, warning=FALSE, message=FALSE, echo = FALSE, echo = FALSE}
#Load libraries
  pacman::p_load(ROCR)

#Set up test data frame
  costAccuracy <- function(y, yhat){
    #
    # Calculate AUC using ROCR package
    #
    # Args:
    # y, yhat = binary target and predicted probabilities
    #
    # Returns: AUC value
    pacman::p_load(ROCR)
    pred.obj <- prediction(yhat, y)
    perf <- performance(pred.obj, measure = "auc")
    return(unlist(perf@y.values))
  }
```

__Evaluate performance__. Is a 20% misclassification error in the training sample a good result? Are KNNs effective at anticipating where tree-related calls will originate for the seven days after the hurricane? The habitual thing to do is to calculate some loss and accuracy statistic to summarize out-of-sample prediction performance, such as the Area Under the Curve (AUC) of the ROC Curve for assessing the robustness of a classification predictions. Rather than writing a new script to calculate the statistic, we can re-use the `costAccuracy` function constructed in the previous chapter. With an out-of-sample `AUC = ``r round(costAccuracy(test$tree.next7, test$prob),3)`, this first attempt with KNNs would have had some operational value, offering greater clarity during a chaotic time.  However, a single AUC value does not reveal how useful the predictions would have been for triaging downed tree requests. 

\vspace{12pt}
```{r, warning=FALSE, message=FALSE, eval = FALSE}
#Load libraries
  pacman::p_load(ROCR)

#Calculate ROC
  costAccuracy(test$tree.next7, test$prob)
```
\vspace{12pt}

A closer visual inspection of the predictions shows that the KNNs do in fact mold to the surface that it is trained to predict. Below, we compare the KNNs predictions produced using information that was available right as the storm passed versus the status of all grid cells collected over the seven days following the storm. The algorithm appears to be relatively effective in detecting both the unaffected (blue) and affected (red). Predictions are distributed across a gradient when the evidence is less clear, indicating that there is value in using the rankings for prioritizing aid.

\vspace{12pt}
```{r, message=FALSE, warning = FALSE, echo = FALSE, fig.height = 3, fig.width = 8, fig.cap = "Comparison of actual and predicted areas with reported downed trees. Red indicates at least one tree was reported in a given 1000 x 1000 square-mile area"}

  test_tube <- test[complete.cases(test$tree.next7), ]

 par(mfrow = c(1,3))

  plot(train[,2:1], main = "(1) Calls during storm",
       col = rgb(train$tree.sandy , 0, 1- train$tree.sandy, 1), 
       cex = 0.4, pch = 16, asp = 1, axes = FALSE, xlab = "", ylab = "")
  
  plot(test[,2:1], main =  "(2) Predicted probabilities",
       col = rgb(test$prob, 0, 1 - test$prob, 1), 
       cex = 0.4, pch = 16, asp = 1, axes = FALSE, xlab = "", ylab = "")
  
  plot(test[,2:1], main =  "(3) Actual next 7 days",
       col = rgb(test_tube$tree.next7, 0, 1 - test_tube$tree.next7, 1), 
       cex = 0.4, pch = 16, asp = 1, axes = FALSE, xlab = "", ylab = "")
    
```
\vspace{12pt}

When applied in a prioritization context where higher predicted probabilities are triaged and targeted first, the hit rates (e.g. finding downed trees) among the highest probability areas would be quite high -- nearly 80% at its peak. Approximately 40% of impacted grid cells have predicted probabilities north of 90%, suggesting that some areas were quite obviously impacted and the data contained enough signal to flag these areas.

\vspace{12pt}
```{r, echo = FALSE, message=FALSE, warning=FALSE, fig.cap = "Hit rate.", fig.height= 2.5 }
#Load
  pacman::p_load(dplyr)
#Bin the test probabilities to nearest 5%
  test$score_bucket <- round(test$prob / 0.05) * 0.05
#Calculate hit rates
  rates <- test %>% 
            group_by(score_bucket) %>%
            summarise(hit.rate = 100*round(mean(tree.next7),2),
                      cell.n = n(),
                      cell.target = sum(tree.next7))

  #Load
  pacman::p_load(ggplot2, gridExtra)
  
#Plot 
  a <-   ggplot(data = rates, aes(x = score_bucket, y = hit.rate)) +  
           geom_area(colour = "navy", fill = "navy") + 
    xlab("Predicted Probability") + ylab("Percent with downed trees") + 
    ggtitle("Hit rate") + 
    theme_minimal() + theme(text = element_text(size=10), title = element_text(size = 10))
  
  b <-   ggplot(data = rates, aes(x = score_bucket, y = cell.target)) +  
           geom_area(colour = "navy", fill = "navy") + 
    xlab("Predicted Probability") + ylab("Number of true cases") + 
    ggtitle("Number of affected grid cells") + 
    theme_minimal() + theme(text = element_text(size=10), title = element_text(size = 10))
  
  grid.arrange(a, b, ncol =2)

```
\vspace{12pt}

Despite the promising result, we should be cognizant that KNNs are among the simplest of prediction algorithms. They are great with relatively little data. We naturally should ask: _Is there a better classifier?_ 


## Decision Tree Learning

Lack of memory, lack of ability to identify useful variables, adapts to local patterns whereas KNNs fix the k

Unlike KNNs, decision tree learning molds []. Trees are designed to look at inputs and partition the sample into smaller more homogeneous cells with respect to the target. This recursive partitioning allows a tree to resemble an inverted tree: moving away from the base of the tree, the tree trunk splits into two or more large branches, which then in turn split into even smaller branches, eventually reaching even small twigs with leaves. 

Decision trees use recursive partitioning to learn patterns, doing so using central concepts of _information theory_. There are a number of decision tree algorithms that were invented largely in the 1980s and 1990s, including the ID3 algorithm, C4.5 algorithm, and Classification And Regression Trees for Machine Learning (CART). All these algorithms follow the same framework that includes the following elements: (1) nodes and edges, (2) attribute tests, and (3) termination criteria.


### Under the hood

__Anatomy of a decision tree__. The tree is comprised of nodes and edges. Nodes (circles) contain records. Edges (lines) show dependency between nodes and is the result of an *attribute test* -- or a process that finds the optimal criterion to subset records into more homogeneous groups of the target variable.  The node at the top of the tree is known as the *root* and represents the full population. Each time a node is split, the result is two nodes -- each of which is referred to as a *child node*. A node without any child nodes is known as a *leaf*. The node is labeled using majority voting based on whichever class is most represented.  The goal is to grow a tree from the root node into as many smaller child nodes that contain more of one class than another.

Decision trees split nodes based on finding thresholds along the input variables. There can be seemingly infinite number of potential variable-threshold combinations -- which is best? Drawing from *information theory*, we can apply an *information gain* formula to evaluate all candidate splits and find one that provides the most information. This optimal split yields to more homogeneous child nodes, which in turn can be split even further. The search for the best threshold is known as an *attribute test*.

As we can see in the example decision tree for health care insurance, each node is connected to at least one other node. Starting at the root node, we can see that overall, the population is labeled "no coverage" based on the decimal percentage $0.5$. The 100% indicates the proportion of the sample that is contained at the node. Below is `age >= 64`, which is the most informative  attribute test that is used to split. To the left, the edge leads to another node at the bottom left corner of the diagram, which contains people who are age 64 or older. While the leaf node only contains 12% of the entire sample, it is almost exclusively people who have health care coverage. To the right, the remaining 88% of the sample, which is further split by wage and other variables. Each leaf node is defined as the intersection of multiple binary criteria, giving way to profiles of users that can be easily segmented.

\vspace{12pt}

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "A grown decision tree.", fig.height = 2.5}

#Load packages
  require(rpart)
  require(rpart.plot)

#Set up data
  train1 <- train
  test1 <- test
  
  train1$xcoord <- 100*(train1$xcoord - min(train1$xcoord ))/diff(range(train1$xcoord))
  train1$ycoord <- 100*(train1$ycoord - min(train1$ycoord ))/diff(range(train1$ycoord))
  test1$xcoord <- 100*(test1$xcoord - min(test1$xcoord ))/diff(range(test1$xcoord))
  test1$ycoord <- 100*(test1$ycoord - min(test1$ycoord ))/diff(range(test1$ycoord))

#Train
  mod <- rpart(tree.sandy~xcoord + ycoord, data = train1, cp = 0.0042272)
  out <- predict(mod, test1)
  test1$prob <- out

#Plot
  rpart.plot(mod, shadow.col="gray", nn=TRUE, 
             box.palette = "BuGn", cex = 0.7)
```
\vspace{12pt}


__Growing trees__.   There are a number of decision tree algorithms that were invented largely in the 1980s and 1990s, including the ID3 algorithm, C4.5 algorithm, and Classification And Regression Trees for Machine Learning (CART). In all cases, the algorithms follow a common strategy: 

1. *Base Cases*.  The process starts with checking for "base cases" at the root node, the idea being that it might not be worth exerting effort to grow the tree if the data do not support it. The algorithm will first check to see if (a) all values of the target are of one class, and (b) none of the input variables offer any useful information. There are other base cases to consider depending on the algorithm, but any true base case will result in stopping the algorithm and returning only the root node. 

2. *Recursive Partitioning*. If none of the base cases are true, the algorithm proceeds with searching for a threshold along an input variable that splits the data into two smaller samples. With respect to the target, each partition should be more homogeneous in the composition of discrete classes or lower variance in the case of a continuous target. *How is the threshold determined?* Given a large number of inputs, there is likewise a large number of candidate splits. The optimal split is selected by an *attribute test* to identify the split that maximizes *information gain*, or a split that gives the most information, increased homogeneity or reduced variance. Attribute tests are applied to each resulting partition, thus the sample is *recursively* diced into smaller concentrated subsamples. 

3. *Stopping Criteria versus Pruning*.  At some point, the algorithm will need to stop. The question is *when*? One approach allows the tree to grow until each leaf node contains $n=1$, in which case we run the risk of an overgrown, overfit tree. Furthermore, like a real tree, a fully grown tree may take some time to grow when the data are large.  An alternative approach employs  *stopping criteria*. The tree partitioning is terminated if a leaf has fewer records than a pre-specified threshold (e.g. do not split if $n<5$), the information gain can no longer improve beyond a pre-specified level, among other criteria. And while these stopping criteria may sound reasonable, premature termination of the tree growing can prevent the tree from reaching its full potential -- it can bias the predictions. The happy medium grows a tree to its fullest, employing cross-validation to identify the optimal degree of tree complexity, then *pruning* the tree to the optimal level. 


__Attribute tests__.  The engine of the CART algorithm is information gain -- it is the statistical heart that enable tree learning to make meaningful predictions. Simply put, information Gain (*IG*) is a comparative measure that estimates the information content laerned (referred to as Impurity $I$) from splitting a parent node ($D_\text{parent}$) into a pair of child nodes $D_j$, taking the weighted contribution of each child node $j$.

$$IG = I(D_\text{parent}) - \sum_{j=1}^{J} \frac{n_j}{N}I(D_j)$$

The impurity measure is critical -- it gauges the amount of information content contained in a pair of partitions. It can also determine if one candidate split is more meaningful than an alternative. Impurity is typically measured using Gini Impurity, Entropy or Variance Reduction. The first two measures are utilized for classification problems, calculated on the proportion of records that belong to each of the $J$ target classes. Both have similar performance, though Gini Impurity is slightly easier to implement. Variance is a natural choice for continuous targets, designed for prioritizing splits that minimize within-partition variance. 

\vspace{12pt}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/table_impurity.csv", stringsAsFactors = FALSE)
  usescases <-usescases[,-3]
  colnames(usescases) <- c("Measure", "Formula", "Description")
  pander::pander(usescases, split.cell = 80, split.table = Inf, 
                 caption = "Common measures used for decision tree learning attribute tests.", 
                 justify = "left")
```
\vspace{12pt}


To illustrate the power of impurity metrics, consider the case of predicting which neighborhoods have downed trees, which emergency crews can use to prioritize resources. To simplify the problem, let's suppose that a single latitudinal boundary line can adequately separate affected and unaffected areas. *Which degree of latitude do we choose?* 

The table below presents two candidate boundaries. Split A is somewhere north and Split B is farther north (North+).  As quantitative thinkers, we naturally will attempt to make sense of the numbers to contextualize and weigh the pros and cons. Perhaps the distinguishing factor between the two options is the positive rate or maybe there is a ratio that can be calculated that elegantly illustrates the tradeoffs. In a tree learning setting, metrics like Gini Impurity are an effective comparison strategy.

\vspace{12pt}

```{r, echo = FALSE, message = FALSE, warning = FALSE, error=FALSE}
  usescases <- read.csv("data/gini_impurity_example.csv", stringsAsFactors = FALSE)
  colnames(usescases) <- c("Label", "A: North", "A: Remainder", "B: North+", "B: Remainder")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "Two sets of candidate splits.", 
                 justify = "left")
```
\vspace{12pt}

To evaluate which split is better, we calculate the *Gini Gain*, which is information gain (IG) when applying Gini Impurity ($I$). The IG value is calculated for each proposed split. Both calculations require the impurity of the root node  ($I(D_\text{region})$) to capture the baseline information content in the data, then the weighted impurity of the pair of candidate child nodes are subtracted to obtain the Gini Gain. Ideally, the child nodes would have values close to zero that indicate the presence of an overwhelming amount of signal, which in turn would mean a larger IG value is desirable.

$$IG_A = I(D_\text{region})  - \frac{n_\text{A,north}}{N}I(D_\text{north}) - \frac{n_{\text{A,remainder}}}{N}I(D_{\text{A,remainder}})$$


$$IG_B = I(D_\text{region})  - \frac{n_\text{B,north+}}{N}I(D_\text{B,north+}) -\frac{n_{\text{B,remainder}}}{N}I(D_{\text{B,remainder}})$$

As we step through each element of the calculation, it becomes apparent that Split B can more effectively isolate areas with downed trees, achieving an 100% hit rate in the *North+* partition. Granted, the estimated hit rate is on a small sample, nonetheless is a marked improvement over Split A's 66% hit rate in the *North* partition. Indeed, the IG calculation confirms that Split B would yield twice as much information as Split A. 

It is unlikely that a solitary split would do the job. For each partition (*North+* and *Remainder*), the attribute test would be applied recursively, searching for the optimal split as the tree grows deeper giving way to finer, more homogeneous partitions.

\vspace{12pt}

```{r ginicalcs , echo = FALSE, warning=FALSE, message=FALSE, error=FALSE}
inputs <- read.csv("data/gini-calcs.csv", sep = ",")
knitr::kable(inputs, booktabs = T,
                 caption = "Calculating information gain for each Split A and B.",
                 col.names = c("Element", "Split A", "Split B"),
                 justify = "left")
```



[Variable importance]


### In Practice

Decision trees are an incredible contribution to machine learning. Their versatility allows them to conduct automated variable selection on any type of data type, even when the number of input variables exceed the number of records. 



The way in which CART learns can detect discontinuities, interactions, and non-linearities in the data, which would otherwise be ignored when applying regression. Furthermore, it is the splitting mechanism that  allows for each leaf node to be concretely defined using a set of binary criteria. A cluster of neighborhoods affected with downed trees, for example, can be identified using as a bounding box defined by four pairs of geographic coordinate learned through the splitting process. 

There are detractors, however. Trees can grow so deeply that the model overfits. It does not take much to see why that may be. A leaf node can be comprised of as little as $n=1$, meaning that splits that occur deep into the tree might not be meaningful -- they are simply noisy. If left unpruned, leaf nodes can produce high variance predictions. Furthermore, there may be too many leaf nodes to articulate.   Nontheless, decision tree learning is an important contribution to classification problems and form the basis of many other algorithms.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
  usescases <- read.csv("data/dt_sw.csv")
  colnames(usescases) <- c("Useful Properties", "Challenges")
  pander::pander(usescases, split.cell = 80, split.table = Inf, caption = "The good and ugly of decision trees.", 
                 justify = "left")

```


### DIY: Predicting monetary and non-monetary relief

Everyday, consumers of financial products submit complaints to the Consumer Financial Protection Bureau (CFPB). Below, is an anonymized complaint written by a real consumer of a non-federal student loan:

> Navient has been unresponsive after I've asked to speak with several managers. I have called them consistently since summertime as my automatic payments were not being taken out. They say it came back as NSF, but my bank says it was never billed. Then Navient admitted to having I.T. issues. Late fees, interest, you name it. After calling to resolve many times, they offered me payment options and promised to repair my credit damage of their reporting me. After making each payment, they would change the terms and tell me I had to pay additional money or pay this or that, and then they would rescind the negative info. I finally scraped up the money and paid what it said I was past due on after their lies - and the online account still shows I owe money as of {xx/xx/xxxx} and its {xx/xx/xxxx} - so my payment made on {xx/xx/xxxx} obviously didn't post although it was taken from my bank account. I called again today and requested that a manager call me back - nothing yet. Supposedly they record all calls - well, someone should listen to all the lies they've been telling me over the months and changing the agreements. It is unethical and immoral. They are ruining students lives! Check the recordings and the call logs - I'm the one who calls them! And if they ever seldom call me, they never leave a voicemail but the recordings will prove they lie and say they did leave a voicemails, and I dispute that and tell them they are lying directly. listen to the recordings!

For transparency, CFPB publishes complaint narratives as an open, anonymized database that indicates the disposition of cases and their details. Looking at the text above, certain words and phrases signal nature of them problem and the word choice also signals the sentiment of the users. Those details, in turn, may explain whether companies provide the consumer any relief, whether monetary or otherwise. 


```{r, warning=FALSE, message=FALSE, echo = FALSE}
#Load data
  load("data/cfpb_dtm.Rda")
```


Imagine a scenario in which we would like to anticipate the outcome of a case or the label of a document before its available, but the only covariates are embedded in the text. A large team of people could read through and assign tags to each piece of text to generate usable covariates, but this is a long arduous task requiring significant manpower. Alternatively, what if we forced the unstructured text into a structure similar to data sets we have encountered before? Each narrative can been *tokenized*. First, text is standardized through stemming word endings (e.g. tell, tells, and telling all become "tell"), then are processed into *n-grams* of individual words and short word sequences. For example, "*the online account still shows I owe money*" contains eight unigrams (e.g. the, online, account, still, shows, I, owe, money), seven bigrams (e.g. the online, online account, account still, still shows, shows I, I owe, owe money), and six trigrams (e.g. the online account, online account still, account still shows, still shows I, shows I owe, I owe money). In total, the sentence yields a *bag of words* of 21 variables. Of course, some of these words are filler (e.g. "the", "I") and can be treated as *stop words* or words that can be removed as they do not likely contain signal. Combinations of these n-grams are the key to understanding what is associated with monetary relief -- it is a game of interactions that is well-suited for CART algorithm.

In this DIY, we explore how CART the power of a non-parametric approach can learn the intracies in text. Covering a 49-month period from March 2015 to March 2019, we have pre-processed CFPB data into a training set (`r paste0("n = ",nrow(train))`) and test set of the remaining years (`r paste0("n = ",nrow(test))`). Note that the training set is smaller than the test for ease of computation as text data sets tend to be highly dimensional. In fact, the pre-processed training set had 469,046 unique n-grams, but was reduced to the one-percent of tokens ($k = 4657$) with three or more character and appear ten or more times in 2015. For more on the  mechanics of working with unstructured textual data, refer to the advanced topics chapter.


```{r,  eval = F}
#Load CFPB data
  load("data/cfpb_dtm.Rda")
```

\vspace{12pt}


__Training__. Our primary target is the `target.series` variable that includes labels for *Closed with non-monetary relief* and *Closed with monetary relief*. There is a slight case of class imbalance, but so severe to require re-balancing. 

\vspace{12pt}

```{r}
#Quick summary
  table(train$target.series)
```

\vspace{12pt}


A cursory view of the frequency of words gives some indication of the structure of the narratives. The most frequent words help set the stage. CART will likely use less frequent words to modulate its predictions, relying on finer, thematic details to inform its predictions. Some words 

\vspace{12pt}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
#Importance
  vals <- apply(train[,-c(1:6)], 2, sum)
  vals <- vals[vals > 0]
  imp <- data.frame(var = names(vals), 
                     freq = vals)
  imp$var <- gsub("(t\\d{1,4}.)","", imp$var)
  imp <- imp[order(-imp$freq),]
  
#Table
  imp_table <- cbind(imp[1:10, 1:2], 
                    imp[1450:1459, 1:2],
                    imp[3800:3809, 1:2])
  colnames(imp_table) <- c("var.high", "imp.high", "var.mean", "imp.mean", "var.min", "imp.min")
  
#Render
  require(knitr)
  require(kableExtra)
  kable(imp_table, digits = 3,
         row.names = FALSE, "latex",
         col.names = c("Variable", "Frequency","Variable", "Frequency","Variable", "Frequency"),
        caption = "Sample words frequencies.") %>%
    add_header_above(c( "High" = 2, "Medium" = 2, "Low" = 2))

  
```

We make use of CART using the `rpart` library. 

\vspace{12pt}
```{r, message = FALSE, warning=FALSE}
  pacman::p_load(rpart)
```
\vspace{12pt}

The main function within the library comes with flexible capabilities to induct decision trees: 

`rpart(formula, method, data, cp, minbucket, minsplit)`

where:

- `formula` is a formula object. This can take on a number of forms such as a symbolic description (e.g. $y = f(x_1, x_2, ...)$ is represented as "`y ~ x1 + x2`). 
- `method` indicates the type of tree, which are commonly either a classification tree "class" or regression tree "anova". Split criteria can also be custom written.
- `data` is the data set in data frame format.
- `cp` is a numeric indicates the complexity of the tree. $cp = 1$ is a tree without branches, whereas $cp = 0$ is the fully grown, unpruned tree. If $cp$ is not specified, `rpart()` defaults to a value of 0.01.
- `minbucket` is a stopping criteria that specifies the minimum number of observations in any terminal leaf.
- `minsplit`  is a stopping criteria that specifies the number of observation in a node to qualify for an attribute test.

As a first pass, we'll run `rpart()` setting `cp = 0`, meaning that the tree will be fully grown without any stopping criteria applied. It may take the CART algorithm a few minutes to learn the patterns in the words.

```{r, message = FALSE, warning = FALSE}
fit <- rpart(target.series ~ ., 
             method = "class", 
             data = train[, -c(1:5)],
             cp = 0)
```

The `fit` object captures all of the inner workings of the decision tree. For example, just plotting the fit object will show the full depth of the tree. More importantly is the cross validation results collected at each level of additional complexity. Using the `printcp()` function, we can extract the *CP table*, which contains various accuracy measures associated with each value of the tree complexity value `cp`, including: 

- the number of splits `nsplit`,
- the prediction error in the training data `rel error`,
- the cross-validation error `xerror`, and
- the standard error `xstd`. 



```{r, message = FALSE, echo = FALSE, warning=FALSE}
#Get cross-validated error
  xerror <- fit$cptable[,4]  

#Lowest xerror
  best_error <- min(xerror)
  best_splits <- fit$cptable[,2][which(xerror == min(xerror))]
  best_sd <- fit$cptable[,5][which(xerror == min(xerror))]

#Optimal
  opt_error <- best_error + best_sd
  opt_select <- fit$cptable[,1][which(xerror <= opt_error)][1]
  opt_xerror <- fit$cptable[,4][which(xerror <= opt_error)][1]
  opt_select_split <- fit$cptable[,2][which(xerror <= opt_error)][1]
  
```


\vspace{12pt}

```{r, eval = F}
  printcp(fit)
```
```{r, echo = FALSE, message=FALSE, warning=FALSE}
  
#Render
  require(knitr)
  require(kableExtra)
  
  x <- as.data.frame(head(fit$cptable, 5))
  
  kable(x, digits = 3,
         row.names = FALSE, "latex", 
        caption = "First five levels of a CP table showing cross validated error by model compexity")
```

\vspace{12pt}


__Tuning__. *How do we find the optimal tree depth?* First, find the lowest cross-validation `xerror`, then find the tree that has the lowest number of splits that is still within one standard deviation `xstd` of the best tree^[@esl2001]. The idea behinds this rule of thumb takes advantage of uncertainty: the true value lies somewhere within a confidence interval, thus any value within a tight confidence interval of the best value is approximately the same. In this first model, the best tree has `r paste0("nsplit = ", best_splits)` and `r paste0("xerror = ", best_error)`. By applying the rule, the upper bound of acceptable error is `r paste0("xerror = ", round(best_error,6)," + ", round(best_sd, 6), " = ", opt_error)`. As it turns out, the tree with `r paste0("nsplit = ", opt_select_split)` is within one standard deviation and is thus the best model. 

In other words, the following function can extract the optimal `cp` value.

\vspace{12pt}

```{r}
bestCP <- function(fit_obj){
  ## Returns best CP val within 1 SD of lowest xerror
  #
  ## Args:
  ##   fit_obj: decision tree object
  #
  
  #Pull cross-validated error
    xerror <- fit$cptable[, 4]
  
  #Find lowest error and associated xstd
    best_error <- min(xerror)
    best_sd <- fit$cptable[, 5][which(xerror == best_error)]

  #Pull CP closest to lower bound
    lower_bound <- best_error + best_sd
    opt_select <- fit$cptable[,1][which(xerror <= lower_bound)][1]

   return(opt_select)
}
```

\vspace{12pt}

Now, we can prune the tree using the optimal `cp` value, then score both the test set. As a comparison point, we will also apply the unpruned model as well.

\vspace{12pt}

```{r, message = FALSE, warning = FALSE}
#Get best CP
  best_value <- bestCP(fit)

#Prune tree
  fit.opt <- prune.rpart(fit, cp = best_value)
  
#Score, returning probabilities
  pred.full <- predict(fit, test, type = 'class')
  pred.opt <- predict(fit.opt, test, type = 'class')
```

\vspace{12pt}


__What works__. One of the fascinating aspect of CART is its interpretability. Each terminal node is a set of binary criteria, making it possible to articulate under what conditions can the target occur. This is a reasonable mode of interpretable when trees are relatively simple. Alternatively, CARTs can be reviewed through variable importance that builds upon the *impurity* measures used to construct the trees.



Variable $X_m$
$v(s_t)$ is the variable used in split $s_t$

Mean Decrease Impurity or Gini Importance

$$Imp(X_m) = \frac{1}{N_T} \sum_T \sum_{t \in T:v(s_t)=X_m} p(t) \Delta i(s_t,t)$$

Where *Variable Importance* for variable $k$ is the sum of *Goodness of Fit* (e.g. Gini Gain or Information Gain) at a given split involving variable $k$. In other words, a variable's importance is the sum of all the contributions variable $k$ makes towards predicting the target. Below, we can see that the measure can be extracted from the `fit.opt` object. As may be expected, `accel` is not the main contributor to predictions, but rather measures of the maximum, mean and variability of acceleration. This also implies that the model could be further tuned by trying different windows for producing the engineered variables -- perhaps shorter or longer windows could be even more important.

\vspace{12pt}
```{r, eval = FALSE}
  fit.opt$variable.importance
```
\vspace{12pt}

When applied to the function to the predictions (`pred.opt` and `pred.full`), we find that the mean F1-statistics reached `meanF1(test$activity, pred.opt)` and ` meanF1(test$activity, pred.full)` -- not bad for a first cut, but certainly can benefit from extra attention.

\vspace{12pt}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
#Importance
 imp <- data.frame(var = names(fit.opt$variable.importance), 
                    impurity = fit.opt$variable.importance)
  imp$var <- gsub("(t\\d{1,4}.)","", imp$var)
  imp <- imp[order(-imp$impurity),]
  
#Table
  imp_table <- cbind(imp[1:10, 1:2], 
                    imp[65:74, 1:2],
                    imp[135:144, 1:2])
  colnames(imp_table) <- c("var.high", "imp.high", "var.mean", "imp.mean", "var.min", "imp.min")
  
#Render
  require(knitr)
  require(kableExtra)
  kable(imp_table, digits = 3,
         row.names = FALSE, "latex",
         col.names = c("Variable", "Impurity","Variable", "Impurity","Variable", "Impurity"),
        caption = "Words with high, medium and low importance for predicting monetary relief.") %>%
    add_header_above(c( "High" = 2, "Medium" = 2, "Low" = 2))

  
```

\vspace{12pt}



## Random Forests   

How do we know anything for sure? Virtually every aspect of life has some uncertainty tied in. When a hurricane approaches the US Eastern Seaboard, forecasters often map the *cone of uncertainty* that provides the possible range of motion of a storm based on the results of many forecasted simulations. In presidential elections, often times the most polling results are ones that ensemble or average the results of many other similarly conducted polls. The reliance on predictions from a group of models with the same aims may well improve prediction accuracy. In statistical learning, average the results of multiple models is known as *ensemble learning* or *ensembling* for short.

Single models may imposes biases on data and may be well-suited in specific situations. Ensemble methods combine the results of many models to obtain more stable results.  For example, the curve in graph #1 can be approximated using a decision tree algorithm. The result of a single tree only loosely fits the curve in a jagged fashion (#2). That one tree may impose biases on the data, perhaps through how the tree is pruned or the assumption that the jagged approximation is appropriate, which may then translate into greater variance in predictions. One could imagine that the structure of that one tree may have happened by chance, and under different situations, the fit could be better. 

Bootstrapping can help. Recall from elementary statistics that bootstrapping is defined as any statistical process that involves sampling records with replacement. By bootstrapping a sample, we treat a sample like a population, we can expose and characterize the qualities of an estimator under various scenarios already available in the data, which in turn produces an empirical probability distribution for predictions using the estimator. We can bootstrap the decision tree by (1) sampling the data with replacement up to the full size of the sample, then (2) run the decision tree. The result of repeating the process 50 times is (graph #3) produces a result that appears to be more organic and more accurate. This process of _bootstrapping_ and _aggregating_ the results is referred to as _bagging_.

\vspace{12pt} 

```{r, message=FALSE, warning = FALSE, fig.height = 2.5, echo = FALSE, fig.cap = "Comparison of results of applying a single model to fit a curve versus an ensemble of models."}

library(rpart)
library(gridExtra)
library(ggplot2)

set.seed(100)
x <- 1:100
y <- 5 + sin(x/20) + 2*cos(x/10)
df <- data.frame(x, y)

fit <- rpart(y ~ x, data = df)
df$yhat <- predict(fit, df)

base <- ggplot(df) + geom_line(aes(x = x, y = y))  + 
  ggtitle("(1) Actual" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

single_tree<- ggplot(df) + geom_line(aes(x = x, y = y)) +
  geom_line(aes(x = x, y = yhat), colour = "orange") + 
  ggtitle("(2) Single Tree" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

for(k in 1:50){
  temp <- df[sample(df$x, 100, replace=T),1:2]
  fit <- rpart(y ~ x, data = temp)
  yhat <- predict(fit, df)
  df <- cbind(df, yhat)
}

colnames(df)[4:ncol(df)] <- paste0("yhat",1:50)

many <- ggplot(df) + geom_line(aes(x = x, y = y)) + geom_line(aes(x = x, y = yhat), colour = "orange") +
  geom_line(aes(x = x, y = yhat2), colour = "red") + geom_line(aes(x = x, y = yhat3), colour = "orange") +
  geom_line(aes(x = x, y = yhat4), colour = "red") + geom_line(aes(x = x, y = yhat5), colour = "orange") +
  geom_line(aes(x = x, y = yhat5), colour = "red") + geom_line(aes(x = x, y = yhat6), colour = "orange") +
  geom_line(aes(x = x, y = yhat7), colour = "red") + geom_line(aes(x = x, y = yhat8), colour = "orange") +
  geom_line(aes(x = x, y = yhat9), colour = "red") + geom_line(aes(x = x, y = yhat10), colour = "orange") +
  geom_line(aes(x = x, y = yhat11), colour = "red") + geom_line(aes(x = x, y = yhat12), colour = "orange") + 
  geom_line(aes(x = x, y = yhat13), colour = "red") + geom_line(aes(x = x, y = yhat14), colour = "orange") +
  geom_line(aes(x = x, y = yhat15), colour = "red") + geom_line(aes(x = x, y = yhat16), colour = "orange") +
  geom_line(aes(x = x, y = yhat17), colour = "red") + geom_line(aes(x = x, y = yhat18), colour = "orange") +
  geom_line(aes(x = x, y = yhat19), colour = "red") + geom_line(aes(x = x, y = yhat20), colour = "orange")  + 
  geom_line(aes(x = x, y = yhat21), colour = "red") + geom_line(aes(x = x, y = yhat22), colour = "orange") + 
  geom_line(aes(x = x, y = yhat23), colour = "red") + geom_line(aes(x = x, y = yhat24), colour = "orange") +
  geom_line(aes(x = x, y = yhat25), colour = "red") + geom_line(aes(x = x, y = yhat26), colour = "orange") +
  geom_line(aes(x = x, y = yhat27), colour = "red") + geom_line(aes(x = x, y = yhat28), colour = "orange") +
  geom_line(aes(x = x, y = yhat29), colour = "red") + geom_line(aes(x = x, y = yhat30), colour = "orange") +
  geom_line(aes(x = x, y = yhat31), colour = "red") + geom_line(aes(x = x, y = yhat32), colour = "orange") + 
  geom_line(aes(x = x, y = yhat33), colour = "red") + geom_line(aes(x = x, y = yhat34), colour = "orange") +
  geom_line(aes(x = x, y = yhat35), colour = "red") + geom_line(aes(x = x, y = yhat36), colour = "orange") +
  geom_line(aes(x = x, y = yhat37), colour = "red") + geom_line(aes(x = x, y = yhat38), colour = "orange") +
  geom_line(aes(x = x, y = yhat39), colour = "red") + geom_line(aes(x = x, y = yhat40), colour = "orange")  + 
  geom_line(aes(x = x, y = yhat41), colour = "red") + geom_line(aes(x = x, y = yhat42), colour = "orange") + 
  geom_line(aes(x = x, y = yhat43), colour = "red") + geom_line(aes(x = x, y = yhat44), colour = "orange") +
  geom_line(aes(x = x, y = yhat45), colour = "red") + geom_line(aes(x = x, y = yhat46), colour = "orange") +
  geom_line(aes(x = x, y = yhat47), colour = "red") + geom_line(aes(x = x, y = yhat48), colour = "orange") +
  geom_line(aes(x = x, y = yhat49), colour = "red") + geom_line(aes(x = x, y = yhat50), colour = "orange")  + 
  ggtitle("(3) 50 Models" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())


df$average <- rowMeans(df[,4:ncol(df)])

avg <- ggplot(df) + geom_line(aes(x = x, y = y)) +
  geom_line(aes(x = x, y = average), colour = "blue") + 
  ggtitle("(4) Ensemble Average" ) + 
  theme(plot.title = element_text(size = 10), axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(base, single_tree,  many, avg, ncol = 2)
```

\vspace{12pt} 


Applying bagging to decision trees may not necessarily be enough to develop a well-balance prediction. In the social sciences and public policy, it is generally assumed that a model's specification is a choice left to the analyst; However, it may also be a source of methodological bias. 

_Random forests_ can help. The technique is an extension of decision trees using a modified form of bootstrapping and ensemble methods to mitigate overfitting and bias issues.^[@breiman2001] Not only are individual records bootstrapped, but input features are bootstrapped such that if $K$ variables are in the training set, then $k$ variables are randomly selected to be considered in a model such that $k < K$. Each bootstrap sample is exhaustively grown using decision tree learning and is left as an unpruned tree. The resulting predictions of hundreds of trees are ensembled. The logic is described below.

__Pseudo-code__
\vspace{12pt} 

```
Let S = training sample, K = number of input features
  1. Randomly sample S cases with replacement from the original data.
  2. Given K features, select k features at random where k < K.
  3. With a sample of s and k features, grow the tree to its fullest complexity.
  4. Predict the outcome for all records.
  5. Out-Of-Bag (OOB). Set aside the predictions for records not in the s cases.
Repeat steps 1 through 5 for a large number of times saving the result after each tree.
Vote and average the results of the tree to obtain predictions. 
Calculate OOB error using the stored OOB predictions. 
```
\vspace{12pt} 

The *Out-Of-Bag* (OOB) sample is a natural artifact of bootstrapping: approximately one-third of observations are naturally left un-selected, which can be used as the basis of calculating each tree's error and the overall model error. Think of it as a convenient built in test sample.

_How about interpretation?_ Unlike decision trees, it is not a simple task to deduce rules or criteria that describe the target variable. Instead, random forests use *variable importance*, which, like for a decision tree, measures the contribution of a feature to the homogeneity of a classifier. Unlike decision trees, variable importance for a Random Forest is calculated as the mean decrease in the Gini coefficient of a split relative to the Gini coefficient of the root node. Gini coefficients measures homogeneity on a scale of 0 to 1, where 0 is perfect homogeneity and 1 is perfect heterogeneity. The Gini changes are summed for each variable and normalized. 

\vspace{12pt} 

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Random Forests construct hundreds of trees sampling from both observations and features, then combine the trees into one prediction through voting.", fig.height = 3}

library(rpart.plot)
library(rpart)

id <- 1:100
y <- 5 + sin(id/20) + 2*cos(id/10)
df <- data.frame(id, y)

for(i in 1:100){
  x <- y + runif(length(id))*10
  df <- cbind(df, x)
  colnames(df)[i+2] <- paste0("X",i)
}


par(mfrow = c(1,5))
for(k in 1:3){
  temp <- df[sample(1:nrow(df), replace=T), 2:ncol(df)]
  sampled.vars <- sample(colnames(temp)[2:ncol(temp)], 3)
  fit <- rpart(y ~ ., data = df[, sampled.vars], cp = 0)
  rpart.plot(fit, shadow.col="gray", nn=TRUE, 
             main = paste0("Tree ", k, "\n uses ", paste(sampled.vars, collapse = ", "),""), cex.main = 1.1)
}

plot(c(0), pch = 19, col = "white", xaxt = "n", yaxt = "n", frame.plot=FALSE, xlab = "", ylab="")
text(1, 0, ". . .", cex = 5)
for(k in 500){
  temp <- df[sample(1:nrow(df), replace=T), 2:ncol(df)]
  sampled.vars <- sample(colnames(temp)[2:ncol(temp)], 3)
  fit <- rpart(y ~ ., data = df[, sampled.vars], cp = 0)
  rpart.plot(fit, shadow.col="gray", nn=TRUE, 
             main = paste0("Tree ", k, "\n uses ", paste(sampled.vars, collapse = ", ","")), cex.main = 1.1)
}
```

\vspace{12pt} 

### Tuning
Whereas methods like regression have a closed form solution, Random Forest require tuning as optimal models need to be searched for under different conditions. The principal tuning parameters include: Number of features and number of trees.

- _Number of input features_. As $k$ number of parameters need to be selected in each sampling round, the value of $k$ needs to minimize the error on the OOB predictions. 
- _Number of trees_ influences the stability the Variable Importance metric that is commonly used to infer variable influence in decision tree learning. More trees help to stabilize the Variable Importance estimate. To determine the number of trees, keep adding trees to a sample until the OOB error for a randomly select set of trees is approximately equal to that of the ensemble.

### DIY: Revisiting monetary relief

There are a number of R libraries that implement the Random Forest algorithm. The more commonly used version is `randomForest` as it automates most of the procedure, but is less scalable and efficient than its younger sibling `ranger`. As we will revisit the bag-of-words data set in this DIY, the `ranger` library will vastly reduce the time required to train the Random Forest. The `ranger` function expects at least a formula and a data frame, 

`ranger(formula, data, mtry, numtree)`

where: 
- `formula` is an expression of the model to be train. The target variable should be in factor format.
- `data` is a data frame.
- `mtry` (optional) is the number of variables to be randomly sampled per iteration. Default is $\sqrt{k}$ for classification trees. Default set to the square root of the number of variables.
- `ntree` (optional) is the number of trees. Default is 500.
- `importance` (optional) needs to be specified as "impurity" in order to retrieve variable importance measures.
- `num.threads` (optional) is a speed enhancing option that allows the many repetitive steps of a Random Forest to be parallelized across multiple cores or CPUs. By default, `ranger` uses the number of CPUs available.

Using the same formula as the `rpart()` function, we can train a Random Forest with default settings and check the OOB error. 
\vspace{12pt} 

```{r, warning=FALSE, message=FALSE, echo = FALSE}
#Load data
  load("data/cfpb_dtm.Rda")
```

```{r, warning=FALSE, message = FALSE}
#Load randomForest library
  pacman::p_load(ranger)

#Run Random Forest
  fit.rf <- ranger(target.series ~ ., 
                 data = train[, -c(1:5)], 
                 num.trees = 500,
                 importance = "impurity")

```


\vspace{12pt} 

Approximately 75.6% of observations in the OOB sample were correctly classified using  randomly selected variables in each of the 500 trees.

The `fit.rf` records a number of model outputs such as variable importance calculated as the Mean Decrease Gini. However, the values themselves do not have any meaning outside of a comparison with other Gini measures.

\vspace{12pt} 

```{r, warning=FALSE, message = FALSE, eval = FALSE}
  fit.rf$variable.importance
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
#Importance
  vals <- fit.rf$variable.importance
  imp <- data.frame(var = names(vals), 
                     freq = vals)
  imp$var <- gsub("(t\\d{1,4}.)","", imp$var)
  imp <- imp[order(-imp$freq),]
  
#Table
  imp_table <- cbind(imp[1:10, 1:2], 
                    imp[1450:1459, 1:2],
                    imp[3800:3809, 1:2])
  colnames(imp_table) <- c("var.high", "imp.high", "var.mean", "imp.mean", "var.min", "imp.min")
  
#Render
  require(knitr)
  require(kableExtra)
  kable(imp_table, digits = 3,
         row.names = FALSE, "latex",
         col.names = c("Variable", "Frequency","Variable", "Frequency","Variable", "Frequency"),
        caption = "Sample words frequencies.") %>%
    add_header_above(c( "High" = 2, "Medium" = 2, "Low" = 2))

  
```

\vspace{12pt} 

By default, the `ranger` library sets the number of trees to equal 500. But what if we would like to find the model that optimizes predictive accuracy? Random Forest algorithms can be tuned by the number of underlying trees in the forest (`num.trees`), the number of variables sub-sampled for any given tree (`mtry`), the minimum node size of terminal nodes (`min.node.size`) among others. 

As we know that $n = 500$ trees is more than enough, we will now need to tune the tree for the number of variables. To tune the algorithm, we will use the `tuneRF()` method. The method searches for the optimal number of variables per split by incrementally adding variables. While it's a useful function, it is relatively verbose. In addition to the target and input features, a number of other parameters need to be specified:

`tuneRF(x, y, ntreeTry,  mtryStart, stepFactor, improve, trace, plot)`

where: 
- `x` is a data frame or matrix of input features.
- `ntreeTry` is the number of trees used in each iteration of tuning.
- `mtryStart` is the number of variables to start.
- `stepFactor` is the number of additional variables tested per iteration.
- `improve` is the minimum relative improvement in OOB error for the search to go on.
- `trace` is a boolean that indicates where to print the search progress. 
- `plot` is a boolean that indicates whether to plot the search results. 


Below, we conduct a search from `mtryStart = 1` with a `stepFactor = 2`. The search result indicates that 2 variables per tree are optimal.

\vspace{12pt} 

```{r, warning=FALSE, message = FALSE, eval = F}
library(caret)
data(iris)

grid_params <-  expand.grid(mtry = c(1,4), 
                            min.node.size = 1,
                            splitrule = "gini")

fitControl <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = FALSE)

fit = train(
  x = iris[ , names(iris) != 'Species'],
  y = iris[ , names(iris) == 'Species'],
  method = 'ranger',
  num.trees = 200,
  tuneGrid = grid_params,
  trControl = fitControl
)

```

\vspace{12pt} 

Normally, we can plug the tuned parameter back into the `randomForest()` method and re-train the algorithm, but it unnecessary in this case as the default model already uses the same parameters. When applied to the test set, we see that the mean F1-statistic is much improved -- or a whole 10-percentage point increase.

\vspace{12pt} 

```{r, eval = F}
#Predict classes in test
  yhat <- predict(fit.rf, test)

```

\vspace{12pt} 


This result does not mean that Random Forests will always turn better results, but rather multiple techniques should be tested when tackling prediction problems. Also, remember the policy goal: is the objective to predict or to explain? If a little of both, then it is worth understanding the value of increased accuracy at the cost of interpretability. 

## References