# Chapter 8: Regression Methods

```{r ch8 load packages silently, echo = F}
invisible(library(pacman))
invisible(p_load(ggplot2, data.table, magrittr, viridis, ggthemes))
```

__Main question of the chapter:__ How can we construct predictions of wages, sales prices, test scores, and other continuous outcomes using their associated underlying characteristics?

__Gist:__ Decision makers often are missing information, which then requires one to use their gut. But this need not be the case in the information age. Suppose we would like to anticipate how wages are valued in the labor market. We can construct a statistical model that relates worker characteristics and skills to their wages. In other words, each characteristic is worth something in the marketplace and the average value of each characteristic can be assessed.  In application, constructing a regression model not only can fill in the blanks for missing cases, but show if someone is over- or undervalued relative to the market and identify disparities holding all else constant. Wage models are only one of a many forms of regression—techniques that predict the conditional expected value given some input attributes. Regression not only can support causal inference by measuring the average effect of an intervention, but also serve as scoring engines that help inform expectations. In this chapter, we introduce ordinary least squares (OLS)—the long standing workhorse of the sciences—and present it in the many contexts in which it is effective. We then extend regression to variants that are more flexible and are well-suited in big data environments.

__Outline__

- Opening Story Related To Policy or Public Affairs
- Regression
  - Basic formulation of OLS
    - Dummy variables ('hot encodings' and other names)
    - Hierarchical/categorical variables (LHS and RHS)
    - Make a joke: Things that look like numbers are not always numbers.
  - Prediction problems
  - Causal experiments
    - Regression in the context of effect identification and parameterization
    - ARIMAs for Interrupted Time Series?
    - OLS and its implicit weighting of heterogeneous effects
    - Sensitivity to outliers
- Regularized Regression
  - Although OLS is the long-standing workhorse in data analysis, it has shortcomings in the modern data age.
  - LASSO and Ridge Regression.
- k-Nearest Neighbors.
  - In cases where we believe that features have equal weight or there is not a clear pattern as to how targets are related, kNN is useful.
  - More...
- DIY
  - Housing sales price prediction (Data: NYC Housing Data)
  - Wage attainment and Oaxaca Decomposition (Data: US Census Bureau ACS)
  - An homage to Google Flu and a cautionary tale (Data: Google Trends and CDC)
  - Imputation models for different types of data (Data: Landsat NDVI, employment data)

## A Story of Regression

__TODO__

## Opening

In this chapter, we will focus on one of the classic workhorses of social science: linear regression. At its heart, linear regression is a fairly simple method for fitting a line (or a curve) through a set of data points. In other words: Linear regression helps you visualize and quantify how one variable (your _outcome_ or _dependent_ variable) tends to change as your other variables (the _explanatory_ or _independent_ variables) change. This linear regression setting will also bring up the age-old _correlation versus causation_ argument—just because your model says that $y$ (the dependent variable) depends upon $x$ (the independent variable) does not mean you've actually shown that changes in $x$ will __cause__ changes in $y$.

In addition to helping you understand how variables change as other variables change, linear regression can often do a surprisingly nice job with prediction. This prediction/forecasting discussion inevitably brings up the challenges of extrapolation. And the issues of extrapolation and overfitting will lead us into questions of model fit, leading us deeper down the rabbit hole into regularized regression settings (i.e., LASSO and ridge regression), weighting, heterogeneity, and k-nearest neighbors.

## Simple linear regression

Because linear regression focuses on fitting a line through a set of points, it is helpful to recall the formula of a line:

$$ y = mx + b $$

where $m$ gives the slope of the line^[Recall the "classic" definition of slope: rise over run (or the change in $y$ divided by the change in $x$).] and $b$ gives the $y$-intercept. In the land of regression, we tend to write the equation slightly differently:

$$ y = \beta_0 + \beta_1 x $$

where $\beta_0$ denotes the $y$ intercept, and $beta_1$ refers to the slope. For instance, here's a line with an intercept^[Regression jargon: You will generally hear/see "intercept" rather than "$y$ intercept".] of 5 and a slope of 0.5, i.e., $y = 5 + 0.5 x$.

```{r ch8 line equation, echo = F, fig.cap = "A line"}
  # Set seed
  set.seed(1)
  # Sample size
  n <- 25
  # Parameters
  b0 <- 5
  b1 <- 0.5
  # Create the data
  reg_dt <- data.table(
    x = runif(n = n, min = -10, max = 10),
    v = rnorm(n = n, mean = 0, sd = 1)
    )
  reg_dt[, y := b0 + b1 * x + v]
  # Plot best-fit line
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    stat_smooth(method = lm, se = F) +
    theme_pander()
```

### Choosing a line

So you know what a line is. And you know that linear regression fits a line through a set of data points. But are infinite potential lines you could fit through a set of points. So how does linear regression pick one line out of all of these possible lines?

Lets start with a set of points. For the moment, we're in the world of _simple linear regression_, which means we have one outcome variable ($y$), one explanatory variable ($x$) (and we're assuming the relationship between $y$ and $x$ can be explained by a simple line—i.e., an intercept and a slope). The points:

```{r ch8 points, echo = F, fig.cap = "Some data"}
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_point() +
    theme_pander()
```

As we mentioned above, there are infinite possibilities when it comes to drawing lines through this (or any) set of points. Let's draw three.

```{r ch8 three lines, echo = F, fig.cap = "Diagnosing fit: Three candidate lines"}
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_abline(
      aes(intercept = 10/4, slope = 0.25, color = "line 1"),
      size = 0.8
    ) +
    geom_abline(
      aes(intercept = 7, slope = 0.05, color = "line 2"),
      size = 0.8
    ) +
    geom_abline(
      aes(intercept = 5, slope = -0.5, color = "line 3"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    scale_color_viridis_d(end = 0.95) +
    theme_pander()
```

How do we choose the _best_ line for a given dataset? Line 3 looks pretty clearly worse than the other two lines, but why? And how would we choose between line 1 and line 2? Is there a better line out there? All of these questions push us toward a bigger question: How can we formalize the concept of _fit_ so that we can choose a line in a reasonable, repeatable^[Replication!], objective, and transparent manner?

The simple answer to this (big) question: Use the data. For any line that we draw, we can see how well it fits each point in the data but checking how _close_ the line gets to each point. If you think of the line as a prediction, then the distance between our proposed line and the actual data point gives us the _error_ based upon the prediction. Let's draw each of these _errors_ for line 1:

```{r ch8 line1 residuals, echo = F, fig.cap = "Diagnosing fit: Errors from line 1"}
  # Add the predictions from line 1 to the data table
  reg_dt[, y_hat1 := 10/4 + 0.25 * x]
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_segment(
      aes(xend = x, yend = y_hat1),
      color = "grey70", size = 0.3
    ) +
    geom_abline(
      aes(intercept = 10/4, slope = 0.25, color = "line 1"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    scale_color_manual("", values = viridis(3, end = 0.95)[1]) +
    theme_pander()
```

Each of the vertical grey lines connecting _line 1_ to a black point from the dataset illustrates the error from predicting that data point with the line. In regression land, this error gets a special name: the _residual_. Formally, we define the residual as the difference between the observed value ($y$) and the predicted value ($\widehat{y}$), i.e.,

$$ e = y - \widehat{y} $$

where $e$ refers to the residual. The predicted value for $y$ (i.e., $\widehat{y}$) comes from plugging in the associate value of $x$ into the line's equation. For example, the equation underlying line 1 is $\widehat{y} = 2.5 + 0.25 x$. So to get a prediction for observation $(x = 0, y = 3.75)$, we plug in $x = 0$ (and get $\widehat{y} = 2.5$).

Looking at the residuals from line 1, it seems pretty clear that we could do better. For instance, if we shifted the whole line up a bit, we would reduce all of the residuals. In addition, if we increased the slope---making it steeper---we would also reduce nearly all of the residuals. This type of reasoning is essentially what is going on in the background when linear regression methods choose the line that best fits a dataset: these methods attempt to minimize some measure of error.

### Mean squared error

In the classical linear regression model, the measure of error that we minimize is called _mean squared error_. Mean squared error (or MSE) is essentially what the name implies: it is the mean of the squared errors (the squared residuals), i.e.,

$$ \text{Mean squared error (MSE)} = \frac{1}{n} \sum_{i = 1}^{n} e_{i}^2 $$

So why are we squaring the error term? You might have thought we would just minimize the total (summed) error (or take the mean of the residuals), but there are a few reasons we tend to use MSE instead of mean error:

1. When you sum a positive and negative residual, they effectively cancel out each other. You might have many, many positive errors with one huge negative error, and your sum (or mean) could be very close to zero. To get around this issue, people will occasionally use mean (or total) absolute error—replacing the square of the residual with the absolute value of the residual (i.e., $|e|$).
2. Square errors turn out to be very mathematically tractable—meaning they are easy to work with in matrices. Summed absolute value does not have this advantage.
3. MSE is extra sensitive to values farther from the center of the data (for the explanatory variables). For some people, this sensitivity to outliers is a problem. For others, it is a selling point. We'll let you decide (and we'll discuss in depth later in the chapter).

So for now, minimizing MSE will be our objective.

### Ordinary least squares

We've revealed the secret hiding beneath most linear regression models that you will encounter: they choose the "best fit" line by finding the line that minimizes the mean squared error. The method that gives you this best-fitting line (from the perspective of minimizing the mean squared error) is called _ordinary least squares_ (or _OLS_ for short). In other words: OLS provides you with the line that minimizes the mean squared error for your dataset. Go out and find any other line; OLS will provide a smaller MSE.^[If your goal is to minimize the mean absolute error, then OLS is not best. Change your objective, and you will find a new winner.]

Let's see how OLS fits our data:

```{r ch8 ols fit, echo = F, fig.cap = "Diagnosing fit: OLS is best (for minimizing MSE)"}
  # OLS regression
  reg_lm <- lm(y ~ x, data = reg_dt)
  b <- reg_lm$coefficients
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_abline(
      aes(intercept = b[1], slope = b[2], color = "OLS"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    theme_pander()
```

Alright, so OLS does a nice job of fitting a line between though a dataset, but how does OLS actually come up with this line? Where do the numbers come from?

While you will rarely need to calculate the OLS intercept and slope coefficient by hand, in the case of one dependent variable and one independent variable, writing out the math actually offers some insights into the magic behind OLS.

But first: a bit of notation. It is standard notation to use Greek letters as the unknown parameters in our model. If we have a simple linear regression, then the model takes the form

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

where $y$ denotes our outcome (or dependent) variable, $\beta_0$ references the (unknown) intercept, $x$ refers to our single explanatory (or independent) variable, $\beta_1$ gives the slope of the line, and $\varepsilon$ represents unobserved errors in the model. We want to know/estimate $\beta_0$ and $\beta_1$. We observe $y$ and $x$. So our task is to estimate $\beta_0$ and $\beta_1$ using only what we "know" (i.e., our data: $y$ and $x$). Finally, we will refer to estimators by adding a hat to the parameter that the estimator estimates. For instance, $\widehat{\beta_1}$ estimates $\beta_0$.

Now for the good stuff.

OLS estimates the slope coefficient $\beta_1$ using the correlation between $y$ and $x$ (recall that _correlation_ measures the strength and direction of the linear relationship between two variables), the standard deviation of $y$, and the standard deviation of $x$. Specifically,

$$ \widehat{\beta_1} = \text{Cor}(x,y) \dfrac{s_y}{s_x} $$

which says that the OLS estimate for the slope coefficient comes from multiplying the correlation between $x$ and $y$ by the ratio of the their standard deviations.

Let's think about the intuition of this formula for a minute. First, the coefficient is based upon the correlation between the two variables, which makes a lot of sense. When the correlation (the strength of the linear relationship) is strong (close to -1 or 1), the coefficient will be larger. When the correlation is small (a weak linear relationship with correlation near zero), this estimated coefficient moves closer to zero. Furthermore, the sign of the coefficient will match the sign of the correlation between $y$ and $x$. This part also makes a lot of sense, as the correlation indicates whether $y$ tends to increase or decrease as $x$ increases—we want the estimated slope coefficient to match the correlation. Finally, the ratio of the standard deviations tells us how much $y$ relative to $x$. If $y$ covers a lot of distance, relative to a small distance in $x$, then we want a steeper line. If $y$ barely changes (a small standard deviation) relative to the variation in $x$, then the slope should be approximately zero.

The formula for the OLS estimator of the intercept is simpler

$$ \widehat{\beta_0} = \overline{y} - \widehat{\beta_1} \overline{x} $$

where $\overline{y}$ and $\overline{x}$ denote the mean of $y$ and $x$, respectively.

TODO: Explain this OLS formula.
