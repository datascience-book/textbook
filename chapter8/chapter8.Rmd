# Chapter 8: Regression Methods

```{r ch8 load packages silently, echo = F}
  # Packages
  invisible(library(pacman))
  invisible(p_load(ggplot2, latex2exp, data.table, magrittr, viridis, ggthemes))
  # Change directory to Chapter 8
  setwd("/Users/edwardarubin/Dropbox/Research/MyBooks/DataScience/Textbook/chapter8")
```

__Main question of the chapter:__ How can we construct predictions of wages, sales prices, test scores, and other continuous outcomes using their associated underlying characteristics?

__Gist:__ Decision makers often are missing information, which then requires one to use their gut. But this need not be the case in the information age. Suppose we would like to anticipate how wages are valued in the labor market. We can construct a statistical model that relates worker characteristics and skills to their wages. In other words, each characteristic is worth something in the marketplace and the average value of each characteristic can be assessed.  In application, constructing a regression model not only can fill in the blanks for missing cases, but show if someone is over- or undervalued relative to the market and identify disparities holding all else constant. Wage models are only one of a many forms of regression—techniques that predict the conditional expected value given some input attributes. Regression not only can support causal inference by measuring the average effect of an intervention, but also serve as scoring engines that help inform expectations. In this chapter, we introduce ordinary least squares (OLS)—the long standing workhorse of the sciences—and present it in the many contexts in which it is effective. We then extend regression to variants that are more flexible and are well-suited in big data environments.

__Outline__

- Opening Story Related To Policy or Public Affairs
- Regression
  - Basic formulation of OLS
    - Dummy variables ('hot encodings' and other names)
    - Hierarchical/categorical variables (LHS and RHS)
    - Make a joke: Things that look like numbers are not always numbers.
  - Prediction problems
  - Causal experiments
    - Regression in the context of effect identification and parameterization
    - ARIMAs for Interrupted Time Series?
    - OLS and its implicit weighting of heterogeneous effects
    - Sensitivity to outliers
- Regularized Regression
  - Although OLS is the long-standing workhorse in data analysis, it has shortcomings in the modern data age.
  - LASSO and Ridge Regression.
- k-Nearest Neighbors.
  - In cases where we believe that features have equal weight or there is not a clear pattern as to how targets are related, kNN is useful.
  - More...
- DIY
  - Housing sales price prediction (Data: NYC Housing Data)
  - Wage attainment and Oaxaca Decomposition (Data: US Census Bureau ACS)
  - An homage to Google Flu and a cautionary tale (Data: Google Trends and CDC)
  - Imputation models for different types of data (Data: Landsat NDVI, employment data)

__Questions__

- Do we want to include R^2^?

## A Story of Regression

__TODO__

## Opening

In this chapter, we will focus on one of the classic workhorses of social science: linear regression. At its heart, linear regression is a fairly simple method for fitting a line (or a curve) through a set of data points. In other words: Linear regression helps you visualize and quantify how one variable (your _outcome_ or _dependent_ variable) tends to change as your other variables (the _explanatory_ or _independent_ variables) change. This linear regression setting will also bring up the age-old _correlation versus causation_ argument—just because your model says that $y$ (the dependent variable) depends upon $x$ (the independent variable) does not mean you've actually shown that changes in $x$ will __cause__ changes in $y$.

In addition to helping you understand how variables change as other variables change, linear regression can often do a surprisingly nice job with prediction. This prediction/forecasting discussion inevitably brings up the challenges of extrapolation. And the issues of extrapolation and overfitting will lead us into questions of model fit, leading us deeper down the rabbit hole into regularized regression settings (i.e., LASSO and ridge regression), weighting, heterogeneity, and k-nearest neighbors.

## Simple linear regression

Because linear regression focuses on fitting a line through a set of points, it is helpful to recall the formula of a line:

$$ y = mx + b $$

where $m$ gives the slope of the line^[Recall the "classic" definition of slope: rise over run (or the change in $y$ divided by the change in $x$).] and $b$ gives the $y$-intercept. In the land of regression, we tend to write the equation slightly differently:

$$ y = \beta_0 + \beta_1 x $$

where $\beta_0$ denotes the $y$ intercept, and $beta_1$ refers to the slope. For instance, here's a line with an intercept^[Regression jargon: You will generally hear/see "intercept" rather than "$y$ intercept".] of 5 and a slope of 0.5, i.e., $y = 5 + 0.5 x$.

```{r ch8 line equation, echo = F, fig.cap = "A line"}
  # Set seed
  set.seed(1)
  # Sample size
  n <- 25
  # Parameters
  b0 <- 5
  b1 <- 0.5
  # Create the data
  reg_dt <- data.table(
    x = runif(n = n, min = -10, max = 10),
    v = rnorm(n = n, mean = 0, sd = 1)
    )
  reg_dt[, y := b0 + b1 * x + v]
  # Plot best-fit line
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    stat_smooth(method = lm, se = F) +
    theme_pander()
```

### Choosing a line

So you know what a line is. And you know that linear regression fits a line through a set of data points. But are infinite potential lines you could fit through a set of points. So how does linear regression pick one line out of all of these possible lines?

Lets start with a set of points. For the moment, we're in the world of _simple linear regression_, which means we have one outcome variable ($y$), one explanatory variable ($x$) (and we're assuming the relationship between $y$ and $x$ can be explained by a simple line—i.e., an intercept and a slope). The points:

```{r ch8 points, echo = F, fig.cap = "Some data"}
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_point() +
    theme_pander()
```

As we mentioned above, there are infinite possibilities when it comes to drawing lines through this (or any) set of points. Let's draw three.

```{r ch8 three lines, echo = F, fig.cap = "Diagnosing fit: Three candidate lines"}
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_abline(
      aes(intercept = 10/4, slope = 0.25, color = "line 1"),
      size = 0.8
    ) +
    geom_abline(
      aes(intercept = 7, slope = 0.05, color = "line 2"),
      size = 0.8
    ) +
    geom_abline(
      aes(intercept = 5, slope = -0.5, color = "line 3"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    scale_color_viridis_d(end = 0.95) +
    theme_pander()
```

How do we choose the _best_ line for a given dataset? Line 3 looks pretty clearly worse than the other two lines, but why? And how would we choose between line 1 and line 2? Is there a better line out there? All of these questions push us toward a bigger question: How can we formalize the concept of _fit_ so that we can choose a line in a reasonable, repeatable^[Replication!], objective, and transparent manner?

The simple answer to this (big) question: Use the data. For any line that we draw, we can see how well it fits each point in the data but checking how _close_ the line gets to each point. If you think of the line as a prediction, then the distance between our proposed line and the actual data point gives us the _error_ based upon the prediction. Let's draw each of these _errors_ for line 1:

```{r ch8 line1 residuals, echo = F, fig.cap = "Diagnosing fit: Errors from line 1"}
  # Add the predictions from line 1 to the data table
  reg_dt[, y_hat1 := 10/4 + 0.25 * x]
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_segment(
      aes(xend = x, yend = y_hat1),
      color = "grey70", size = 0.3
    ) +
    geom_abline(
      aes(intercept = 10/4, slope = 0.25, color = "line 1"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    scale_color_manual("", values = viridis(3, end = 0.95)[1]) +
    theme_pander()
```

Each of the vertical grey lines connecting _line 1_ to a black point from the dataset illustrates the error from predicting that data point with the line. In regression land, this error gets a special name: the _residual_. Formally, we define the residual as the difference between the observed value ($y$) and the predicted value ($\widehat{y}$), i.e.,

$$ e = y - \widehat{y} $$

where $e$ refers to the residual. The predicted value for $y$ (i.e., $\widehat{y}$) comes from plugging in the associate value of $x$ into the line's equation. For example, the equation underlying line 1 is $\widehat{y} = 2.5 + 0.25 x$. So to get a prediction for observation $(x = 0, y = 3.75)$, we plug in $x = 0$ (and get $\widehat{y} = 2.5$).

Looking at the residuals from line 1, it seems pretty clear that we could do better. For instance, if we shifted the whole line up a bit, we would reduce all of the residuals. In addition, if we increased the slope---making it steeper---we would also reduce nearly all of the residuals. This type of reasoning is essentially what is going on in the background when linear regression methods choose the line that best fits a dataset: these methods attempt to minimize some measure of error.

### Mean squared error

In the classical linear regression model, the measure of error that we minimize is called _mean squared error_. Mean squared error (or MSE) is essentially what the name implies: it is the mean of the squared errors (the squared residuals), i.e.,

$$ \text{Mean squared error (MSE)} = \frac{1}{n} \sum_{i = 1}^{n} e_{i}^2 $$

So why are we squaring the error term? You might have thought we would just minimize the total (summed) error (or take the mean of the residuals), but there are a few reasons we tend to use MSE instead of mean error:

1. When you sum a positive and negative residual, they effectively cancel out each other. You might have many, many positive errors with one huge negative error, and your sum (or mean) could be very close to zero. To get around this issue, people will occasionally use mean (or total) absolute error—replacing the square of the residual with the absolute value of the residual (i.e., $|e|$).
2. Square errors turn out to be very mathematically tractable—meaning they are easy to work with in matrices. Summed absolute value does not have this advantage.
3. MSE is extra sensitive to values farther from the center of the data (for the explanatory variables). For some people, this sensitivity to outliers is a problem. For others, it is a selling point. We'll let you decide (and we'll discuss in depth later in the chapter).

So for now, minimizing MSE will be our objective.

### Ordinary least squares

We've revealed the secret hiding beneath most linear regression models that you will encounter: they choose the "best fit" line by finding the line that minimizes the mean squared error. The method that gives you this best-fitting line (from the perspective of minimizing the mean squared error) is called _ordinary least squares_ (or _OLS_ for short). In other words: OLS provides you with the line that minimizes the mean squared error for your dataset. Go out and find any other line; OLS will provide a smaller MSE.^[If your goal is to minimize the mean absolute error, then OLS is not best. Change your objective, and you will find a new winner.]

Let's see how OLS fits our data:

```{r ch8 ols fit, echo = F, fig.cap = "Diagnosing fit: OLS is best (for minimizing MSE)"}
  # OLS regression
  reg_lm <- lm(y ~ x, data = reg_dt)
  b <- reg_lm$coefficients
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_abline(
      aes(intercept = b[1], slope = b[2], color = "OLS"),
      size = 0.8
    ) +
    geom_point() +
    labs(color = "") +
    theme_pander()
```

Alright, so OLS does a nice job of fitting a line between though a dataset, but how does OLS actually come up with this line? Where do the numbers come from?

While you will rarely need to calculate the OLS intercept and slope coefficient by hand, in the case of one dependent variable and one independent variable, writing out the math actually offers some insights into the magic behind OLS.

But first: a bit of notation. It is standard notation to use Greek letters as the unknown parameters in our model. If we have a simple linear regression, then the model takes the form

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

where $y$ denotes our outcome (or dependent) variable, $\beta_0$ references the (unknown) intercept, $x$ refers to our single explanatory (or independent) variable, $\beta_1$ gives the slope of the line, and $\varepsilon$ represents unobserved errors in the model. We want to know/estimate $\beta_0$ and $\beta_1$. We observe $y$ and $x$. So our task is to estimate $\beta_0$ and $\beta_1$ using only what we "know" (i.e., our data: $y$ and $x$). Finally, we will refer to estimators by adding a hat to the parameter that the estimator estimates. For instance, $\widehat{\beta_1}$ estimates $\beta_0$.

Now for the good stuff.

OLS estimates the slope coefficient $\beta_1$ using the correlation between $y$ and $x$ (recall that _correlation_ measures the strength and direction of the linear relationship between two variables), the standard deviation of $y$, and the standard deviation of $x$.^[Also recall that the standard deviation is simply the square root of the variance. The (sample) variance of a variable $x = \{x_1,\, \ldots,\, x_n\}$ is $s^2_x = \text{Var}(x) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})$ (where $\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i$, i.e., the mean of $x$). Thus, the standard deviation of $x$ = s_x = \sqrt{\text{Var}(x)}. The correlation between two variables $x$ and $y$ is given by $\text{Cor}(x,\,y) = \frac{\text{Cov}(x,\,y)}{s_x s_y}$, where $\text{Cov}(x,\,y) = \frac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{n-1}$ is the covariance of the two variables. It is also worth remembering that correlation is bounded between $-1$ (a strongly linear negative relationship) and $1$ (strongly linear positive relationship), whereas covariance is unbounded.] Specifically,

$$ \widehat{\beta_1} = \text{Cor}(x,y) \dfrac{s_y}{s_x} $$

which says that the OLS estimate for the slope coefficient comes from multiplying the correlation between $x$ and $y$ by the ratio of the their standard deviations.

Let's think about the intuition of this formula for a minute. First, the coefficient is based upon the correlation between the two variables, which makes a lot of sense. When the correlation (the strength of the linear relationship) is strong (close to -1 or 1), the coefficient will be larger. When the correlation is small (a weak linear relationship with correlation near zero), this estimated coefficient moves closer to zero. Furthermore, the sign of the coefficient will match the sign of the correlation between $y$ and $x$. This part also makes a lot of sense, as the correlation indicates whether $y$ tends to increase or decrease as $x$ increases—we want the estimated slope coefficient to match the correlation. Finally, the ratio of the standard deviations tells us how much $y$ relative to $x$. If $y$ covers a lot of distance, relative to a small distance in $x$, then we want a steeper line. If $y$ barely changes (a small standard deviation) relative to the variation in $x$, then the slope should be approximately zero.

The formula for the OLS estimator of the intercept is simpler

$$ \widehat{\beta_0} = \overline{y} - \widehat{\beta_1} \overline{x} $$

where $\overline{y}$ and $\overline{x}$ denote the mean of $y$ and $x$, respectively.

One way to interpret this equation is that OLS will run its line (with slope $\beta_1$) through the point $(\overline{x},\, \overline{y})$—a point that sits at the means of the two variables. We can calculate the $y$-intercept of the OLS regression line by starting at this point of the two means and following the line for $\overline{x}$ units until we hit the $y$ axis.

```{r ch8 ols intercept, echo = F, fig.cap = "The OLS regression line passes through a point at the means of the two variables"}
  # OLS regression
  reg_lm <- lm(y ~ x, data = reg_dt)
  b <- reg_lm$coefficients
  # Plot the points
  ggplot(data = reg_dt, aes(x = x, y = y)) +
    geom_abline(
      aes(intercept = b[1], slope = b[2], color = "OLS"),
      size = 0.8
    ) +
    geom_point(alpha = 0.2) +
    geom_point(
      data = reg_dt[, .(x = mean(x), y = mean(y))],
      size = 2
    ) +
    geom_text(
      aes(
        x = mean(reg_dt[,x]),
        y = mean(reg_dt[,y]),
        label = TeX("$\\left(\\bar{x},\\,\\bar{y}\\right)$", output = "character")
      ),
      hjust = 0.5, vjust = -1, nudge_y = -0.1, nudge_x = 0.1,
      parse = T
    ) +
    labs(color = "") +
    theme_pander()
```

### Our first regression in R

You now have a sense of how OLS creates its "best fit" regression line. Now let's run a regression in R. The basic^[And `base`.] linear regression function in R is `lm`, which stands for _linear model_. To run a regression with the `lm` function, you need to give `lm` a formula where the outcome variable (on the LHS) is separated from the explanatory variables (on the RHS) by a tilde (i.e., `~`). If the variables are part of an `data.frame` or similar object, then you also need to tell R the name of the object using the `data` argument in `lm`. For example,

```{R ch8 ex lm, eval = F}
  lm(y ~ x, data = my_data)
```

will regress the variable `y` from the data frame `my_data` on the variable `x` (also from `my_data`).^[This phrase "regression $y$ on $x$" is the standard way to refer to your outcome and explanatory variables: you regress your outcome variable on your explanatory variable(s).]

Now that we're ready to run a regression, we need some data. For this example, we will use 12,687 residential property sales from 2017 and 2018 in NYC.

```{R ch8 read sales data, warning = F, message = F, results = 'hide'}
  # Load 'readr' package
  library(readr)
  # Read in .csv of property sales
  sale_df <- read_csv("home_sales_nyc.csv")
```

While the dataset (`home_sales_nyc.csv`) contains a number of interesting variables, we are going to estimate the relationship between a property's sales price (the aptly named `sale_price` variable in the dataset) and the property's size (measured in square feet, i.e., the variable `gross_square_feet`). Before running any analysis, you should always plot your data to see if everything looks right.

```{r ch8 nyc sales plot, echo = T, fig.cap = "Residential property sales and property size, 2017-2018"}
  ggplot(data = sale_df, aes(x = gross_square_feet, y = sale_price)) +
    geom_point(alpha = 0.15, size = 1.2) +
    scale_x_continuous("Property size (gross square feet)", labels = scales::comma) +
    scale_y_continuous("Sale price (USD)", labels = scales::comma) +
    theme_pander()
```

In NYC's (crazy) housing market, it looks like bigger properties cost more (unsurprisingly), but the relationship is not as strong as you might have guessed. Let's actually run the regression now. Our outcome variable is `sale_price`, and our sole explanatory variable is `gross_square_feet`—both of which are contained in the data frame `sale_df`. We feed all of this information to the `lm` function, which we assign to an object named `reg_est`

```{R, ch8 nyc sales lm}
  reg_est <- lm(sale_price ~ gross_square_feet, data = sale_df)
```

The object that `lm` returns (which we've cleverly stored as `reg_est`) has its own class—`lm`. If you enter the name of the object into the console, R will return the estimates for the intercept and the slope coefficient on `gross_square_feet`—but that's about it:
```{R, ch8 lm console}
  reg_est
```
The real magic happens when you apply the `summary` function to the `lm` object, i.e.,
```{R, ch8 lm summary}
  summary(reg_est)
```

Now we have some information to digest.

First, R kindly reminds you of the actual call to `lm` that produced the estimates. That's nice, R.

Next, R prints a basic summary of the residuals—the error we make for each of the points in the dataset when we predict sale price with an intercept and gross square feet. We're missing some property sales by quite a lot. We can (and will) do better. Just wait.

After the residuals, are gives the coefficient estimates and several other important statistics. Within the `Coefficients` section, R first prints the intercept (i.e., `(Intercept)`). Wait! We never asked for an intercept... or did we? Because intercepts are standard in statistics, econometrics, data science, and essentially every other empirical field, R defaults to using an intercept—it assumes you want an intercept. If you do not want an intercept, then you need to put a `-1` in your `lm` equation.^[For example, you would write `lm(sale_price ~ -1 + gross_square_feet, data = sale_df)` if you wanted to estimate the relationship between sale price and property size without an intercept.] You will often hear an intercept interpreted as the predicted value for the outcome variable when the explanatory variable takes on a value of zero. However, this interpretation can often be misleading. For instance, this interpretation would say that a property in NYC with approximately zero square feet would be worth negative 46,000 dollars. This interpretation is obviously absurd but what about the statistics makes this interpretation so absurd? The problem is that this interpretation of the intercept generally ignores the fact that we only observe data for a certain set of values for the outcome and explanatory variables—we do not observe properties smaller than 120 square feet^[Which is still amazingly small.], and we do not observe any sales below $10,000.^[When assembling these data, we restricted the sample to sales between $10,000 and $10,000,000 so as to avoid gifted properties and extreme values.] Making predictions/interpretations outside of the range of your data is called _extrapolation_. Extrapolation is not necessarily bad—sometimes you have to make an educated guess about a data point that is unlike any data you have previously seen. However, when you extrapolate, you should be aware that you are moving outside of the range of data on which you fit your model—and the relationships between your outcome variable and explanatory variable(s) may look very different in different ranges of the data.

Next: The estimate slope coefficient on `gross_square_feet`. The estimated coefficient on square feet is approximately `r coef(reg_est)[2] %>% round(1)`. First, consider the _sign_ of the coefficient. The fact that this coefficient is positive indicates that properties with more gross square feet (bigger properties) tend to sell for more money. No surprises here. The actual value of the coefficient tells us that _in this dataset_ each additional gross square foot correlates with an increase in sales price of approximately `r coef(reg_est)[2] %>% round(0)` dollars.

But how confident should we be in this estimate? The next three columns provide us with tools for statistical inference—telling how precise our estimate is. The column labeled `Std. Error` gives the _standard error_, the column labeled `t value` gives the (Student's) t statistic for testing whether the coefficient is different from zero, and the column `Pr(>|t|)` gives the p-value for a two-sided hypothesis test that tests whether there is statistically significant evidence that our estimated coefficient differs from zero (i.e., testing the null hypothesis $\beta_1 = 0$). Common practices within statistics and social sciences suggests that p-values below 0.05 indicate sufficient statistical evidence to reject the null hypothesis. Here, because our p-value is (much) less than 0.05, we would say that we find statistically significant evidence of a non-zero relationship between a property's sales price and its gross square feet at the five-percent level.

__TODO__ How much do we assume/teach about hypothesis testing?

## Forming and testing hypotheses with regression

So why might we care about the relationship between price and property size? You'll often run into measures like _price per square foot_, but the idea behind _price per square foot_ is that there is some constant dollar per square foot paid within the market. If the relationship between sale price and square footage is not exactly linear—meaning that larger properties cost more (or less) per square foot than smaller properties—then this _price per square foot_ measure is flawed. It's probably important to know whether this measure is indeed flawed, but how do we test this hypothesis?

As is frequently the case both in policy analysis and in data science, there are several approaches we could take in testing this idea.

### Transforming the outcome variable

At the heart of our hypothesis is the question whether _price per square foot_ is indeed constant across property sizes—so let's try to directly test this! This hypothesis has _price per square foot_ as the outcome variable, so we need to transform our outcome variable. We then want to determine whether _price per square foot_ is correlated with the variable `gross_square_feet`. If the coefficient on the `gross_square_feet` differst (statistically) significantly from zero, then we can reject the hypothesis that price per square is constant across property sizes.

Our new outcome variable is price per square foot, which does not currently exist in the dataset. We _could_ create a new variable, but R's `lm` function allows us to create combinations of other variables as our outcome variable using the variables' names and mathematical operators. Thus, for our outcome variable, we will have `sale_price / gross_square_feet`. The explanatory variable will still be `gross_square_feet`.

__TODO__ Regression and plot.
```{R, ch8 nyc sales new reg}
  # Estimate the model
  reg_tran <- lm(sale_price / gross_square_feet ~ gross_square_feet, data = sale_df)
  # Summary of results
  summary(reg_tran)
```

```{r ch8 ny prop tran plot, echo = F, fig.cap = "Testing whether price per square foot is constant in footage"}
  ggplot(data = sale_df, aes(x = gross_square_feet, y = sale_price / gross_square_feet)) +
    geom_point(alpha = 0.15, size = 1.2) +
    geom_smooth(
      color = "orange",
      method = lm, formula = y ~ x,
      se = F, size = 0.8
    ) +
    scale_x_continuous("Property size (gross square feet)", labels = scales::comma) +
    scale_y_continuous("Dollars per square foot (USD)", labels = scales::dollar) +
    scale_color_viridis_d(begin = 0.3, end = 0.8) +
    theme_pander()
```


__TODO__ We can also plot a flexible "smoothed" line...

### Quadratic explanatory variables

Another interpretation of our hypothesis is whether the relationship between sales price and square footage is indeed a straight line. And one relationship that is not a simple, straight line is a quadratic relationship, i.e., a relationship where the outcome variable ($y$) depends on a squared explanatory variable ($x^2$)—not just the linear term ($x$).

A linear regression^[Notice that we're still using the term _linear regression_ even when the relationship between the outcome variable and explanatory variable is not strictly a straight line. Why? The _linear_ in _linear regression_ refers to the fact that the right-hand side of the regression equation can be written as a _linear combination_ of the unknown parameters (the $\beta$s) and the explanatory variables. In other words, _linear_ means that the $\beta$s are multiplied by the explanatory variables.] that allows for a quadratic relationship between $y$ and $x$ takes the form
$$ y = \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon $$

In our case, we can want to know whether there is statistically significant evidence of a quadratic relationship between sale price and gross square feet. Or in other words, we want to run the regression
$$ \text{Price} = \beta_0 + \beta_1 \text{Footage} + \beta_2 \text{Footage}^2 + \varepsilon $$
and then want to test the statistical evidence that $\beta_2$ differs significantly from zero.

The only thing we need to add to our previous regression in R is a squared `gross_square_feet` term. You could create a new variable in your dataset, squaring the variable `gross_square_feet`, but R has a simpler way: add the squared variable inside your regression. We can add `+ I(gross_square_feet^2)` to our regression, and R will know that we want to include the square of the variable `gross_square_feet`.

Let's do it.

First, we estimate the regression model (saving the estimated model as `reg_est2`):
```{R, ch8 nyc sales lm quad}
  reg_est2 <- lm(sale_price ~ gross_square_feet + I(gross_square_feet^2), data = sale_df)
```

And now we check the results:
```{R, ch8 lm summary quad}
  summary(reg_est2)
```

A hypothesis test of the coefficient on the quadratic term ($\beta_2$ above) indicates rejects the null hypothesis ($\beta_2 = 0$, meaning there is no quadratic relationship) at the five-percent level. What does all of this mean? There is statistically significant evidence supporting the hypothesis that sales prices are not constant across

That said, it can be hard to mentally visualize this estimated regression. Lucky for us: R does visualization. Let's plot the data, along with first model (with only linear footage) and the second model (with linear footage and quadratic footage).

```{r ch8 ny prop quad plot, echo = F, fig.cap = "Comparing linear and quadratic models"}
  ggplot(data = sale_df, aes(x = gross_square_feet, y = sale_price)) +
    geom_point(alpha = 0.15, size = 1.2) +
    geom_smooth(
      aes(color = "Model 1: Linear footage"),
      method = lm, formula = y ~ x,
      se = F, size = 0.8
    ) +
    geom_smooth(
      aes(color = "Model 2: Quadratic footage"),
      method = lm, formula = y ~ x + I(x^2),
      se = F, size = 0.8
    ) +
    scale_x_continuous("Property size (gross square feet)", labels = scales::comma) +
    scale_y_continuous("Sale price (USD)", labels = scales::comma) +
    labs(color = "") +
    scale_color_viridis_d(begin = 0.3, end = 0.8) +
    theme_pander() +
    theme(legend.position = "bottom")
```

It's hard to say which line _looks_ to be doing a better job explaining the price at which a property sales—maybe there is more going on than just size...

## Multiple regression (means more variables)

What if we are missing something? Maybe it's not just about size... maybe there are certain parts of NYC that are categorically more expensive than other parts of NYC... maybe Manhattan is just more expensive than Queens. Or maybe apartments are different than non-apartments.

In many cases, several—likely _many_—factors affect/predict the outcome variable. Whether the goal is to accurately predict the outcome variable (e.g., predicting property values) or explain the variation in the outcome variable (e.g., why some houses are worth much more than other houses), it is often helpful to bring in multiple explanatory variables—a concept you will hear people refer to as __multiple regression__.

Aside from having more explanatory (right-hand side) variables, multiple regression is pretty much the same thing as the simple-linear regression framework we just covered: you have a single variable as your outcome variable, and you estimate a set of parameters ($\beta$) by minimizing the sum of the squared residuals. The multiple linear regression model—now with $k$ explanatory variables—looks like

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \cdots + \beta_{k-1} x_{k-1} + \beta_k x_k + \varepsilon $$

where $y$ still represents our outcome variable, $\beta_0$ still gives the intercept, and $\varepsilon$ still denotes the unexplained error in the model. We now have $k$ explanatory variables ($x_1$ through $x_k$) and $k+1$ unknown parameters ($\beta_0$ through $\beta_k$).

Now that we've laid out our expanded framework for multiple linear regression, let's estimate expand our NYC property-value regression model.^[We actually already ran a multiple regression model when we regressed the price on footage and footage squared.] Specifically, let's estimate the model

$$ \text{Price} = \beta_0 + \beta_1 \text{Footage} + \beta_2 \text{Age} + \beta_3 \text{Year} + \varepsilon $$

which says that the sale price depends upon an intercept $\beta_0$,

```{R, }
  lm(
    sale_price ~
    gross_square_feet + age + sale_year,
    data = sale_df
  )
```

The interpretation of the estimated coefficients changes a bit when you are in a multiple linear regression setting. We now interpret the coefficients as the relationship between $y$ and $x_i$, _conditional_ on the other variables. What do we mean by 'conditional'? The estimate of the relationship comes from a model that includes a specific set of other variables. Thus, _conditional_ means we've estimated the relationship between $y$ and $x_i$ _controlling for_ the variables.

### Functional form matters

The multiple regression model that we've been discussing makes several important assumptions.

$$ \text{Price} = \beta_0 + \beta_1 \text{Footage} + \beta_2 \text{Age} + \beta_3 \text{Year} + \varepsilon $$

- Linear effects of footage, age, and year
  - What about effects that are not explained by a simple line? Some effects may be a polynomial
  - What if the effect of one variable depends upon the levels of other variables? Interactions...
  - Other effects may be
- How do we know which model/functional form is better?

### Measures of fit

- Common/classic: R^2^ (adjusted and unadjusted)
- AICC/BIC
- Key: You need to penalize models with more variables, because you will always see a reduction in the MSE when you add an additional variable.
- R^2^ w/ cross validation?

### Indicator (dummy) variables

What happens when you have a

### Categorical variables

Categorical variables are dummy variables.

__Public-service announcement:__ This is a (dire) warning: not all things that look like numbers are actually numerical variables:^[And may not be cardinal at all.] many categorical variables are _coded_ with numbers, for instance, zip codes, area codes, interstates, serial numbers, NAICS codes (for classifying industries in North America). Fight the temptation to use all numbers as numeric. It helps to think about your data—and you'll avoid some very embarrassing moments.

### Interactions

```{R, }
  ggplot(data = sale_df,
    aes(x = gross_square_feet, y = sale_price, color = as.character(borough))
  ) +
  geom_point(alpha = 0.25) +
  geom_smooth(method = lm, formula = y ~ poly(x,2), se = F) +
  theme_pander() +
  theme(legend.position = "bottom")
```

## Prediction?

## Causality
