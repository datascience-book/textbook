---
title: "Chapter 3. DIY: Keywords"
output:
  pdf_document: default
  html_document: default
---



### I have a lot of text. How do extract keywords from text?
  
#### Motivation {-}

Tabular data can sometimes be a luxury. For it to exist, it requires someone to spend time and effort to meticulously collect and structure data into a clean, well-defined format. Textual data, in contrast, does not have structure.  As is obvious to most people, text conveys meaning, but not all of the words in a document are necessary to understand its contents. We naturally search for keywords and groups of keywords, which then become structured information. From the structure information, we can glean what topics are contained in numerous documents. When done at scale, it becomes possible to traverse boundless amounts of textual information and sift the things that matter.

This is the basic idea of text processing and natural language processing (NLP). Text and language can be manipulated to glean insight at scale, enabling one to answer questions such as:

- What are keywords best describe a text? (e.g. tagging)
- How do two texts relate to one another?
- Which other texts are similar?
- What's the tone of a corpus of documents?
- What distinct topics are mentioned in the text and by whom?


#### Principles {-}

NLP is a vast and growing field. Similar to the example presented at the beginning of the chapter, basic text manipulation starts with _tokenization_, or the process of parsing a character sequence or string into smaller pieces referred to as tokens. In the case of a sentence, a token may be a word, but it also may take on the form of a n-word phrase. The goal is to convert strings into smaller more comparable units. Thus, before tokenization is applied to a sentence, strings are converted into comparable formats such as capitalization (`tolower()` or `toupper()`), without punctuation (`gsub()`), among others. 

Upon cleaning the text, _n-grams_ -- a sequence of n-sequential tokens -- can be derived from each sentence. For example, let's suppose we were to tokenize the following two sentences:

> Maya is a physical scientist.
> Olivia is a data scientist.

From these two sentence, we can derive a vector of uni-grams and bi-grams for each sentence:

```{r, echo = FALSE}
sentence <- c(rep(1,5), rep(2,5))
gram1 <- c("Maya", "is", "a", "physical", "scientist", "Olivia","is","a","data","scientist")
gram2 <- c("Maya is", "is a", "a physical", "physical scientist", "", "Olivia is", "is a", "a data", "data scientist", "")

knitr::kable(cbind("Sentence" = sentence, `1-gram` = gram1, `2-grams` = gram2), booktabs = TRUE, 
             caption = "n-grams for two sentences")
```

This should result in a vector of many words and short phrases.  Notice that short sentences quickly grow into a larger number of records. Imagine when a corpus of documents contains thousands of records that need to be analyzed. To whittle down the data to essential terms, stop words such as "is and "an" can be removed, which remove words that are essentially the padding that makes language sound good. Removing stop words also reduces the storage and process requirements.

From the vector of words, term frequencies can be tabulated for each sentence or document. In some cases, term frequencies can be used to identify topics or represent the absolutely significance of certain words. But, more often than not, the _relative_ importance of words is more meaningful. _Term Frequency - Inverse Document Frequency_ (TF-IDF), a simple term re-weighting calculation, can vastly improve ranking of importance of words by converting frequencies into values that reflect relative importance between textual documents. TF-IDF is widely used in information indexing and search systems to help re-rank documents based on terms The calculation is as follows:

$$\text{TF-IDF} = \frac{n_{it}}{n_i} \times ln(\frac{N}{N_t})$$ 

where $n_t$ is the number of times term $t$ appears in a document $i$, $n$ is the number of terms in document $i$, $N$ is the total number of documents, and $N_t$ is the number of documents that contain term $t$. This two step calculation compares the relative prevalence of a term in a document and scales it by that term's prevalence in the corpus -- a simple, but powerful analytical trick. 
From these fundamental processing steps, more advanced techniques can be built on top such as topic modeling, which is used to find latent groups of topics within documents, as well as build content-based recommendation engines that suggest products based on how the qualities of a product overlap with a consumer's interest profile. There are plenty of libraries designed to make text processing easy, such as `tm` and `tidyr`, but to understand the underlying mechanics, we will illustrate basic processing with base R functionality.


#### A Worked example {-}

  Political scientists and journalists often times count the number of times Congress applauds the President when delivering the State of the Union (SOTU) Address as well as analyzes the number of times words are used. While it is not a clear science, applying data manipulation techniques to create an analyzable dataset can certainly be fascinating. To illustrate a real clean up workflow with some data manipulation, we will use the SOTU transcripts from the Obama Administration. An interesting attribute of the transcripts are that they reflect the number of  applause breaks as planned for by the speechwriters and policymakers as opposed to the actual number. Using this data, we will answer the following three questions: 
  
  1. How many breaks were planned in 2010 vs 2016?
  2. What were the top 10 words used in each of those state of the unions?
  3. Which words experienced relatively greater use?

__How many planned applause breaks in 2010 versus 2016?__ To start, let's load just the 2010 data into memory and inspect one paragraph from the speech.  Looking at the data, the speechwriters included queues for applause as denoted as `(Applause.)`. 

The 2010 data can be accessed using the `digIt()` function

```{r, warning=FALSE, message=FALSE}
  library(digIt)
  speech <- digIt("speech_2010")
```

 or using `readLines()` to import the data directly via from the Amazon Web Services server that hosts the data:
 
```{r, eval = FALSE}
  speech <- readLines("https://s3.amazonaws.com/dspp/speech_2010.txt")
```

Upon loading the data, we remove blank lines and take a look at a randomly selected line in the speech.
```{r}
  speech <- speech[speech!=""]
  speech[12]
```
Using that piece of information, we can write a relatively short set of steps to match the *Applause* pattern. 57 breaks were planned in 2010, which is 11 more than the 46 breaks planned in 2016. While the same code was run two separate times, we will learn in a subsequent chapter how to automate repetitive tasks.
  
\vspace{12pt}     
```{r}
  
#2010
  #read in lines from the text
  speech10 <- digIt("speech_2010")
  
  #remove any blank lines
  speech10 <- speech10[speech10!=""]
  
  #get string position of each Applause (returns positive values if matched)
  ind <- regexpr("Applause", speech10)
  sum(attr(ind,"match.length")>1)
  
#2016
  speech16 <- digIt("speech_2016")
  speech16 <- speech16[speech16!=""]
  ind <- regexpr("Applause", speech16)
  sum(attr(ind,"match.length")>1)
```
\vspace{12pt}     
  
__What were the top words in 2010 vs 2016__

  To do this, we'll need to do some basic cleaning to start (e.g. remove punctuation, remove numbers, remove non-graphical characters like `\r`), parse the words into a vector of words or 'bag of words', and aggregate words into word counts.
  
\vspace{12pt}     
```{r}
#2010
  #Clean up and standardize values
  clean10 <- gsub("[[:punct:]]","",speech10)
  clean10 <- gsub("[[:digit:]]","",clean10)
  clean10 <- gsub("[^[:graph:]]"," ",clean10)
  
  #convert into bag of words
  bag10 <- strsplit(clean10," ")
  bag10 <- tolower(trimws(unlist(bag10)))
  
  #Count the number of times a word shows up
  counts10 <- aggregate(bag10, by=list(bag10), FUN=length)
  colnames(counts10) <- c("word","freq")
  counts10$len <- nchar(as.character(counts10$word))
  counts10 <- counts10[counts10$len>2,]
  counts10 <- counts10[order(-counts10$freq),]
  head(counts10, 10)
  
#2016
  clean16 <- gsub("[[:punct:]]","",speech16)
  clean16 <- gsub("[[:digit:]]","",clean16)
  clean16 <- gsub("[^[:graph:]]"," ",clean16)
  
  bag16 <- strsplit(clean16," ")
  bag16 <- tolower(trimws(unlist(bag16)))
  
  counts16 <- aggregate(bag16, by=list(bag16), FUN=length)
  colnames(counts16) <- c("word","freq")
  counts16$len <- nchar(as.character(counts16$word))
  counts16 <- counts16[counts16$len > 2,]
  counts16 <- counts16[order(-counts16$freq),]
  head(counts16, 10)
```
\vspace{12pt}     

Looking at the words above, it feels a bit unsatisfying. To improve the list, we'll use a stop word list to remove words that hold little meaning (e.g. the padding language).
  
\vspace{12pt}     
  
```{r}

#Import and remove stop words
 stopwords <- digIt("stopwords")
 stopwords <- as.vector(stopwords)
  
#Remove stop words
  counts10 <- counts10[!(counts10$word %in% stopwords),]
  counts16 <- counts16[!(counts16$word %in% stopwords),]
  
```
\vspace{12pt}     

In addition, the importance of words may not be well-represented using term frequencies. The words "America" and "Freedom" are likely to appear in many SOTU speeches, but do not reflect the distinct foci of each address. We can write a simple function to calculate `TF-IDF` in order to surface terms of relative importance with respect to each speech.

```{r}
tfidf <- function(terms, freq, doc){
  #
  # Desc:
  #   Returns a TF-IDF index for a set of terms and documents
  #
  # Args:
  #   terms = vector of terms
  #   freq = vector of frequencies for each term
  #   doc = vector of document membership 
  #
  # Returns:
  #   TF-IDF values for each term by document
  
  #Calculate components
    N <- length(unique(doc))
    Nt <- aggregate(doc, by = list(terms), FUN = length)
    colnames(Nt)[2] <- "Nt"
    nit <- freq
    ni <- aggregate(freq, by = list(doc), FUN = sum)
    colnames(ni)[2] <- "ni"
    
  #Combine  
    out <- data.frame(terms, doc, N = N, nit = nit)
    out <- merge(out, Nt, by.x = "terms", by.y = "Group.1", all = T)
    out <- merge(out, ni, by.x = "doc", by.y = "Group.1", all = T)
    out$tfidf <- (out$nit/out$ni) * log(out$N / out$Nt)
  
  #Return
    return(out[order(-out$tfidf), c("doc", "terms",  "tfidf")])
}

```


We can now reweight the terms of the two speeches. Note that TFIDF values will become more distinct with a greater diversity of textual documents. 

\vspace{12pt}     
```{r, eval = FALSE}
#Append data sets together
  master <- rbind(data.frame(doc = 2010, counts10),
                  data.frame(doc = 2016, counts16))

#Weight results
  reweighted <- tfidf(terms = master$word,
                      freq = master$freq,
                      doc = master$doc)
  
#Results
  head(reweighted[reweighted$doc==2010,], 10)
  head(reweighted[reweighted$doc==2016,], 10)
```
```{r, echo = FALSE}
#Append data sets together
  master <- rbind(data.frame(doc = 2010, counts10),
                  data.frame(doc = 2016, counts16))

#Weight results
  reweighted <- tfidf(terms = master$word,
                      freq = master$freq,
                      doc = master$doc)
  
#Results
  out <- cbind(head(reweighted[reweighted$doc==2010,], 10), head(reweighted[reweighted$doc==2016,], 10))
  knitr::kable(out, booktabs = TRUE, row.names = FALSE)
```
\vspace{12pt}     

  At this point, we've arrived at words that are more presidential sounding, but can still be whittled down to the core message. But that can be left for another day.
  
  
#### Exercises {-}
- Modify the code to compare the 2010 SOTU to the 2012 SOTU.
- Optimize the above code to import and process 2010 through 2016 SOTUs to estimate the TF-IDF values.
