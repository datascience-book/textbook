---
title: "Chapter 4: Readying the Data"
output:
  pdf_document: 
      number_sections: true
  html_document: default
  geometry: margin=1in
---

# Concepts

## Text is data

Speeches contains a wealth of information. As humans, we are taught to understand verbal and written communication -- pick out the nouns, verbs, and adjectives, then combine the information to decipher meaing. Take the following excerpt from the 2010 State of the Union:

> Now, one place to start is serious financial reform.  Look, I am not interested in punishing banks.  I'm interested in protecting our economy.  A strong, healthy financial market makes it possible for businesses to access credit and create new jobs. It channels the savings of families into investments that raise incomes.  But that can only happen if we guard against the same recklessness that nearly brought down our entire economy. 
We need to make sure consumers and middle-class families have the information they need to make financial decisions.  We can't allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy.

To many, text might not be considered data, but they are. Our minds analyze the text and arrive at inferences about what the author or speaker is signaling. Analyzing text is largely a matter of identifying key terms:

> ~~Now, one place to start is serious~~ financial reform.  ~~Look, I am not interested in~~ punishing banks.  ~~I'm interested in~~ protecting our economy.  ~~A~~ strong, healthy financial market ~~makes it possible for~~ businesses ~~to access~~ credit ~~and~~ create new jobs. ~~It channels the~~ savings of families ~~into~~ investments ~~that~~ raise incomes.  ~~But that can only happen if we guard against the same~~ recklessness ~~that nearly brought down our entire~~ economy. 
~~We need to make sure~~ consumers ~~and~~ middle-class families ~~have the information they need to make~~financial decisions. ~~We can't allow~~ financial institutions, ~~including those that take your~~ deposits,~~to take risks that threaten the whole~~ economy.

Much like the logic that guides keyword identification, text can be shaped from an unstructured dataset into a well-defined, structured dataset: 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
  library(knitr)
  library(kableExtra)
  library(rvest)
  library(plyr)
  library(DT)
  speech <- "Now, one place to start is serious financial reform.  Look, I am not interested in punishing banks.  I'm interested in protecting our economy.  A strong, healthy financial market makes it possible for businesses to access credit and create new jobs. It channels the savings of families into investments that raise incomes.  But that can only happen if we guard against the same recklessness that nearly brought down our entire economy. We need to make sure consumers and middle-class families have the information they need to make financial decisions.  We can't allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy. "
   stripped <- gsub("[[:punct:]]","",speech)
  stripped <- gsub("[[:digit:]]","",stripped)
  stripped <- gsub("[^[:graph:]]"," ",stripped)
  bag <- strsplit(stripped," ")
  bag <- tolower(trimws(unlist(bag)))
  
#Stopwords
  library(digIt)
  stoplist <- digIt("stopwords")
  stoplist <- as.vector(stoplist[,1])
  
#Read through text
  vals <- as.data.frame(table(unlist(bag)))
  vals <- vals[!(vals$Var1 %in% stoplist),]
  vals$len <- nchar(as.character(vals$Var1))
  vals <- vals[vals$len > 2,]
  vals <- vals[order(-vals$Freq),]
  colnames(vals) <- c("Terms", "Frequency of Term","Number of Characters")
  row.names(vals) <- 1:nrow(vals)
  
  kable(head(vals,4), booktabs = TRUE,
  caption = 'Most frequent terms found in excerpt.')  %>%
            kable_styling(latex_options = c("hold_position"))
```


Of course, this process could be done manually, but imagine sorting through all 7,304 words in the 2010 address or scaling the process to the roughly [_1.9 million words_](http://www.presidency.ucsb.edu/sou_words.php) in State of the Union addresses between 1790 and 2016. All the steps required to convert unstructured text into usable data can be done with a little bit of planning, technical imagination and data manipulation. Every little detail about the data needs to be considered and meticulously converted into a usable form. From a data format perspective, capitalized characters are not the same as lower case. Contractions are not the same as terms that are spelled out. Punctuation affects spacing. Carriage returns and new line markers, while not visible in reading mode, are recorded. 

Let's take one line from above and dissect the changes that need to be made:

> "We need to make sure consumers and middle-class families have the information they need to make financial decisions.  We can't allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy."

We then turn everything into lower case so all letters of the alphabet are read the same.

> "we need to make sure consumers and middle-class families have the information they need to make financial decisions. we can't allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy."

Then, we get rid of punctuation by substituting values with empty quotations (`""`).

> "we need to make sure consumers and middleclass families have the information they need to make financial decisions  we cant allow financial institutions including those that take your deposits to take risks that threaten the whole economy"

Each space between each word can be used as a _delimiter_ that can be used as a symbol for a program to  break apart words into elements in a list. 

```{r, warning=FALSE, echo=FALSE, message=FALSE}
example <- "We need to make sure consumers and middle-class families have the information they need to make financial decisions. We can't allow financial institutions, including those that take your deposits, to take risks that threaten the whole economy."
example <- gsub("[[:punct:]]","",tolower(example))
example <- as.character(unlist(strsplit(trimws(example), " ")))
  kable(matrix( c(example, rep("",3)), nrow = 8,ncol = 5), booktabs = TRUE,
  caption = 'Terms')  %>% kable_styling(latex_options = c("hold_position"))
```

There are words in there that don't add much value as they are commonplace and filler. In text processing, these words are known as *stop words*. In each domain, the list of stop words likely differs, thus data scientists may need to  build a customized list. For simplicity, we've used a stop words list that is used in mySQL -- an open source relational database management system. The result is the list of remaining words. 

```{r, warning=FALSE, echo=FALSE, message=FALSE}
  example <- example[!(example %in% gsub("[[:punct:]]","", stoplist))]
  
  knitr::kable(matrix(c(example, rep("",1)), nrow = 4,ncol = 4), booktabs = TRUE,
  caption = 'Terms after removing stop words')  %>%
            kable_styling(latex_options = c("hold_position"))
```

From that data, we can aggregate the data into a form that is meaningful to answer a research question. For example, the frequency of words may provide a clue as to what the text is about. In this case, each "financial" and "make" appear twice in the text, perhaps indicating that there is an orientation towards action (make) for financial considerations. 

```{r, warning=FALSE, echo=FALSE, message=FALSE}
  examp <- as.data.frame(table(example))
  examp <- examp[order(-examp$Freq), ]
  end <- cbind(examp[1:7,], rbind(examp[8:13,],data.frame(example = "", Freq = "")))
  colnames(end) <- c("Term", "Freq", "Term", "Freq")
  
  
  knitr::kable(end, caption = "Term Frequencies", booktabs = TRUE, row.names = FALSE)  %>%
            kable_styling(latex_options = c("hold_position"))
```

This is just the tip of the iceberg. Text processing is just one aspect of readying data for use. 

There are entire texts dedicated to data engineering and manipulating data. This chapter is intended to be a simple primer, providing a brief review of programming paradigms that are necessary to shape data into usable form so it can be the engine that drives impact in the public and social sectors. The chapter begins with illustrating a workflow for retrieving and assembling data, then explores common methods of manipulating data values and formats, and closes with manipulating data structures.


# Retrieval and Assembly

The retrieval and assembly of a data set  can be one of the greatest barriers to launching a project. Here, we lay out a few practices that make the process far simpler. 

There are a multitude of data storage formats in use such as extensible markup language (XML), comma separated values (CSV), Excel files among others. Each has its quirks. Some require special software to load while others simply need time and patience to crack the code. Fortunately, R is equipped to load virtually all data formats. Below is a recommended set of functions that are easy to use and flexible.

 \vspace{12pt} 
```{r, echo = FALSE, warning=FALSE, message = FALSE, fig.cap = "Recommended functions to load an assortment of files."}
require(knitr)

temp.table <- read.csv("data/loading_functions.csv")
pander::pander(temp.table, split.cell = 60, split.table = Inf, justify = "left",
               caption = "Recommended functions to load an assortment of files.")
```
 \vspace{12pt} 
 
 
In the course of writing this text, the authors have come across a number of misconceptions about how to work with data, leading to haphazardous manual editing of data files. In particular, we have found that: 

- Analysts will often manually open files and delete unnecessary rows at the top of a file as functions may not be able to read in files. 
- The number of columns will differ, requiring columns to be deleted or empty columns added before loading.
- Column names differ, requiring manual editing of headers.
- The edits often overwrite the original file. This means that an edit were erroneous, there is no hope in restoring the original data. 
- Manual edits are learned and shared through as an oral history.

These manual steps become tedious endeavor when presented with hundreds if not thousands of files. When loading and assembling data sets, the raw data should be stored as a copy. It then should be read into the programming environment where all processing is conducted. Since the processing is coded, it can be reproduced. The result is then stored as a new file. 

In this section, we illustrate a simple workflow that is efficient and accurate. Our goal is to see if Energy Information Administration (EIA) spot gas prices recorded in New York Harbor and the US Gulf Coast are correlated. To do this, we will need to assemble a time series data set taking the following: 

- Load a CSV of spot gas prices from the US Gulf Coast as a data frame.
- Load two Excel worksheets of spot gas prices from NY Harbor from two different time windows as data frames, then append one to the other.
- Combine the two into a single data frame as illustrated below, then calculate the Pearson Correlation coefficient. 

 \vspace{12pt} 
```{r, echo = FALSE, warning=FALSE, message = FALSE}
require(knitr)

temp.table <- read.csv("data/gas_target.csv")
pander::pander(temp.table, split.cell = 60, split.table = Inf, justify = "left",
               caption = "Target format")
```
 \vspace{12pt} 
 

__Loading CSVs__. Before reading in a file, it is generally a good idea to open the file in a code editor to take a peek at the structure, but "blind loading" the file can also give clues as well. Let's start with the CSV `doe_usgulf.csv` using the `read_csv` function from the `readr` package, assign the data to the object `gulf`, then inspect the first few rows using `head`.

 \vspace{12pt} 
```{r, warning = FALSE, message = FALSE}
library(readr)
gulf <- read_csv("data/doe_usgulf.csv")
head(gulf, 3)
```
 \vspace{12pt} 
 
It appears that the data was successfully loaded. The data is read in as a *tibble*, which is a variant of a data frame designed for more efficient data manipulation. The headers require cosmetic changes. Generally, it is good practice to keep column names as the same case (lower case) and avoid the use of spaces and punctuation with the exception of periods. We can edit the column headers, which can be called as a vector using `colnames(gulf)`, by assigning a new string vector that turns all letters into lower case (`tolower`) and replaces substrings (`gsub`) that are eiher spaces or punctuation (`[[:space:][:punct:]]`) with periods. These string manipulation commands will be covered in more detail later in this chapter.
 
 \vspace{12pt} 
```{r, eval = FALSE}
colnames(gulf) <- tolower(gsub("[[:space:][:punct:]]", ".", colnames(gulf)))
```
 \vspace{12pt} 
 
However, as these column names are quite long, we can opt to overwrite them with something more concise. Notice that we did not need to open the CSV and manually edit the column names.
 
 \vspace{12pt} 
```{r}
colnames(gulf) <- c("date", "gulf.price")
```
\vspace{12pt} 

__Loading an Excel file__. Excel files tend to have a few more curve balls than CSVs. While Excel files are more readable by aesthetic and cosmetic styling, parsing is challenging. Using `read_excel` from the `readxl` package, we take a first look at the first sheet (`sheet = 1`) of our Excel file `doe_ny.xlsx`, then use `head` to examine what was loaded. Unlike our CSV, our first attempt requires some tweaks: The `Date` header is treated as the first row of data and there are two additional empty columns. 

\vspace{12pt} 
```{r, message = FALSE, warning = FALSE}
library(readxl)
attempt1 <- read_excel("data/doe_ny.xlsx", sheet = 1)
head(attempt1, 3)
```
\vspace{12pt} 

At this point, some may be tempted to manually delete the first row. Fortunately, we can skip rows when using most loading functions using a `skip` argument. Below, we read in the first and second sheets of `doe_ny.xlsx`, skipping the first row in each, then rename the headers.

\vspace{12pt} 
```{r, message = FALSE, warning = FALSE}
#Load first two sheets
sheet1 <- read_excel("data/doe_ny.xlsx", sheet = 1, skip = 1)
sheet2 <- read_excel("data/doe_ny.xlsx", sheet = 2, skip = 1)

#Rename columns
colnames(sheet1) <- c("date", "ny.price")
colnames(sheet2) <- c("date", "ny.price")
```
\vspace{12pt} 

__Appending data__. It turns out that *sheet1* and *sheet2* are from the same time series of spot covering different time windows. We can construct a new, more complete data frame by appending one data frame to the other using the `rbind` function.

\vspace{12pt} 
```{r}
  ny <- rbind(sheet1, sheet2)
```
\vspace{12pt} 

The above scenario is the ideal: two data frames with the same number of columns and the same column names. But what if one data frame has at least one more column than the other? The `rbind.fill` function in the `plyr` package appends the two data frames together, filling any additional missing columns with `NA` values. Below, rather than loading the entire `plyr` library, we can selectively load functions using the double colon operator `::`.

\vspace{12pt} 
```{r}
  ny <- plyr::rbind.fill(sheet1, sheet2)
```
\vspace{12pt} 

Note that if two columns in two different data frames represent the same concept but have different names, `rbind.fill` does not know to make the connection. Column names are interpreted literally, meaning that it is of the utmost importance that headers are exactly labeled.

__Combining data by rows__. The two data frames `gulf` and `ny` contain the same number of rows. If they are in the same exact desired order and have the same number of rows, we can use the `cbind` function to join the datasets by rows. The `cbind` function will join  all columns in both data frames together, including the dates. To ensure the data is neatly organized, we keep all columns from the `gulf` dataset and only the second column from the `ny` data set. The resulting data frame contains three columns containing daily spot prices from two sources. 

We calculate the correlation using the `cor` function finding a value of $\rho = 0.989617$ -- the two sets of prices are highly correlated. Note that `cor` only accepts numeric values, thus the date column is temporarily dropped.  

\vspace{12pt} 
```{r}
#Combine data
  prices <- cbind(gulf, ny[,2])

#Calculate correlation
  cor(prices[,-1])
```
\vspace{12pt} 


# Manipulating values

More often than not, working with raw data means working with string values. It is easy to imagine that necessary data are embedded in a sentence. For example, below is a vector of four string values: 

\vspace{12pt} 
```{r, warning=FALSE, message = FALSE}
  budget <- c("Captain's Log, Stardate 1511.8. I have $10.20 for a big galactic mac.",
            "The ensign has $1,20 in her pocket.", 
            "The ExO spent has $0.25 left after paying for overpriced warp core fuel.",
            "Chief medical officer is the high roller with $53,13.")
```
\vspace{12pt} 

What if we need to extract the total available funds available to buy galactic big macs? All four elements contain dollar values, which can benefit from feature engineering. To do so, we use a combination of text manipulation functions and _regular expressions_ or _regex_ --  a series of characters that describe a regularly occurring text pattern.
  
First, commas should be replaced with a period using `str_replace_all()`, assigning the result to a new object `new`.  Note that in some regions, such as Europe, commas are used as decimals rather than periods.

\vspace{12pt} 
```{r, warning=FALSE, message = FALSE}
  require(stringr)
  new <- str_replace_all(budget, ",", "\\.")
```
\vspace{12pt} 

Second, find the elements that contain the following pattern: a dollar sign followed by one to two digits, followed by a period, then another two digits (`\\$\\d{1,2}\\.\\d{2}`). Using the `str_extract_all` function in the `stringr` package, we can extract all matching substrings, specifying `simplify = TRUE` to return a vector:

\vspace{12pt} 
```{r, warning=FALSE, message = FALSE}
  funds <- str_extract_all(new, "\\$\\d{1,2}\\.\\d{2}", simplify = TRUE)
  print(funds)
```
\vspace{12pt} 

Third, we should replace dollar sign with blank, then strip out any leading white space using `str_trim()`.

\vspace{12pt} 
```{r, warning=FALSE, message = FALSE}
  funds <- str_replace_all(funds, "\\$","")
  funds <- str_trim(funds)
  print(funds)
```
\vspace{12pt}   

Lastly, we convert the character vector to numeric values using `as.numeric`, then sum the vector.

\vspace{12pt} 
```{r, warning=FALSE, message = FALSE}
  money <- as.numeric(funds)
  print(paste0("Total galactic big mac funds = $", sum(money)))
```
\vspace{12pt} 
  
A number of observations. In steps one through three, you will have noticed that the characters `"$"`, `"."`, and `"d"` were preceded by double backslash. These are known as _escaped characters_ as the double backslash preceding the characters changes their meanings. In step two, a sequence of unusual characters (`\\$\\d{1,2}\\.\\d{2}`) was used to find the `$x.xx` pattern, which can be broken into specific commands:
  
  - `\\$` is a dollar sign.
  - `\\d{1,2}` is a series of numerical characters that is between one to two digits long.
  - `\\.` is a period.
  - `\\d{2}` is a series of numerical characters that is exactly two digits long.
  
Mastering _regex_ is a productivity multiplier, opening the possibility of ultra-precise text replacement, extraction, and other manipulation. Imagine scenarios where raw data is not quality controlled and mass errors plague the usefulness of the data. An analyst may spend days if not weeks or months cleaning data by hand (or rather through find and replace). With regex, haphazard cleaning is no longer an issue. To make the most of regex requires a command of both _text manipulation functions_ that are designed to interpret regex as well as _regex_ itself.


## Text manipulation functions

*Find and replace* functionality are common in word processing and spreadsheet softwares. But what does it take to do apply it to a sizable amount of data? The following text manipulation functions simplify string manipulation, whether to extracting or editing specific values. We provide two sets of functions. The first are core functions that are furnished with R -- these closely resemble string manipulation functions implemented in most programming languages. While they are powerful, they at times may require some additional trial and error to use. Alternatively, the `stringr` package simplifies text manipulation and syntax, and in some case extends functionality. For the most part, we will use `stringr` in this text.`
  

```{r, echo = FALSE, warning=FALSE, message = FALSE}
temp.table <- read.csv("data/string-manip.csv")
colnames(temp.table) <- c("Description", "Base R", "stringr")

pander::pander(temp.table, split.cell = 60, split.table = Inf, justify = "left",
               caption = "Recommended text manipulation functions.")
```
  
Some basic tasks can be accomplished such as exact matches of specific text. As will be seen later, these functions combined with regex are quite powerful. To illustrate the basic functionality, let's assume we have four sentences that indicate when four US laws were signed. 

\vspace{12pt}  
```{r}
  laws <- c(". Dodd-Frank Act was signed into federal law on July 21, 2010.", 
      "Glass-Steagall Act was signed into federal law by FDR on June 16, 1933", 
      "Hatch Act went into effect on August 2, 1939", 
      "Sarbanes-Oxley Act was signed into law on July 30, 2002")
```
\vspace{12pt} 

Suppose we need to find acts that are named for two congressmen. The `str_detect()` function can be used to produce a boolean vector if the regex pattern  `[A-z]{1,}-[A-z]{1,}` is matched (more on this pattern in the next section). 

\vspace{12pt}   
```{r}
  str_detect(laws, "[A-z]{1,}-[A-z]{1,}")
```
\vspace{12pt} 

We can extract the specific senator names using `str_extract_all`, returning a vector of string values.

\vspace{12pt}   
```{r}
  str_extract_all(laws, "[A-z]{1,}-[A-z]{1,}", simplify = TRUE)
```
\vspace{12pt} 


  

## Regular Expressions (regex)
As we can see, much of the secret ingredient of the text manipulation functions is the regex, which are are powerful commands that give coders the flexibility to search data and surface results following a substring pattern. Before proceeding into more complex string combinations, knowledge of a few cleverly designed capabilities may go a long way: 
  
(1) Alternatives (e.g. "OR" searches) can be surfaced by using a pipe "`|`". For example, a string search for "Bob or Moe" would be represented as "Bob|Moe".
  
(2) The extent of a search should be denoted by parentheses `()`. For example, a string search for "Jenny" or an alternative spelling like Jenny would be represented as "Jenn(y|i)"."
  
(3) A search for one specific character should be placed between square brackets `[]`. 
  
(4) The length of a match is specified using curly brackets `{}`. 
  
In New York City, the famed avenue _Broadway_ is may be written and abbreciated in a number of ways. The vector `streets` contains a few instances of spellings of Broadway mixed in with other streets that start with the letter `B`.

\vspace{12pt}   
```{r}
#A sampling of street names
  streets <- c("Bruckner Blvd", "Bowery", "Broadway", "Bway", "Bdway", 
        "Broad Street", "Bridge Street", "B'way")
  
#Search for two specific options
  str_detect(streets, "Broadway|Bdway")
  
#Search for two variations of Broadway
  str_detect(streets, "B(road|')way")
  
#Search for cases where either d or apostrophe are between B and way
  str_detect(streets, "B[d']way")
  
```
\vspace{12pt} 

  
### Escaped Characters

Quite a few single characters hold a special meaning in addition to the literal meaning. To disambiguate their meaning, a backslash precedes these characters to denote the alternative meaning. A few include:
  
  - `\n`: new line
  
  - `\r`: carriage return
  
  - `\t`: tab
  
  - `\'`: single quote when in a string enclosed in single quotes (`'Nay, I can\'t'`)
  
  - `\"`: double quote when in a string enclosed in double quotes  (`"I have a \"guy\"."`)
  
In other cases, double backslashes should be used:
  
  - `\\.`: period. Otherwise, un-escaped periods indicate searches for _any_ single character.
  
  - `\\$`: dollar sign. A dollar sign without backslashes indicates to find patterns at the end of a string.
  
### Character Classes

A _character class_ or _character set_ is used to identify specific characters within a string. How would one represent "12.301.1034" or "?!?!?!"? One or more of the following character classes can do the job:
  
  - `[:punct:]`: Any and all punctuation such as periods, commas, semicolons, etc. For specific specific punctuation, simply enclose the characters between two brackets. For example, to find only commas and carrots, use `[<>,]`.
  
  
  - `[:alpha:]`: Alphabetic characters such as a, b, c, etc. With other languages including R, it is commonly written as `[a-z]` for lower case, `[A-Z]` for upper case, and `[A-z]` for mixed case.
  
  
  - `[:digit:]`: Numerical values.  With other languages including R, it is commonly written as `\\d` or `[0-9]`. For any non-digit, write `\\D`.
  
  
  - `[:alnum:]`: Alphanumeric characters (mix of letters and numbers). With other languages including R, it is indicated using to as `[0-9A-Za-z]` or `\\w`. For any non-alphanumeric character, use `\\W`.
  
  
  - `[:space:]`: Spaces such as tabs, carriage returns, etc. For any white space, use `\\s`. For any non-whitespace character, use `\\S`.
  
  - `[:graph:]`: Human readable characters including `[:alnum:]` and `[:punct:]`.
  
  - `\\b`: Used to denote "whole words". `\\b` should be placed before and after a regex pattern. For example, `\\b\\w{10}\\b` indicates a 10 letter word.
  
  
These are only a few of the most important character classes. It is worth keeping in mind that these are R-specific character classes and implementations may differ from one programming language to another. 
  
### Quantifiers

Each character class on its own indicates a search for any character of that class. For example, a search for the pattern `[[:alpha]]`  will return any element with an alphabetic character -- not very specific. In practice, most character searches will involve a search for more than just one character. To indicate such a search, regex relies on _quantifiers_ to indicate the length of patterns. For example, a search for a year between the year 1980 and 2000 will require exactly four digits, but a search for the speed of a gust of wind will likely vary between 1 and 3 digits. The following six quantifiers provide flexibility and specificity to effectively accomplish search tasks:
  
  - `{n}`: match pattern n times for a preceding character class. For example `"\\d{4}"` looks for a four digit number.
  
  - `{n, m}`: match pattern at least n-times and not more than m times for a preceding character class. For example `"\\d{1,4}"` looks for one to four digit number.
  
  - `{n, }`: match at least n times for a preceding character class. For example `"\\d{4,}"` searches for a number that has at least four digits.
  
  - `*`: Wildcard, or match at least 0 times. 
  
  - `+`: Match at least once. 
  
  - `?`: Match at most once. 
  

In the example below, quantifiers are used to extract specific number patterns with a high degree of accuracy.
\vspace{12pt} 

```{r}
  big_dates <- c("Octavian became Augustus on 16 Jan 27 BCE", 
             "In the year 2000, a computer bug was expected to topple society.", 
             "In the 5400000000 years, our sun will become a red dwarf.")
  
#Return a 9 digit number
  str_extract(big_dates, "\\d{9}")

#Return a 4 digit substring that is flanked by empty value at either end
  str_extract(big_dates, "\\b\\d{4}\\b")
  
#Match a date that follows 16 January 27 BCE
  str_extract(big_dates, "\\d{2}\\s\\w{3}\\s\\d{2}\\s\\w{3}")
```
\vspace{12pt} 


## Positions

Regex builds in functionality to search for patterns based on location of a substring, such as at the start or end of a string. There are quite a few other position matching patterns, but the following two are the main workhorses:
  
  - `$`: Search at the end of a string.
  
  - `^`: Start of string when placed at the beginning of a regex pattern. 
  
To demonstrate these patterns, we'll apply `str_extract` to three headlines from the BBC.

\vspace{12pt}   
```{r, warning = FALSE, message=FALSE}
  headlines <- c("May to deliver speech on Brexit", 
           "Pound falls with May's comments", 
           "May: Brexit plans to be laid out in new year")
  print(headlines)
  
#Find elements that contain May at the beginning of the string
  str_extract(headlines, "^May")
  
#Find elements that contain Brexit at the beginning of the string
  str_extract(headlines, "Brexit$")

```
\vspace{12pt}   
  
### DIY: Redact PII
  
In an increasingly digital world, data privacy is a sensitive issue that has taken center stage. At the center of it is the safeguarding of Personally identifiable information (PII). Legislation in the European Union, namely the General Data Protection Regulation or GDPR, requires companies to protect the personal data of European Union (EU) citizens  associated with transactions conducted in the EU [(@gdpr)]. The US Census Bureau, which administers the decennial census, must apply disclosure avoidance practices in order so that individuals cannot be identified [(@censusavoidance)]. Anonymization has become a common task when working with sensitive PII data, spanning complex probabilistic methods to simple redaction. We focus here on the latter.

The first element below, for example, contains hypothetical PII and sensiive information -- John's social security number and balance in his savings account are shown. When presented with many lines of sensitive information, one could review each sentence and manually redact sensitive information, but given thousands if not millions of sensitive information, this is simply not feasible.

\vspace{12pt}  
```{r}
statement <- c( "John Doe (SSN: 012-34-5678) has $2303 in savings in his account.",
                "Georgette Smith  (SSN: 000-99-0000) owes $323 to the IRS.",
                "Alexander Doesmith (SSN: 098-76-5432) was fined $14321 for overdue books.")
```
\vspace{12pt}   

Using a combination of regex and `stringr` we can redact sensitive information with placeholders. To remove the SSN, we need a regex pattern that captures a pattern with three digits (`\\d{3}`), a hyphen, two digits (`\\d{2}`), a hyphen, then four digits (`\\d{4}`), or when combined: `\\d{3}-\\d{2}-\\d{4}`. The matched pattern is then replaced with `XXXXX`.

\vspace{12pt}  
```{r}
  library(stringr)
  new_statement <- str_replace(statement,"\\d{3}-\\d{2}-\\d{4}", "XXXXX")
  print(new_statement)
```
\vspace{12pt}   

Next, we replace the dollar value by matching a string that starts with the dollar sign (`\\$`) followed by at least one digit (`\\d{1,}`). And finally, the John Doe's first and last name are replaced by looking for two substrings that each have at least one uppercase letter with an unspecified length (`[A-z]{1,} [A-z]{1,}`) and are found at the beginning of the string (`^`). The resulting sentence has little to no information about the individual in question.

\vspace{12pt}  
```{r}
#Find a replace dollar amount
  new_statement <- str_replace(new_statement,"\\$(\\d{1,})", "XXXXX")

#Find and replace first and last name
  new_statement <- str_replace(new_statement,"^[A-z]{1,} [A-z]{1,}", "XXXXX")
  
  print(new_statement)
```
\vspace{12pt}   

### DIY: Extract PII

The inverse of this task is to extract all information and create a data set. Using the same regex patterns, we can apply `str_extract` to create a three variable data frame containing person name, SSN and money -- all done with minimal effort.

\vspace{12pt}  
```{r}
  ref_table <- data.frame(name = str_extract(statement,"^[A-z]{1,} [A-z]{1,}"),
                          ssn = str_extract(statement,"\\d{3}-\\d{2}-\\d{4}"), 
                          money = str_extract(statement,"\\$(\\d{1,})"))
  print(ref_table)
```

## Working with Dates

Working dates can be challenging. Most programming languages are unable to "auto-magically" recognize a date variable, requiring coders to specify the format using a set of date symbols. The `lubridate` [package](https://lubridate.tidyverse.org/) significantly lowers the bar for R users, making the process of working and manipulating dates more seamless and less prone to error. 

```{r, message=FALSE, warning = FALSE}
require(lubridate)
```

Upon loading lubridate, we can convert string and numeric objects with values that represent dates into date objects. The `as_date` function is most intuitive, automatically detecting the date format and assumes that the date is recorded in UTC. 

```{r}
d0 <- as_date("2010-08-20")
```

In case `as_date` is unable to detect the date, the user can more formally define the date format using functions such as `mdy` and `ymd`. Both are able to accommodate both string and numeric formats.

```{r}
d1 <- mdy("01/20/2010")
d2 <- mdy_hm("01/20/2010 00:00 AM")
d3 <- ymd(20100101)
```

Once data are converted into date objects, it is easy to process derivative information. To calculate duration between two dates is as simple as subtraction. 

```{r}
d1 - d3
```


Often times will need to extract parts of the date. With lubridate, each `year`, `month`, `quarter`, `day`, `hour`, `minute`, and `second` can be extracted using a dedicated function.  For example, we can extract the `year` and `quarter` and concatenate into a string using `paste0`.

```{r}
  y1 <- year(d0)
  q1 <- quarter(d0)
  paste0(y1, "Q", q1)
```

In base R, we can alternatively use the `format` function to specify an output format using date symbols. The example below outputs a concatenation of year and month.

```{r}
  format(d0, "%Y-%m")
```


## Basic Math 

Once the data ar Arithmetic operations: Sum, mean, etc.

| Operation | General |   Row-Wise   | Column-wise |
|:---------|:---------------|:-------------|:---------|
| Sum      | `sum`  | `rowSums` | `colSums` |
| Mean      | `mean`  | `rowMeans` | `colMeans` |
| Minimum | `min` | | |
| Maximum | `max` | | |


Distance measures: dist() - L1/L2
String distance: adist() - Levenshtein, stringdist() - soundex



# Sculpting the data

The ultimate goal of data retrieval and processing is to construct a data set that is ready for analysis, modeling and visualization.This process, however, can occupy between 50% to 80% of a data scientist's time [(@lohr2014)]. Data needs to be processed into different forms depending on the use case. The number of tools available to *wrangle* data are many. But what if a standard framework can be applied make data more actionable sooner. As laid out in @wickham2014, we can follow the principles of *tidy data*:

- Each variable is a column.
- Each observation is a row.
- Each type of observational unit forms a table. 

Otherwise, a *messy* data set is one that does not follow the described structure. Let's walk through a hypothetical company's financial data. Below, each month's data is a column and each row represents a different financial concept. It is a standard way of displaying balance sheets, optimized for readability but not empirical analysis. Working with this data in the current form to produce a time series forecast can prove challenging -- it's messy data. The data should be reshaped into a multivariate time series: each row contains a month's data months and each financial concept as a column. 

\vspace{12pt} 

```{r, echo = FALSE, warning=FALSE, message=FALSE}
temp.table <- read.csv("data/examp_messy.csv")
pander::pander(temp.table, split.cell = 60, split.table = Inf, justify = "left",
               caption = "Example of a messy data set.")


```
\vspace{12pt} 


Fortunately, this is a relatively simple task in this case -- we simply need to transpose the data into tidy form so the data can be more easily manipulated and used for data science purposes. The tidied table can not only be used for time series and cross sectional regression analysis, but visualized in a variety of ways. 

\vspace{12pt} 
```{r, echo = FALSE, warning = FALSE, message = FALSE}
temp.table <- read.csv("data/examp_tidy.csv")
pander::pander(temp.table, split.cell = 60, split.table = Inf, justify = "left",
               caption = "Example of a tidy data set.")


```
\vspace{12pt} 

For the much of this text, we will tend towards tidy data and use them as the preferred data structure that to enable analysis and modeling. Tidying involves moving columns, rotating, slicing, deleting -- all tasks that change the shape of the data, requiring analysts to become intimately familiar with data structures. In the remainder of this section, we introduce core concepts for sculpting data into the right shape.


## Matrix or Data Frame?

Last chapter, we introduced data structures. 

matrices are vectors but in two dimensions ($n \times m$ dimensions). Data frames are a generalization of matrices that allow for each column of data to hold different data types as well as refer to individual columns by a user-specified name.

_When should each matrices and data frames be used?_ From a pure logistical perspective, data frames are more flexible with respect to its ability to store multiple data types. Some code libraries are built specifically for matrices and others for data frames. Ultimately, it is up to the data scientist to choose.

| | Matrices | Data Frames |
|---------|-------------------------------|----------------------------------|
| Pros | Memory efficient. Good for advanced mathematical operations. |  Store mixed types of data types. Allows user to refer to columns by an explicit name. | 
| Cons | Able to store one data type at a time -- leads to slightly more work required to manage multiple matrices. Columns can only be referred to by index number. | Not as memory efficient. |

In this section, we review among the most powerful functions for data manipulation: `sort`, `reshape`, `collapse`, and `merge`.  A mastery of the logic and operations that guide matrix and data frame processing opens the possibilities to work with virtually any kind of data. To illustrate this, consider a data frame with a list of the top 25 male long jumpers and top 25 female long jumpers as found on [Wikipedia](https://en.wikipedia.org/wiki/Long_jump#All-time_top_25_athletes).  Each row contains information about one athlete such as with their record-setting longest jump, date of jump, location of jump among other features. Imagine the sort of tasks that one could do to munged the data into a usable shape.

Note that data can be obtained directly from this URL (https://s3.amazonaws.com/dspp/long_jump_top25.csv):

```{r, warning = FALSE, message=FALSE}
  jumps <- read.csv("https://s3.amazonaws.com/dspp/long_jump_top25.csv")
```


```{r, warning = FALSE, message=FALSE, echo = FALSE}
  
  library(kableExtra)
  rownames(jumps) <- NULL
  blank <- jumps[1:2,]
  blank$sex <- "."
  blank[,1:6] <- "."
  knitr::kable(rbind(jumps[1:2,], blank,jumps[40:41,]), row.names = FALSE, 
               booktabs = TRUE, caption = "Example records from long jump data set")  %>%
            kable_styling(latex_options = c("hold_position"))
```


## Indices and Subsetting 

In both matrices and data frames, individual and ranges of rows and columns can be extracted by calling their index number. The jumps data contains $n = 51$ rows and $k = 6$ features for a total of $306$ data elements. Each row and each feature has a unique index number that starts from $1$ and increases sequentially. In other programming languages, index numbers start from $0$. 

__*To extract the second row*__ from `jumps`, we simply type the number `2` before the commaa in square brackets. The line below essentially indicates that given a matrix or data frame, extract the second row and all columns.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE}
  jumps[2,]
```
\vspace{12pt} 

To __*extract multiple records by row index*__ depends on whether the request is sequential or piecemeal. Below, the first line extracts a range of rows from the 2nd through 4th rows in `jumps`, whereas the second extracts two non-overlapping ranges that are included in a vector. 

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps[2:4, ] # apply the index range to extract rows
  jumps[c(1:2,10:11), ] # specific indices
```
\vspace{12pt} 

The same notation can be used __*to extract all athlete names for all records*__ by typing the number `3` after the comma in square brackets. The number 3 is the column index that contains `athlete` names. _Keep in mind that extracting one column from a matrix or data frame results in a vector -- the data structure is not retained_.

\vspace{12pt} 

```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps[, 3] 
```

\vspace{12pt} 

In addition, data frames provide a few additional methods of extracting the `athlete` column. 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps[, "athlete"] # extract column with "athlete" label
  jumps[["athlete"]] # list syntax to extract column from data frame
  jumps$athlete #compact version of data column manipulation
```

To __*extract two or more columns*__ follows a familiar pattern, making use of either a range of column indices or a vector of column names.
\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps[, 3:4] # extract multiple columns
  jumps[, c("athlete", "mark.meters")] # multiple column labels
```
\vspace{12pt} 

To __*extract rows that meet one specific criterion*__ requires the creation of either a vector of booleans than indicate if the criterion is `TRUE` or `FALSE` for each row (logical vector has the same number of rows as the data), or a list of row index numbers that meet the criterion. The example below performs operations on a data frame.
\vspace{12pt}   

__*Example 1*__: Create a vector of booleans over 8.8 meters, then select rows based on whether a row contains a `TRUE` value.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  group1 <- jumps$mark.meters > 8.8 # return vector of booleans for jumps over 8.8 meters
  jumps[group1, ] # select the indices
```
\vspace{12pt} 

__*Example 2*__: Find row indices that contain the place "Eugene", then use the vector of matching row indices to extract rows from `jumps`. 

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  group2 <- grep("Eugene", jumps$place)
  jumps[group2, ] 
```
\vspace{12pt} 

__*Example 3*__: Find record setting jumps in Tokyo, Mexico City and New York City using the `%in%` operator.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
#Find place in vector of cities
  group3 <- jumps$place %in% c("New York City", "Mexico City", "Tokyo")

#Return matching entries 
  jumps[group3, ]
  
#Return non-matching entries
  jumps[!group3, ]
```
\vspace{12pt} 


__*Extract rows that meet two criteria*__ is as simple as using logical operators. To search for two non-overlapping criteria, the pipe operator `"|"` should be used to represent _OR_, whereas `"&"` represents _AND_.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
#Find athletes who jumped beyond 8.9m or below 7.2m
  group2 <- (jumps$mark.meters > 8.8) | (jumps$mark.meters < 7.2)
  jumps[group2, ] # select the indices
```
\vspace{12pt} 


__*Rows and columns can be re-ordered*__ simply by enumerating indices in a specified order. For example, columns can be re-ordered by passing a vector of column indices into a data frame.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE}
#Original order
  colnames(jumps)

#Reorder according vector of column indices
  jumps <- jumps[, c(2, 4, 3, 6, 1, 5)]
  
#Check new order
  colnames(jumps)
```
\vspace{12pt} 
 
To __*sort a column*__ can be done using `order()`, which returns a vector of row indicies in ascending order.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE}
  order(jumps$mark.meters)
```
\vspace{12pt} 

This simple function can be included in the rows index to sort a entire data frame. To sort descending, add a `"-"` before the feature to be sorted.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps[order(jumps$mark.meters), ] # order records by mark.meters
  jumps[order(-jumps$mark.meters), ] # order records by mark.meters, decreasing
  jumps[order(jumps$mark.meters, decreasing=TRUE), ]# order records by mark.meters, decreasing
```
\vspace{12pt} 

To __*sort on multiple columns*__ is a matter of adding additional fields separated by commas.

\vspace{12pt} 
```{r, warning = FALSE, message=FALSE, eval = FALSE}
# first by rank, then sex, then place
  jumps[order(jumps$rank, jumps$sex, jumps$place), ] 

 # first desc. by rank, then sex, then place
  jumps[order(-jumps$rank, jumps$sex,  jumps$place), ]
```
\vspace{12pt} 



### Exercises {-}
  
  1. Extract records with female athletes who are ranked greater than rank #10.
  2. Extract records with jump marks greater than 7.250 meters and less than 8.625 meters.
  
  
## Reshape
  
Data usually takes on two basic _shapes_: **wide** and **long**.  The data frame in the previous section is in **long** format in which each row represents an individual that is ranked among the top 25 within each sex. Each row has a value (`mark.meters`) associated with the characteristics that make that row unique (`athlete`, `place`, `rank`, `sex`, `date`). 
  
We will find that the **wide** form is also useful. For those who have familiarity with spreadsheet software, reshaping data essentially is the functionality behind a pivot table.  Some features can be used to identify rows and others features can be used to stratify the data by discrete bins. The `reshape()` function can be used to convert a data frame such that each row contains values for each rank (1 to 25) placing the distance jumped by each sex in separate columns. 

\vspace{12pt}  
```{r, warning = FALSE, message = FALSE, eval = FALSE}
  reshape(<data>, 
          idvar = <ID variables>,
          timevar = <column variables>,
          direction = <direction>)
```
\vspace{12pt}  

- `<data>` is the data set
- `<ID variables>` is a variable or a combination of variables that serves as a unique identifier for each row
- `<column variables>` is a variable that will be used to define multiple measurements per row
- `<direction>` indicates if the data will be converted into long or wide form.

To illustrate `reshape()`, the data is whittled down to a three feature set containing `rank`, `sex` and `mark.meters`. These features are then input into the `reshape()` to convert a long form data set into wide form.

\vspace{12pt}   
```{r, warning = FALSE, message=FALSE}
  example <-  jumps[, c("rank", "sex", "mark.meters")]
  wide <- reshape(example, 
          idvar = "rank", timevar = "sex", direction = "wide")
  head(wide, 5)
```
\vspace{12pt} 


There are now 21 rows, where there were previously 51.  Returning to the original **long** format is straightforward; but we aren't left with the exact same data table.  There are artifacts of the change-in-shape for both the column and row names.  Sort of like data manipulation breadcrumbs.  This will be cleaned up in the next subsection.

### Rename row and column headers {-}
  
The newly assigned column or row names may not match the meaning in the data.  It is good practice to maintain the column headers at each stage of analysis, even if you don't immediately use the intermediate data table.  Otherwise editing code gets confusing, quickly.  The column names are stored in an attribute of the data frame:

\vspace{12pt}   
```{r, warning = FALSE, message=FALSE}
#Two ways of obtaining data frame field names
  names(wide)
  colnames(wide)
```
\vspace{12pt} 

Renaming column headers using the built-in, base functions in `R` looks confusing.  In words, the following code identifies the positions in `names(wide)` where the values are `mark.meters.male` and `mark.meters.female`.  At the specified positions, the values is reassigned with new values `male.mark` and `female.mark`, respectively.

\vspace{12pt}   
```{r, warning = FALSE, message=FALSE}
  names(wide)[names(wide) == "mark.meters.male"] <- "male.mark"
  names(wide)[names(wide) == "mark.meters.female"] <- "female.mark"
  head(wide, 5)
```
\vspace{12pt} 

Alternatively, groups of fields can be renamed based on the index position in the names list. 

\vspace{12pt}  
```{r eval = FALSE}
  colnames(wide)[c(1,3)] <- c("rank.num", "female.mark")
```
\vspace{12pt} 



## Collapse
  
Reshape and collapse are often used in conjunction in order to calculate summary statistics by group.  The `aggregate()` function is the workhorse for summarizing data. It accepts three arguments:

\vspace{12pt}   
```{r, warning = FALSE, message = FALSE, eval = FALSE}
  aggregate(<x>, by = list(<groups>), FUN = <function>))
```
\vspace{12pt} 

- `<function>` is the specific function that will be applied to the data. Commonly used functions included `mean`, `sum`, `length` (count), `sd` (standard deviation), among others.
- `<x>` is the R object on which the function will be applied. This generally should be a numerical value.
- `<groups>` is a variable that contains groups for which the mathematical function will be calculated. This needs to be provided as a `list()`.
  
Consider this function to __*estimate the jump distance average by sex*__. To arrive at the answer, we will collapse the data frame by `sex` to create a new data frame with the average jump distance by sex. Notice that the group variable can be named within the list and the number of observations is reduced to only two.

\vspace{12pt}   
```{r, warning = FALSE, message = FALSE}
(out <- aggregate(jumps$mark.meters, by = list(athlete.sex = jumps$sex), FUN = mean))
```
\vspace{12pt} 

How about tabulations?  __*count medals by country*__ 

Note that data can be obtained directly from this [URL](https://s3.amazonaws.com/whoa-data/long_jump_olympics.csv):

\vspace{12pt}  
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  jumps <- read.csv("https://s3.amazonaws.com/whoa-data/long_jump_olympics.csv")
  print(jumps)
```

or alternatively using the `digIt()` function designed for this textbook.
```{r, warning = FALSE, message=FALSE, eval = FALSE}
  olympics <- digIt("long_jump_olympics")
  head(olympics,3)
```


```{r, warning = FALSE, message=FALSE, echo = FALSE}
  olympics <- digIt("long_jump_olympics")
  knitr::kable(head(olympics,3), row.names = FALSE, 
            booktabs = TRUE, format = "latex",
            caption = "Example records from Olympic long jump medalists data set") %>%
            kable_styling(latex_options = c("hold_position"))
```

The USA had won 56 olympic long jump medals through 2017.

\vspace{12pt}  
```{r, warning = FALSE, message=FALSE}
  cty_medals <- aggregate(olympics$country, by = list(country = olympics$country), FUN = length)
  head(cty_medals[order(-cty_medals$x),], 3)
```
\vspace{12pt}  

The built-in `R` functions are effective, but new, memory efficient libraries have arisen to augment `R`'s capabilites. 

### Exercises {-}
  
  1. Load the [`iris`](http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html) dataset using the `datasets` package. Calculate the average and maximum sepal length for each Iris species. (**Bonus**: Write the commands without error messages.) 
  2. Use the `aggregate()` function to count the number of observations of sepal width for each species.
  3. (*Difficult*) Create a data frame with the 35th observation of sepal width for each species.
  

# Control Structures
Much of data science requires developing specialized code to handle the eccentricities of a dataset. Re-running blocks of code is required, often times on multiple data samples and subpopulations. It's simply not scalable to manually change variables and assumptions of the code everytime. 

Variables are typically treated differently based on their quality and characteristics. In order to accomplish analytical and programming tasks, control structures are used to determine how a program will treat a given variable given conditions and parameters. In this section, we will cover two commonly used control structures: if...else statements and for loops.

## If and If...Else Statement
If statements evaluate a logical statement, then execute a script based on whether the evaluated statement is true or false. If the statement is `TRUE`, then the code block is executed.

```{r}
  budget <- 450
  if(budget > 400){
    #If statement true, run script goes here
    print("You're over budget. Cut back.")
  }
```

In cases where there are two or more choices, if...else statements would be appropriate. In addition to the `if()` statement, an `else` statement is included to handle cases where the logical statement is `FALSE`.


```{r, eval=FALSE}
  budget <- 399  
  if(budget >= 400){
    #If statement true, run script goes here
    print("You're over budget. Cut back.")
  } else {
    #else, run script goes here
    print("You're under budget, but watch it.")
  }
```

The complexity of these statements can be as simple as `if(x > 10){ print("Hello")}` more complex trees:

```{r}
  age <- 23
    
  if(age <= 12){
      print("kid")
    } else if(age >12 && age <20) {
      print("teenager")
    } else if(age >=20 && age <65) {
      print("adult")
    } else{
      print("senior")
    }
```


## For-loops
Loops can be used to run the a given statement of code multiple times for a specified number of times or a list of index value. This is a functionality that is available in most programming languages, but the programming syntax will be different. Conceptually, for loops can be likened to an assembly line in a car factory. In order to build a car, a series of well-defined, well-timed processes need to coordinated in a serial fashion. To build 500 cars, the process needs to be executed 500 times. For-loops are essentially the same: Given a well-defined, self-contained process, a process can be be iterativelyapplied to address repetive tasks.

Let's take the following example. The code block essentially says "print values for the range of 1 through 5", where `i` is an *index value*. When executing the statement, R will push the first value in the sequence of 1:5 into the index (in this case, it's the number 1), then the code block in between the `{}` (curly brackets) will be executed, treating `i` as if it's the number 1. Upon executing the code without error, R will advance to the next value in the sequence and repeat the process until all values in the sequence have been completed.

```{r}
  for(i in 1:5){
    print(paste0("Car #", i))
  }
```

We can do the same for a vector or list of values. In the example below, the vector `news` contains six terms. Using a for-loop, we can print out each word in the vector. 

```{r}
  news <- c("The","Dow","Is","Up","By","400pts")
  for(i in news){
    print(i)
  }
```

For-loops has a few qualities that users should be aware. First, what happens within the for-loop is written to the R environment as _global variables_. That means that any object (e.g. calculations, models) that is created in the loop will be accessible in the programming enviromment even after the loop ends. This may be a good or bad, depending on the use case: Good if one wants to keep copies of the intermediate results of a loop iteration, but bad if the user is not careful to take note of the potential floor of extraneous objects that may effect downstream calculations.  Second, one of the most common mistakes when using loops is failing to record the result of the loop. There are functions in R that are designed to log and package results from loops, but in plain vanilla loops, this is not the case. 

__A common paradigm__ with for-loops is to iteratively execute repetitive tasks. For example, if a calculation needed to be applied to each of one million files and the results need to be logged, then for-loops are a good option. Typically, the paradigm proceeds as follows:

1. Create placeholder object (e.g. a vector, matrix, or data frame);
2. Initialize loop; and
3. Add outputs to placeholder at the end of each loop iteration. 

This may be applied in a broad variety of cases such as processes each data set in a repository of many large data sets, calculating complex statistics for various strata and subsets within the data, among others. Best practices with loops start with initializing new placeholder objects to full length before the loop rather than increasing the object size within the loop^[https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf]. In R, this is particularly important issue for efficient data processing. 

In the example below, we would like to calculate the minimum and maximum of each of 1000 randomly generated normal distributions with $\mu = 1000$ and $\sigma = 10$. To do this, a placeholder data frame `x` with three columns (iteration, min and max) is created with $n = 1000$ rows for each of the random distributions to be generated. Then, we use `Sys.time()` to capture when the loop starts and end -- a common practice for optimizing code. The loop is initiated for 1 to 1000 iterations to calculate the mininum and maximum. At the end of each iteration, the min and max results are overwritten to the row that corresponds to the iteration in the placeholder `x`. 


```{r}
#Set placeholder data frame with n rows
  n <- 1000
  x <- data.frame(iteration = 1:n, 
                  min = numeric(n), 
                  max = numeric(n))

#Loop
  start <- Sys.time()
  for(i in 1:n){
    y <- rnorm(10000, 1000, 10)
    x$min[i] <- min(y)
    x$max[i] <- max(y)
  }
  Sys.time() - start

```

The above process required roughly 0.8 seconds to process. _What happens if the placeholder length were not pre-specified?_ For the given parameters, the task normally may last between 1.2 and 1.5 seconds. This may not seem to be much time, but at scale with millions if not billions of records and iterations, the time does tend to add up.

```{r}
#Set placeholder data frame without dimensions
  n <- 1000
  x <- data.frame()

#Loop
  start <- Sys.time()
  for(i in 1:n){
    set.seed(i)
    y <- rnorm(10000, 1000, 10)
    x <- rbind(x, cbind(iteration = i, 
                  min = min(y), 
                  max = max(y)))
  }
  Sys.time() - start

```


### R-specific: `apply`

For-loops are common across all languages, but the efficiency of their implementation will vary. As was described in the previous chapter, R is an interpretted language optimized for mathematical and statistical calculation -- quite different than other languages. This means that programming in R is most optimal when vectorizing calculation -- linear algebra calculations of vectors and matrices using operations such as `+`, `-`, `*`, `%*%`, among others.

In R, the speed of for-loops may be improved using `lapply()` under certain circumstances.  `lapply()`, or _list apply_ Whereas the intermediate objects in for-loops are global variables, `lapply()` creates temporary _local variables_. 


```{r}
#Set n
  n <- 1000

#Loop
  start <- Sys.time()
  x <- lapply(1:n, function(i){
     y <- rnorm(10000, 1000, 10)
     return(cbind(iteration = i, 
                  min = min(y), 
                  max = max(y)))
  })
  x <- do.call(rbind, x)
  Sys.time() - start
```


## While
Whereas for loops require a range or list of values through which to iterate, `while()` statements keep iterating until some condition is met. The `while()` statement is formulated as follows:

```{r, eval=FALSE}
  while([condition is true]){
    
    [execute this statement]
    
  }
```

A simple case may involve drawing a random value $x$ from a normal distribution ($\mu = 1.0$, $\sigma = 0.5$) while $x$ is greater than 0.01. 
1
```{r, echo = FALSE}
set.seed(1)
```
```{r}
  x <- 1
  while(x > 0.01){
    x <- rnorm(1, 1, 0.5)
    print(x)
  }
  print("done!")
```


# Functions

Data manipulation tasks are often repeated for many different projects and it is not uncommon for two or more scripts to contain the same exact steps, but the code is hardcoded. Same logic and different variables names equates to a significant amount of time spent editing and modifying programs. 

Rather than tediously modifying programs, try to write your code once, then never again. Each set of code can serve as re-usable tools that can be re-applied to similar problems, but only if it is standardized with well-laid logic. This is the basis of *user-defined functions*: a coder can define some set of standard required inputs on which a set of steps can be applied to produce a standard output.

A typical function is constructed as follows. Using `function`, a set of input parameters are specified as placeholders for any kind of object. For example, `df1` represents a data frame and `var1` is a variable name in string format. Within the curley brackets, we insert code treating the parameters of actual data. In the example below, we calculate the mean of `var1` in data frame `df1`, then assign it to a `temp.mean`. These calculations are executed in a *local environment*, meaning that any calculations steps within the function are temporary. Thus, `temp.mean` is wiped once the function finishes. The `temp.mean` object can be extracted by passing it to `return`. All of the above steps are assigned to the `meanToo` object that is treated like any other function. 

It is good form to include commentary about how to use the function. At a minimum, there should be comments containing what the function is, the arguments, and the output. In the open source tradition, you should be writing the code as if others will read and use it. 

```{r, eval = FALSE}
meanToo <- function(df1, var1, ...){
  #
  # Calculate mean of a variable in a data frame
  #
  # Args: 
  #   df1 = data frame
  #   var1 = variable name (character)
  #
  # Returns:
  #   A numeric value 
  
  #Code goes here
  temp <- mean(df1[[var1]])
  
  #Return desired result
  return(temp)
}
```

To execute the function, we will simply need to call the function with a data frame and a variable name. Basically any script can be genericized into a standardized function. 

```{r, eval=FALSE}
  meanToo(data, "x1")
```


## Style
Every coder approaches coding differently. Thus, each person's code is a digital equivalent of a fingerprint and has a unique touch. This is all the more reason why a style guide for coding is useful for not only producing functional code, but readable code. 


## DIY: Tracking the supply of online content

Services like Google Trends illustrate the demand for online content, giving a sense of what the public are interested in and what people are reacting to at a given moment. But how about the supply of online content? The supply provides a sense if content producers feel it is worth spending any time on creating new materials. 

The opioid epidemic has been a social issue that has long been brewing, but has only become the centered of public attention in recent years. As the crisis deepens, we'd expect more content to be generated. For each year, we could tediously use the web browser to manually copy the number of search results from a platform like [Bing](https://www.bing.com/), but what if we need to track a large number of search terms. We can package searches on Bing into a simple function that serves as a wrapper that extracts the number of search results. For example, the search query below yields approximately 18,000 search results for the year 2010 (the number may change as this is only an estimate):

`https://www.bing.com/search?q=opioid%20epidemic&filters=ex1%3a%22ez5_12810_13893%22`

Breaking down the URL, we see that the substring `q=opioid%20epidemic` indicates that the search `q=` will be based on the terms that follow `q=` and spaces are encoded as `%20`. Next, the substring `filters=ex1%3a%22ez5_12810_13893%22` indicates a time range filter is applied, specifically `12810_13893` are indexes that represent the range 1/27/2005 to 1/15/2008. 

To make this a seamless process, we should construct a function `bingCounts` that will retrieve the number of search results for a given `search.term` in a specified calendar year (`year.filter`). First, we will need to load packages using `require`, which is specifically designed for within-function package loading. In particular, we will use: 

- To convert the year into indexes, we use the `lubridate` package to work with date objects.
- To efficiently construct a search URL, we rely on the `stringr` package.
- `RCurl` is used to make use of the Client URL package to make a request to the web for a webpage.
- As the webpage is in HTML format, we use the `XML` package to parse the information.


```{r, warning = FALSE, message=FALSE}

bingCounts <- function(search.term, year.filter){
  #
  # Retrieve number of search results for exact query
  #
  # Args: 
  #   search.term = search query (character)
  #   year.term = search year.term (int)
  #
  # Returns:
  #   A numeric value 

  #Load package
    require(lubridate)
    require(stringr)
    require(RCurl)
    require(XML)

  #Get date indexes
    origin <- as_date("1970-01-01")
    start.index <- as_date(paste0(year.filter, "01-01")) - origin
    end.index <- as_date(paste0(year.filter, "12-31")) - origin
    
  #Construct search URL by replacing placeholder terms
    search.url <- "https://www.bing.com/search?q=term&filters=ex1%3a%22ez5_tstart_tend%22"
    search.url <- str_replace(search.url, "term", gsub(" ", "%20", search.term))
    search.url <- str_replace_all(search.url, "tstart", as.character(start.index))
    search.url <- str_replace_all(search.url, "tend", as.character(end.index))
    
  #Get URL content as HTML
    search.html <- getURL(search.url)
  
  #Parse HTML
    parse.search <- htmlTreeParse(search.html, useInternalNodes = TRUE)
  
  #Extract result statistics
    nodes <- getNodeSet(parse.search, "//*[@id='b_tween']/span[1]")
    value <- strsplit(xmlValue(nodes[[1]])," ", fixed = TRUE)[[1]][1]
    return(as.numeric(gsub(",", "", value, fixed = TRUE)))

}
```

Rather than copying the above function multiple times, the resulting function can then be neatly wrapped into a loop to extract the number of search results for each year in a nine-year period between 2009 and 2017. We find the number of search results has grown 44-times over the period. Based on CDC data, the number of opioid-related deaths increased 2.4-times from approximately 20,400 to 49,000.^[https://www.drugabuse.gov/related-topics/trends-statistics/overdose-death-rates] Thus, we can see the search queries give a measure of direction of growth, but are misleading in their magnitude. 

```{r, message = FALSE, warning=FALSE}
#Set parameters
  term <- "opioid epidemic"
  results <- data.frame() 

#Loop through 
  for(i in 2009:2017){
    results <- rbind(results,
                     data.frame(year = i, 
                                cnt = bingCounts(term, i)))
  }
```


```{r, echo = FALSE, fig.cap = "Number of search result by year"}
#Plot
  plot(results, type = "l", ylab = "Number of Search Results", xlab = "Year", col = "red")
  points(results, pch = 16, col = "red")
  
```




# Feature engineering
Define: Feature engineering is the process of transforming data to create new variables that provide potentially more insight about the phenomenon of interest. deeper insight. As we'll see later in the book, feature engineering is critical for optimizing the accuracy of machine learning applications -- the models are only as good as the available data 

Feature engineering typically depend on domain knowledge to construct new variables. Examples:
Continuous values can be summarized as counts, minimum, maximum, average, percentile among others. 

Example: Prevailing weather conditions -- give a benchmark of what has happened

Continuous values can also be interacted with one another to mix the signal. 

Example: Spending per capita

In Text data, create a count or binary variable for each keyword
Example: "One hot encoding" for keywords

Among discrete variables, combine sparse classes into aggregate classes that are more meaningful.  
Example: For example, NYC 311 service has 1,700+ complaint + descriptor combinations

- Flavors of feature engineering. Feature engineering is important but expensive due to the time cost
Automated methods like FeatureTools (https://www.featuretools.com/) can construct a large number of new variables for prediction problems.

- DIY: Augment a US Senate Roll Call 
- Goal is to produce additional features that are correlated with the likely outcome of each vote.
- Get data from senate site -- 101st to 115th for sessions 1 and 2
https://www.senate.gov/legislative/LIS/roll_call_lists/vote_menu_101_1.xml
https://www.senate.gov/legislative/LIS/roll_call_lists/vote_menu_101_2.xml
- Clean up dates 
- For each record, we'll calculate a number of additional variables on a 90 day moving window
- Extract a matrix of key words
- We can see that a simple data set can give way to thousands of new variables


